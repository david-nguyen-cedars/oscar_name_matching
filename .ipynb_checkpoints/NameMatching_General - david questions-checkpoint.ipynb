{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_22060\\2727097187.py:13: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  basepath = \"Z:\\Marco Mathias\\OSCAR\\Match names\"\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_22060\\2727097187.py:13: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  basepath = \"Z:\\Marco Mathias\\OSCAR\\Match names\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     13\u001b[39m basepath = \u001b[33m\"\u001b[39m\u001b[33mZ:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mMarco Mathias\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mOSCAR\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mMatch names\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m##df = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\Oscar_match_DC_Marco.csv\", sep = ',', header = 0, dtype='string') #old file wihtout last known alive time\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#df = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\Oscar_match_DC_Marco_new.csv\", sep = ',', header = 0, dtype='string')\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#df = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\DeathCertMatch_1_28_2025.csv\", sep = ',', header = 0, dtype = 'string')\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#df = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\ForDeathCertMatch_4_21_2025.csv\", dtype = 'string', header = 0, encoding='latin1')\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mZ:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mMarco Mathias\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mOSCAR\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mForDeathCertMatch_7_17_2025.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstring\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# header = 0, encoding='latin1')\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#df = pd.read_excel(r\"Z:\\Marco Mathias\\Presto_autopsiesTo Match_5_30_2025.xlsx\", dtype = 'string', sheet_name='Presto')\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# matchto = pd.read_excel(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\DC_15_22.xlsx\",sheet_name=\"Sheet1\")\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# matchto2 = pd.read_excel(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\DC_15_22.xlsx\",sheet_name=\"Sheet4\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#matchto = pd.read_excel(r\"C:\\Users\\MathiasM\\Box\\OSCAR Death data\\Norby_CCDF_full2023_070124_123124.xlsx\", dtype = 'string', header = 0)\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# matchto = pd.read_excel(r\"C:\\Users\\MathiasM\\OneDrive - Cedars-Sinai Health System\\Documents\\Norby_CCDF_010125_033125.xlsx\", dtype = 'string')\u001b[39;00m\n\u001b[32m     26\u001b[39m matchto = pd.read_excel(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mP:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mVital stat\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDeathCert\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mNorby_CCDF_040125_063025.xlsx\u001b[39m\u001b[33m\"\u001b[39m, dtype = \u001b[33m'\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\excel\\_base.py:508\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     data = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\excel\\_base.py:1616\u001b[39m, in \u001b[36mExcelFile.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\n\u001b[32m   1577\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1578\u001b[39m     sheet_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1596\u001b[39m     **kwds,\n\u001b[32m   1597\u001b[39m ) -> DataFrame | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[32m   1598\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[32m   1600\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1614\u001b[39m \u001b[33;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[32m   1615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\excel\\_base.py:778\u001b[39m, in \u001b[36mBaseExcelReader.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m    775\u001b[39m     sheet = \u001b[38;5;28mself\u001b[39m.get_sheet_by_index(asheetname)\n\u001b[32m    777\u001b[39m file_rows_needed = \u001b[38;5;28mself\u001b[39m._calc_rows(header, index_col, skiprows, nrows)\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_sheet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_rows_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[33m\"\u001b[39m\u001b[33mclose\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    780\u001b[39m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[32m    781\u001b[39m     sheet.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\excel\\_openpyxl.py:615\u001b[39m, in \u001b[36mOpenpyxlReader.get_sheet_data\u001b[39m\u001b[34m(self, sheet, file_rows_needed)\u001b[39m\n\u001b[32m    613\u001b[39m data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar]] = []\n\u001b[32m    614\u001b[39m last_row_with_data = -\u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# trim trailing empty elements\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\worksheet\\_read_only.py:85\u001b[39m, in \u001b[36mReadOnlyWorksheet._cells_by_row\u001b[39m\u001b[34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_source() \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[32m     78\u001b[39m     parser = WorkSheetParser(src,\n\u001b[32m     79\u001b[39m                              \u001b[38;5;28mself\u001b[39m._shared_strings,\n\u001b[32m     80\u001b[39m                              data_only=\u001b[38;5;28mself\u001b[39m.parent.data_only,\n\u001b[32m     81\u001b[39m                              epoch=\u001b[38;5;28mself\u001b[39m.parent.epoch,\n\u001b[32m     82\u001b[39m                              date_formats=\u001b[38;5;28mself\u001b[39m.parent._date_formats,\n\u001b[32m     83\u001b[39m                              timedelta_formats=\u001b[38;5;28mself\u001b[39m.parent._timedelta_formats)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_row\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\worksheet\\_reader.py:156\u001b[39m, in \u001b[36mWorkSheetParser.parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    137\u001b[39m properties = {\n\u001b[32m    138\u001b[39m     PRINT_TAG: (\u001b[33m'\u001b[39m\u001b[33mprint_options\u001b[39m\u001b[33m'\u001b[39m, PrintOptions),\n\u001b[32m    139\u001b[39m     MARGINS_TAG: (\u001b[33m'\u001b[39m\u001b[33mpage_margins\u001b[39m\u001b[33m'\u001b[39m, PageMargins),\n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m \n\u001b[32m    152\u001b[39m }\n\u001b[32m    154\u001b[39m it = iterparse(\u001b[38;5;28mself\u001b[39m.source) \u001b[38;5;66;03m# add a finaliser to close the source when this becomes possible\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtag_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtag\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtag_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdispatcher\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1520.0_x64__qbz5n2kfra8p0\\Lib\\xml\\etree\\ElementTree.py:1238\u001b[39m, in \u001b[36miterparse.<locals>.iterator\u001b[39m\u001b[34m(source)\u001b[39m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m pullparser.read_events()\n\u001b[32m   1237\u001b[39m \u001b[38;5;66;03m# load event buffer\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1238\u001b[39m data = \u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1520.0_x64__qbz5n2kfra8p0\\Lib\\zipfile\\__init__.py:1015\u001b[39m, in \u001b[36mZipExtFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28mself\u001b[39m._offset = \u001b[32m0\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m n > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n\u001b[32m-> \u001b[39m\u001b[32m1015\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[32m   1017\u001b[39m         \u001b[38;5;28mself\u001b[39m._readbuffer = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1520.0_x64__qbz5n2kfra8p0\\Lib\\zipfile\\__init__.py:1083\u001b[39m, in \u001b[36mZipExtFile._read1\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1081\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._decompressor.unconsumed_tail\n\u001b[32m   1082\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n > \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m         data += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1085\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._read2(n)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1520.0_x64__qbz5n2kfra8p0\\Lib\\zipfile\\__init__.py:1115\u001b[39m, in \u001b[36mZipExtFile._read2\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1112\u001b[39m n = \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m.MIN_READ_SIZE)\n\u001b[32m   1113\u001b[39m n = \u001b[38;5;28mmin\u001b[39m(n, \u001b[38;5;28mself\u001b[39m._compress_left)\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fileobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28mself\u001b[39m._compress_left -= \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[32m   1117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1520.0_x64__qbz5n2kfra8p0\\Lib\\zipfile\\__init__.py:834\u001b[39m, in \u001b[36m_SharedFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    830\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt read from the ZIP file while there \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    831\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mis an open writing handle on it. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mClose the writing handle before trying to read.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    833\u001b[39m \u001b[38;5;28mself\u001b[39m._file.seek(\u001b[38;5;28mself\u001b[39m._pos)\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28mself\u001b[39m._pos = \u001b[38;5;28mself\u001b[39m._file.tell()\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "##DC matching (second set of files from Priya)\n",
    "import pandas as pd\n",
    "\n",
    "def mrnfix(x):\n",
    "    if len(str(x)) != 9: #if not 9 digits, prepend enough 0's at front to make it 9 digits.\n",
    "        return '0'*(9-len(str(x)))+str(x)\n",
    "    else:\n",
    "        return str(x)\n",
    "global df, matchto\n",
    "savemod = '2ndhalf25'\n",
    "drop_pat_ids = 1\n",
    "removevar = 1\n",
    "basepath = \"Z:\\Marco Mathias\\OSCAR\\Match names\"\n",
    "##df = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\Oscar_match_DC_Marco.csv\", sep = ',', header = 0, dtype='string') #old file wihtout last known alive time\n",
    "#df = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\Oscar_match_DC_Marco_new.csv\", sep = ',', header = 0, dtype='string')\n",
    "#df = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\DeathCertMatch_1_28_2025.csv\", sep = ',', header = 0, dtype = 'string')\n",
    "#df = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\ForDeathCertMatch_4_21_2025.csv\", dtype = 'string', header = 0, encoding='latin1')\n",
    "df = pd.read_excel(r\"Z:\\Marco Mathias\\OSCAR\\ForDeathCertMatch_7_17_2025.xlsx\", dtype = 'string')# header = 0, encoding='latin1')\n",
    "#df = pd.read_excel(r\"Z:\\Marco Mathias\\Presto_autopsiesTo Match_5_30_2025.xlsx\", dtype = 'string', sheet_name='Presto')\n",
    "# matchto = pd.read_excel(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\DC_15_22.xlsx\",sheet_name=\"Sheet1\")\n",
    "# matchto2 = pd.read_excel(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\DC_15_22.xlsx\",sheet_name=\"Sheet4\")\n",
    "#matchto = pd.read_excel(r\"C:\\Users\\MathiasM\\Box\\OSCAR Death data\\Norby_CCDF_010123_063024_combined_SUBSET.xlsx\", dtype='string') #new matchto file for this read-in set 23-24\n",
    "#matchto = pd.read_csv(r\"C:\\Users\\MathiasM\\Box\\OSCAR Death data\\Norby_CCMDF_2016_2022_combined_SUBSET.csv\", dtype='string', header = 0, sep = ',') #16-22\n",
    "#matchto = pd.read_excel(r\"C:\\Users\\MathiasM\\Box\\OSCAR Death data\\Norby_CCDF_full2023_070124_123124.xlsx\", dtype = 'string', header = 0)\n",
    "# matchto = pd.read_excel(r\"C:\\Users\\MathiasM\\OneDrive - Cedars-Sinai Health System\\Documents\\Norby_CCDF_010125_033125.xlsx\", dtype = 'string')\n",
    "matchto = pd.read_excel(r\"P:\\Vital stat\\DeathCert\\Norby_CCDF_040125_063025.xlsx\", dtype = 'string')\n",
    "\n",
    "#matchto = pd.read_excel(r\"Z:\\Marco Mathias\\Presto_autopsiesTo Match_5_30_2025.xlsx\", dtype = 'string', sheet_name='Autopsies')\n",
    "\n",
    "#c:\\Users\\MathiasM\\Downloads\\SUDS (n=284)(Sheet1).csv\n",
    "\n",
    "print('Files read in')\n",
    "if 'matchto2' in locals():\n",
    "    if list(matchto.columns) == list(matchto2.columns):\n",
    "        matchto = pd.concat([matchto, matchto2])\n",
    "        del matchto2\n",
    "    else:\n",
    "        raise Exception('Column mismatch: {}'.format([i for i in matchto.columns if i not in matchto2.columns] + [i for i in matchto2.columns if i not in matchto.columns]))\n",
    "\n",
    "df.insert(df.shape[1], 'Tag', None)\n",
    "\n",
    "#excl = pd.read_excel(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\Dc_Exclude.xlsx\")\n",
    "excl = pd.read_excel(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\DCMatchesOscar1_julY2024.xlsx\")\n",
    "print('Excl read in')\n",
    "if 'MRN' in df.columns:\n",
    "    excl['MRN'] = excl['MRN'].apply(lambda x : mrnfix(x))\n",
    "\n",
    "    sfn = list(excl['F1'].values)\n",
    "    mrn = list(excl['MRN'].values)\n",
    "\n",
    "    df = df[~df['MRN'].isin(mrn)] \n",
    "    print(df.shape)\n",
    "    \n",
    "if 'F1' in matchto.columns:\n",
    "    matchto = matchto[~matchto['F1'].isin(sfn)]\n",
    "    print(matchto.shape)\n",
    "\n",
    "try:\n",
    "    del sfn, mrn\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# print(df.shape, df.columns)\n",
    "# print(matchto.shape, matchto.columns)\n",
    "\n",
    "##NOTE: ADD GENDER, EXCLUSION SECTIONS FOR THIS READ-IN ZONE!\n",
    "gender_match = 0\n",
    "if gender_match == 1:\n",
    "    print('Loading gender table')\n",
    "    gender = pd.read_excel(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\oscar_gender.xlsx\", dtype={'strMRN' : str, 'gender' : str})\n",
    "    gender['strMRN'] = gender['strMRN'].apply(lambda x : mrnfix(x))\n",
    "    gender=gender.rename(columns={'strMRN' : 'MRN'})\n",
    "    #print(len(gender))\n",
    "    print('Merging gender')\n",
    "    df = df.merge(gender, how = 'left', left_on=['MRN'], right_on=['MRN'])\n",
    "\n",
    "    print('Matched {} genders of {} ({}%) in gender file. {} records still missing gender in df'.format(len(df[~pd.isna(df['gender'])]),len(gender),round(100*(len(df[~pd.isna(df['gender'])])/len(gender)), 2),len(df[pd.isna(df['gender'])])))\n",
    "    ### Double check this - I don't think its very good##\n",
    "    del gender\n",
    "#\"Z:\\Marco Mathias\\OSCAR\\Match names\\oscar_gender.xlsx\"\n",
    "\n",
    "if 'F4' in matchto.columns:\n",
    "    matchto['F4'] = matchto['F4'].apply(lambda x : None if str(x) == '-' else x) #cleaning middle name '-' as Null\n",
    "\n",
    "\n",
    "### CHECKING IF THERE ARE duplicated COLUMNS - could result in error further downstream\n",
    "overlap = set([i for i in list(df.columns) if i in list(matchto.columns)] + [i for i in list(matchto.columns) if i in list(df.columns)])\n",
    "\n",
    "if len(overlap) > 0:\n",
    "    #print(overlap)\n",
    "    raise Exception(\"\\nColumn overlap detected: {}. This could cause problems in the future. Please rename one sheets' column(s).\".format(overlap))\n",
    "    for ovie in overlap:\n",
    "        print('Renaming {} to {}_matchto in matchto dataframe.'.format(ovie, ovie))\n",
    "        matchto = matchto.rename(columns = {ovie : ovie+'_matchto'})\n",
    "        #this would work in the future if we make the column names a dict instead of a list, then we could edit the entry for that item?\n",
    "else:\n",
    "    print('No column overlap detected!')\n",
    "\n",
    "\n",
    "#Checking for commas in any of the column names. Because it will cause problems later down the line. NEED TO BE REMEDIED ASAP\n",
    "erroneouslist_m = list()\n",
    "for col in list(matchto.columns):\n",
    "    if ',' in col:\n",
    "        erroneouslist_m.append(col)\n",
    "erroneouslist_df = list()\n",
    "for col in list(df.columns):\n",
    "    if ',' in col:\n",
    "        erroneouslist_df.append(col)\n",
    "    \n",
    "if len(erroneouslist_m) > 0:\n",
    "    raise Exception('PLEASE FIX:\\nThere are commas (,) found in the following MATCHTO columns: {}\\nThese will cause problems later down the line. Please manually change them in the input file.'.format(erroneouslist_m))\n",
    "if len(erroneouslist_df) > 0:\n",
    "    raise Exception('PLEASE FIX:\\nThere are commas (,) found in the following DF columns: {}\\nThese will cause problems later down the line. Please manually change them in the input file.'.format(erroneouslist_df))\n",
    "\n",
    "    \n",
    "\n",
    "def run():\n",
    "    #import datetime\n",
    "    print('Last run at {}'.format(datetime.datetime.now()))\n",
    "\n",
    "del erroneouslist_df, erroneouslist_m, overlap, excl#freeing memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding blank columns to df and matchto\n",
    "import numpy as np \n",
    "df['BlankCol_df'] = np.nan\n",
    "matchto['BlankCol_matchto'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPECIAL SECTION FOR THIS RUN THROUGH - CLEANING UP PAT_NAME COLUMN in df\n",
    "if 'PAT_NAME' in df.columns:\n",
    "    df['First Name'] = df['PAT_NAME'].apply(lambda x : x.split(',')[1].split(' ')[0])\n",
    "    df['Middle Name'] = df['PAT_NAME'].apply(lambda x : x.split(',')[1].split(' ')[1] if ' ' in x.split(',')[1] else None)\n",
    "    df['Last Name'] = df['PAT_NAME'].apply(lambda x : x.split(',')[0])\n",
    "else:\n",
    "    print('PAT_NAME not in df cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              SSN  count\n",
      "0       000000001  62189\n",
      "1       111111111  15870\n",
      "2       000000000    182\n",
      "3       999999999    128\n",
      "4       011111111     21\n",
      "...           ...    ...\n",
      "268628  456641664      1\n",
      "268629  199704053      1\n",
      "268630  058565135      1\n",
      "268631  547921767      1\n",
      "268632  615214760      1\n",
      "\n",
      "[268633 rows x 2 columns]\n",
      "              SSN  count\n",
      "0       123456789      4\n",
      "1       559551834      3\n",
      "2       064740487      2\n",
      "3       215915970      2\n",
      "4       999991111      2\n",
      "...           ...    ...\n",
      "268594  456641664      1\n",
      "268595  199704053      1\n",
      "268596  058565135      1\n",
      "268597  547921767      1\n",
      "268598  546354984      1\n",
      "\n",
      "[268599 rows x 2 columns]\n",
      "             F31  count\n",
      "0      551686108      2\n",
      "1      568110089      2\n",
      "2      569628713      2\n",
      "3      566272936      2\n",
      "4      608882468      2\n",
      "...          ...    ...\n",
      "68254  570257339      1\n",
      "68255  565115785      1\n",
      "68256  566214848      1\n",
      "68257  553577486      1\n",
      "68258  625172853      1\n",
      "\n",
      "[68259 rows x 2 columns]\n",
      "     count\n",
      "F31       \n",
      "9    68264\n",
      "0     3874\n",
      "      count\n",
      "SSN        \n",
      "9    268637\n",
      "0     78476\n"
     ]
    }
   ],
   "source": [
    "### SPECIAL SECTION FOR DC RUN THROUGH - CLEANING UP SSN COLUMN in df and matchto\n",
    "\n",
    "from collections import _count_elements #neat module!\n",
    "# d={}\n",
    "# _count_elements(d,'abcdefabcd')\n",
    "# print(d['a'])\n",
    "# print(z:=list(d.keys()))\n",
    "dfssn = 'SSN'\n",
    "matchtossn = 'F31'\n",
    "\n",
    "if dfssn in list(df.columns) and matchtossn in list(matchto.columns):\n",
    "\n",
    "    df[dfssn] = df[dfssn].apply(lambda x : str(x).replace('-', ''))\n",
    "    matchto[matchtossn] = matchto[matchtossn].apply(lambda x : str(x).replace('-', ''))\n",
    "\n",
    "    print(p:=df[dfssn].value_counts().to_frame().reset_index())\n",
    "\n",
    "    over4 = p[p['count'] > 4][dfssn].to_list()\n",
    "    over4 = over4 + ['<NA>', 'nan', None, 123456789]\n",
    "    def ssncleaner(x):\n",
    "        x=x.replace('.0', '')\n",
    "\n",
    "        if x in over4:\n",
    "            return None\n",
    "        if x == '0' or x == 0:\n",
    "            return None\n",
    "        \n",
    "        while len(x) != 9:\n",
    "            x='0'+str(x)\n",
    "\n",
    "            \n",
    "\n",
    "        d={}\n",
    "        _count_elements(d, x)\n",
    "        if '0' in d.keys() and d['0'] > 6:\n",
    "            return None\n",
    "        elif '1' in d.keys() and d['1'] > 7:\n",
    "            return None\n",
    "        elif '9' in d.keys() and d['9'] > 6:\n",
    "            return None\n",
    "        elif list(d.keys()) == ['0', '1']:  #If ssn composed of 1,0 only - return None\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    df[dfssn] = df[dfssn].apply(lambda x : ssncleaner(x))\n",
    "    print(p2:=df[dfssn].value_counts().to_frame().reset_index())\n",
    "\n",
    "    matchto[matchtossn] = matchto[matchtossn].apply(lambda x : ssncleaner(x))\n",
    "    print(p3:=matchto[matchtossn].value_counts().to_frame().reset_index())\n",
    "        #match SSN as output file so we can clean it later \n",
    "        #intermediate save file\n",
    "    print(matchto[matchtossn].apply(lambda x : len(x) if not pd.isna(x) else 0).value_counts().to_frame())\n",
    "    print(df[dfssn].apply(lambda x : len(x) if not pd.isna(x) else 0).value_counts().to_frame())\n",
    "    del over4\n",
    "else:\n",
    "    try:\n",
    "        del dfssn, matchtossn\n",
    "    except:\n",
    "        print('Could not delete dfssn and matchtossn vars - not found in one or both column sets')\n",
    "        \n",
    "try:\n",
    "    del p, p2, p3\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed dfarrestloc_a from BlankCol_df1 to BlankCol_df11\n",
      "Renamed dfarrestloc_b from BlankCol_df2 to BlankCol_df22\n",
      "Renamed matchtodoa from BlankCol_matchto1 to BlankCol_matchto13\n",
      "df BIRTH_DATE dtype <class 'str'>\n",
      "Converting df BIRTH_DATE dtype...\n",
      "Done converting... checking...\n",
      "      1995-10-17 <class 'datetime.date'> \n",
      "\n",
      "df DEATH_DATE dtype <class 'pandas._libs.missing.NAType'>\n",
      "Converting df DEATH_DATE dtype...\n",
      "Done converting... checking...\n",
      "      2018-01-18 <class 'datetime.date'> \n",
      "\n",
      "df MAX_ENC_DATE dtype <class 'str'>\n",
      "Converting df MAX_ENC_DATE dtype...\n",
      "Done converting... checking...\n",
      "      2018-04-29 <class 'datetime.date'> \n",
      "\n",
      "matchto F8 dtype <class 'str'>\n",
      "Converting matchto F8 dtype...\n",
      "Done converting... checking...\n",
      "      1957-10-09 <class 'datetime.date'> \n",
      "\n",
      "matchto F20 dtype <class 'str'>\n",
      "Converting matchto F20 dtype...\n",
      "Done converting... checking...\n",
      "      2025-05-06 <class 'datetime.date'> \n",
      "\n",
      "\n",
      "Cleaning name columns...\n",
      "Forming \"first_*\" columns\n",
      "Forming \"address_1\" columns\n",
      "\n",
      "\n",
      "Dropped 0 duplicates from df\n",
      "0\n",
      "Dropped 0 duplicates from matchto\n",
      "0\n",
      "['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'GENDER', 'ADD_LINE_1', 'ADD_LINE_2', 'CITY', 'STATE_C', 'HOME_PHONE', 'SSN', 'MAX_ENC_DATE', 'DEATH_DATE', 'Tag', 'BlankCol_df', 'First Name', 'Middle Name', 'Last Name', 'BlankCol_df11', 'BlankCol_df22', 'First Name_f2', 'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1', 'Last Name_f1', 'ADD_LINE_1_first', 'nametag_df']\n"
     ]
    }
   ],
   "source": [
    "################# COLUMN ASSOCIATION CHUNK\n",
    "#### Making sure all relevant columns are of correct data type\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "\n",
    "### NORMAL RUN!\n",
    "dfdob = 'BIRTH_DATE' #make this a tkinter menu later - modify code from FullSuiteGeolocation header - picking columns from drop down list\n",
    "dfdod = 'DEATH_DATE'\n",
    "dffn = 'First Name'\n",
    "dfmn = 'Middle Name'\n",
    "dfln = 'Last Name'\n",
    "dfunids = ['PAT_ID']\n",
    "dfaddress = 'ADD_LINE_1'\n",
    "dfsex = 'GENDER'\n",
    "dfarrestloc_a = 'BlankCol_df1'\n",
    "dfarrestloc_b = 'BlankCol_df2'\n",
    "\n",
    "## ONE OFF AUTOPSY RUN:\n",
    "# dfdob = 'Demographics.DATE OF BIRTH'\n",
    "# dfdod = 'Date of death'\n",
    "# dffn = 'First Name'\n",
    "# dfmn = 'Middle Name'\n",
    "# dfln = 'Last Name'\n",
    "# dfunids = ['SUDS #']\n",
    "# dfaddress = 'BlankCol_df'\n",
    "# dfsex = 'BlankCol_df'\n",
    "# dfarrestloc_a = 'BlankCol_df'\n",
    "# dfarrestloc_b = 'BlankCol_df'\n",
    "\n",
    "# ## ORIGINAL RUN\n",
    "# matchtodob = 'DOB'\n",
    "# matchtodod = 'DOD'\n",
    "# matchtofn = 'FNAME'\n",
    "# matchtomn = 'MNAME'\n",
    "# matchtoln = 'LNAME' #see above\n",
    "# matchtotarget = 'Snumber'\n",
    "# matchtounids = ['Snumber']\n",
    "# matchtoaddress = 'ADDR1'\n",
    "# matchtoaddress2 = 'ADDR2' #\n",
    "# matchtocity = 'CITY_matchto'\t #\n",
    "# matchtosex = 'SEX' #\n",
    "\n",
    "## DC RUN\n",
    "# matchtodob = 'dob'\n",
    "# matchtodod = 'dod'\n",
    "# matchtofn = 'First_name'\n",
    "# matchtomn = 'middle_name'\n",
    "# matchtoln = 'last_name' #see above\n",
    "# matchtotarget = 'SFN'\n",
    "# matchtounids = ['SFN']\n",
    "# matchtoaddress = 'home addr1'\n",
    "# matchtoaddress2 = 'home_addre' #\n",
    "# matchtocity = 'home_city'\t #\n",
    "\n",
    "## New Combined CCDF Norby run\n",
    "matchtodob = 'F8'\n",
    "matchtodod = 'F20'\n",
    "matchtofn = 'F3'\n",
    "matchtomn = 'F4'\n",
    "matchtoln = 'F5' #see above\n",
    "matchtotarget = 'F1'\n",
    "matchtounids = ['F1']\n",
    "matchtoaddress = 'F55'\n",
    "matchtoaddress2 = 'F56' #\n",
    "matchtocity = 'F57'\t #\n",
    "matchtosex = 'F19' #have to get this column from \"C:\\Users\\MathiasM\\Box\\OSCAR Death data\\Norby_CCDF_010123_063024_combined.xlsx\"\n",
    "matchtodoa = 'BlankCol_matchto1'\n",
    "matchtossn = 'F31'\n",
    "\n",
    "matchtoarrestloc_a = 'F132'\n",
    "matchtoarrestloc_b = 'F133'\n",
    "matchtoaddress_arrest_city = 'F134'\n",
    "\n",
    "##One-off autopsy run:\n",
    "# matchtodob = 'Identification - Date of birth'\n",
    "# matchtodod = \"Identification - Date of death\"\n",
    "# matchtofn = 'Identification - First name'\n",
    "# matchtomn = 'BlankCol_matchto'\n",
    "# matchtoln = 'Identification - Last name'\n",
    "# matchtotarget = \"Identification - Case number\"\n",
    "# matchtounids = [\"Identification - Case number\"]\n",
    "# matchtoaddress = 'BlankCol_matchto'\n",
    "# matchtoaddress2 = 'BlankCol_matchto'\n",
    "# matchtocity = 'BlankCol_matchto'\n",
    "# matchtosex = 'BlankCol_matchto'\n",
    "# matchtodoa = 'BlankCol_matchto'\n",
    "#matchtossn \n",
    "\n",
    "# matchtoarrestloc_a = 'BlankCol_matchto'\n",
    "# matchtoarrestloc_b = 'BlankCol_matchto'\n",
    "# matchtoaddress_arrest_city = 'BlankCol_matchto'\n",
    "\n",
    "###!Cleaning up blankcols!\n",
    "\n",
    "bc_matchto = []\n",
    "bc_df = []\n",
    "count_m = 1\n",
    "\n",
    "for var in list(locals()):\n",
    "    if 'matchto' in var:\n",
    "        if type(eval(var)) == str:\n",
    "           if 'blankcol' in eval(var).lower() and '_first' not in eval(var).lower():\n",
    "                print('Renamed {} from {} to {}'.format(var, eval(var), eval(var)+str(count_m)))\n",
    "\n",
    "                exec(\"{}='{}{}'\".format(var, eval(var), count_m))\n",
    "\n",
    "                count_m = count_m + 1\n",
    "                bc_matchto.append(eval(var)) #bc var already changed, don't need to append count_m\n",
    "                if eval(var) not in matchto.columns:\n",
    "                    matchto.insert(matchto.shape[1], eval(var), None)\n",
    "    if 'df' in var:\n",
    "        if type(eval(var)) == str:\n",
    "            if 'blankcol' in eval(var).lower() and '_first' not in eval(var).lower():\n",
    "                print('Renamed {} from {} to {}'.format(var, eval(var), eval(var)+str(count_m)))\n",
    "\n",
    "                exec(\"{}='{}{}'\".format(var, eval(var), count_m))\n",
    "                \n",
    "                count_m = count_m + 1\n",
    "                bc_df.append(eval(var)) #bc var already changed, don't need to append count_m\n",
    "                if eval(var) not in df.columns:\n",
    "                    df.insert(df.shape[1], eval(var), None)\n",
    "\n",
    "del bc_matchto, bc_df, count_m, var\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if not 'BlankCol_matchto1' in matchto.columns:\n",
    "#     matchto.insert(matchto.shape[1], 'BlankCol_matchto1', None) #changed default value from '' to None, could cause errors. Alternative is to update subfuncs s.t they account\n",
    "#     #for blank strings. Notably date_diffs_noabs\n",
    "# if not 'BlankCol_matchto2' in matchto.columns:\n",
    "#     matchto.insert(matchto.shape[1], 'BlankCol_matchto2', None)\n",
    "# if not 'BlankCol_df1' in df.columns:\n",
    "#     df.insert(df.shape[1], 'BlankCol_df1', None)\n",
    "# if not 'BlankCol_df2' in df.columns:\n",
    "#     df.insert(df.shape[1], 'BlankCol_df2', None)\n",
    "\n",
    "def gendertypeswitch(x): #where x is a single entry\n",
    "    if not pd.isna(x):\n",
    "        if x == 'F':\n",
    "            return '1'\n",
    "        elif x == 'M':\n",
    "            return '2'\n",
    "        elif x == '1' or x == 1: #protecting other type if present already (usually for df)\n",
    "            return '1' \n",
    "        elif x == '2' or x == 2: #protecting other type if present already (usually for df)\n",
    "            return '2'\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "preclean_gender_matchto = list(matchto[matchtosex].unique())\n",
    "preclean_gender_df = list(df[dfsex].unique())\n",
    "#matchto[matchtosex] = matchto[matchtosex].apply(lambda x : '1' if x == 'F' else '2') #priya said F = 1, M = 2. THIS IS ONLY FOR THIS CHUNK! COMMENT OUT IF NOT USING!\n",
    "matchto[matchtosex] = matchto[matchtosex].apply(lambda x : gendertypeswitch(x))\n",
    "df[dfsex] = df[dfsex].apply(lambda x : gendertypeswitch(x))\n",
    "\n",
    "\n",
    "\n",
    "#Created here:\n",
    "matchtofn2 = matchtofn+'_f2'\n",
    "matchtoln2 = matchtoln+'_f2'\n",
    "\n",
    "matchtofn3 = matchtofn+'_f3'\n",
    "matchtoln3 = matchtoln+'_f3'\n",
    "\n",
    "matchtofn1 = matchtofn+'_f1'\n",
    "matchtoln1 = matchtoln+'_f1'\n",
    "matchtoaddress_1 = matchtoaddress+\"_first\"\n",
    "\n",
    "######Created here, no need to modify:\n",
    "dffn2 = dffn+'_f2'\n",
    "dfln2 = dfln+'_f2'\n",
    "\n",
    "dffn3 = dffn+'_f3'\n",
    "dfln3 = dfln+'_f3'\n",
    "\n",
    "dffn1 = dffn+'_f1'\n",
    "dfln1 = dfln+'_f1'\n",
    "\n",
    "dfaddress_1 = dfaddress+'_first'\n",
    "\n",
    "# if 'matchtossn' in locals(): #make this more modular...\n",
    "#     matchto_columns = [matchtodob, matchtodod, matchtofn, matchtomn, matchtoln, matchtotarget, \n",
    "#                     matchtofn2, matchtoln2, matchtofn3, matchtoln3, matchtofn1, matchtoln1, matchtossn, matchtoaddress_1, matchtoaddress,\n",
    "#                     matchtoaddress2, matchtocity, matchtosex, matchtoarrestloc_a, matchtoarrestloc_b, matchtoaddress_arrest_city] #+ [i for i in matchto.columns if 'blankcol']\n",
    "# else: \n",
    "#     matchto_columns = [matchtodob, matchtodod, matchtofn, matchtomn, matchtoln, matchtotarget, \n",
    "#                     matchtofn2, matchtoln2, matchtofn3, matchtoln3, matchtofn1, matchtoln1, matchtoaddress_1, matchtoaddress,\n",
    "#                     matchtoaddress2, matchtocity, matchtosex, matchtoarrestloc_a, matchtoarrestloc_b, matchtoaddress_arrest_city]\n",
    "\n",
    "matchto_columns = []\n",
    "for var in list(locals()):\n",
    "    if str(var[0:7]) == 'matchto' and type(eval(var)) == str:\n",
    "        #print(var, var[0:7])\n",
    "        matchto_columns.append(eval(var))\n",
    "\n",
    "def date_parsing(x):\n",
    "    if not pd.isna(x):\n",
    "        try:\n",
    "            if type(x) != datetime.date:\n",
    "                return parser.parse(str(x)).date()\n",
    "            else:\n",
    "                return x\n",
    "        except Exception as e:\n",
    "            #print('Could not date parse: {} due to {}'.format(x, e))\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def DTfix(subset_string, dataframe, list_of_date_columns):\n",
    "    for z in list_of_date_columns:\n",
    "        if z in dataframe.columns:\n",
    "            print('{} {} dtype'.format(subset_string, z), type(dataframe[z].iloc[0]))\n",
    "            if type(dataframe[z].iloc[0]) != datetime.date:\n",
    "                print('Converting {} {} dtype...'.format(subset_string, z))\n",
    "                dataframe[z] = dataframe[z].apply(lambda x : date_parsing(x))\n",
    "                print('Done converting... checking...')\n",
    "                date_test = dataframe[~pd.isna(dataframe[z])][z].iloc[round(len(dataframe[~pd.isna(dataframe[z])])/2)]\n",
    "                print('     ',date_test, type(date_test),'\\n')\n",
    "        else:\n",
    "            print('Could not find {} in {}\\n'.format(z, subset_string))\n",
    "\n",
    "DTfix('df', df, [dfdob, dfdod, 'MAX_ENC_DATE'])\n",
    "DTfix('matchto', matchto, [matchtodob, matchtodod])\n",
    "\n",
    "print('\\nCleaning name columns...')\n",
    "\n",
    "def lower_spacerm_sub(x):\n",
    "    if pd.isna(x):\n",
    "        return ''\n",
    "    \n",
    "    if type(x) == str:\n",
    "        return str(x).lower().replace(' ', '').replace('-','').replace(',', '').replace(\"'\", '').replace('.', '')\n",
    "   \n",
    "def lower_spacerm(dataframe, list_of_name_columns):\n",
    "    for z in list_of_name_columns:\n",
    "        dataframe[z] = dataframe[z].apply(lambda x : lower_spacerm_sub(x))\n",
    "\n",
    "lower_spacerm(df, [dffn, dfmn, dfln])\n",
    "lower_spacerm(matchto, [matchtofn, matchtomn, matchtoln])\n",
    "\n",
    "\n",
    "print('Forming \"first_*\" columns') #occurs after lower_spacerm so don't have to reapply all that stuff!\n",
    "#functionalize this!!!\n",
    "def nameshorten(x, number_of_characters):\n",
    "    if not pd.isna(x):\n",
    "        return x[0:number_of_characters]\n",
    "    else:\n",
    "        return None\n",
    "df[dffn2] = df[dffn].apply(lambda x : nameshorten(x,2))\n",
    "df[dfln2] = df[dfln].apply(lambda x : nameshorten(x,2))\n",
    "matchto[matchtofn2] = matchto[matchtofn].apply(lambda x : nameshorten(x,2))\n",
    "matchto[matchtoln2] = matchto[matchtoln].apply(lambda x : nameshorten(x,2))\n",
    "\n",
    "df[dffn3] = df[dffn].apply(lambda x : nameshorten(x,3))\n",
    "df[dfln3] = df[dfln].apply(lambda x : nameshorten(x,3))\n",
    "df[dffn1] = df[dffn].apply(lambda x : nameshorten(x,1))\n",
    "df[dfln1] = df[dfln].apply(lambda x : nameshorten(x,1))\n",
    "\n",
    "matchto[matchtofn3] = matchto[matchtofn].apply(lambda x : nameshorten(x,3))\n",
    "matchto[matchtoln3] = matchto[matchtoln].apply(lambda x : nameshorten(x,3))\n",
    "matchto[matchtofn1] = matchto[matchtofn].apply(lambda x : nameshorten(x,1))\n",
    "matchto[matchtoln1] = matchto[matchtoln].apply(lambda x : nameshorten(x,1))\n",
    "\n",
    "\n",
    "#Forming address_1\n",
    "print('Forming \"address_1\" columns')\n",
    "\n",
    "def address_split(x):\n",
    "    x=str(x)\n",
    "    if not pd.isna(x) and x != 'nan' and x != '<NA>':\n",
    "        return x.split(' ')[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df[dfaddress_1] = df[dfaddress].apply(lambda x : address_split(x))    \n",
    "matchto[matchtoaddress_1] = matchto[matchtoaddress].apply(lambda x : address_split(x))  \n",
    "\n",
    "#Also dropping duplicate rows in both:\n",
    "def dupedrop(dataframe, subset_string, list_of_unid_columns):\n",
    "    prel = len(dataframe)\n",
    "    temp = dataframe.drop_duplicates()\n",
    "    postl = len(temp)\n",
    "    print('Dropped {} duplicates from {}'.format(prel-postl, subset_string))\n",
    "\n",
    "    for idcol in list_of_unid_columns:\n",
    "        f1list=dataframe[idcol].value_counts().to_frame()\n",
    "        #f2list=dataframe['F2'].value_counts().to_frame()\n",
    "        f1list = f1list[f1list['count'] == 2]\n",
    "        #f2list = f2list[f2list['count'] == 2]\n",
    "        print(len(f1list))\n",
    "        if len(f1list) > 0:\n",
    "            print('There is a duplicate in the {} column - see the following records:\\n {}'.format(idcol, f1list))\n",
    "\n",
    "    return temp\n",
    "\n",
    "print('\\n')\n",
    "df = dupedrop(df, 'df', dfunids)\n",
    "matchto = dupedrop(matchto, 'matchto', matchtounids)\n",
    "\n",
    "df_columns = list(df.columns) + ['nametag_df'] #locking columns here so we can filter using this later!\n",
    "print(df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [First Name, Middle Name, Last Name]\n",
      "Index: [] Empty DataFrame\n",
      "Columns: [First Name, Middle Name, Last Name]\n",
      "Index: []\n",
      "df - dropped 0 blank name rows. Start size: 347113, end size: 347113\n",
      "Empty DataFrame\n",
      "Columns: [F3, F4, F5]\n",
      "Index: [] Empty DataFrame\n",
      "Columns: [F3, F4, F5]\n",
      "Index: []\n",
      "matchto - dropped 0 blank name rows. Start size: 72138, end size: 72138\n"
     ]
    }
   ],
   "source": [
    "####### NEW ADDITION - DROPPING THOSE WHICH HAVE BLANK FIRST, MIDDLE, AND LAST NAMES!\n",
    "#from random import randint\n",
    "def dropblanknames(label, df, dffn, dfmn, dfln, targetcol):\n",
    "    toremove=list()\n",
    "    predflen = len(df)\n",
    "\n",
    "    df1 = df[((df[dffn] == '') & (df[dfmn] == '') & (df[dfln] == ''))]\n",
    "    badtest=df1\n",
    "    example1 = df1[[dffn, dfmn, dfln]].iloc[0:5]\n",
    "    toremove = toremove + (list(df1[targetcol].unique()))\n",
    "    df1 = df[((pd.isna(df[dffn])) & (pd.isna(df[dfmn])) & (pd.isna(df[dfln])))]\n",
    "    example2 = df1[[dffn, dfmn, dfln]].iloc[0:5]\n",
    "    toremove = toremove + (list(df1[targetcol].unique()))\n",
    "\n",
    "    \n",
    "    print(example1, example2)\n",
    "\n",
    "    df = df[~df[targetcol].isin(toremove)]\n",
    "\n",
    "    print('{} - dropped {} blank name rows. Start size: {}, end size: {}'.format(label, predflen-len(df), predflen, len(df)))\n",
    "    return df, badtest\n",
    "\n",
    "df, badtest = dropblanknames('df', df, dffn, dfmn, dfln, dfunids[0])\n",
    "matchto, badtest= dropblanknames('matchto', matchto, matchtofn, matchtomn, matchtoln, matchtotarget)\n",
    "\n",
    "del badtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making formatting for saves better - reduce manual fixing of columns and whatnot\n",
    "from openpyxl.utils.cell import get_column_letter\n",
    "import math\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl.formatting.rule import CellIsRule\n",
    "import xlsxwriter\n",
    "\n",
    "def highlight_column(worksheet, column, color, cols):\n",
    "                for row in worksheet.iter_rows(min_row = 2, max_row = worksheet.max_row, min_col = cols.index(column)+1, max_col = cols.index(column)+1):\n",
    "                    for cell in row:\n",
    "                        cell.fill = PatternFill(fill_type='solid', start_color = color, end_color = color)\n",
    "\n",
    "def formatted_save_wc(savepath, list_of_dfs):\n",
    "    print('Beginning save ({})'.format(datetime.datetime.now()))\n",
    "    list_of_sheets = ['good', 'mid', 'bad']\n",
    "    #Make tuple of df and sheet_name. Used to be dict but tuple is better for handling df w/ data inside\n",
    "    sheetdfdict = list(zip(list_of_dfs, list_of_sheets))\n",
    "    #Save raw workbook:\n",
    "    msnv2threshold = 75 \n",
    "    with pd.ExcelWriter(savepath, engine = 'openpyxl') as writer:\n",
    "        \n",
    "        #Save dataframes to corresponding sheets\n",
    "        for df, sheet in sheetdfdict:\n",
    "            df = df.drop(columns= [i for i in df.columns if 'blankcol' in i.lower()])\n",
    "            if len(df) > 1040000:\n",
    "                if len(df[df['modified_namesim_v2'] > msnv2threshold]) < 1040000:\n",
    "                    df[df['modified_namesim_v2'] > msnv2threshold].to_excel(writer, sheet_name = sheet, index = False)\n",
    "                else:\n",
    "                    df = df.sort_values(by='modified_namesim_v2', ascending=False)\n",
    "                    df = df.iloc[0:1040000] #take 1.04M first rows when sorted by modified namesim v2\n",
    "                    df.to_excel(writer, index = False, sheet_name = sheet)\n",
    "                    #raise Exception('Could not save df of size {}. When limiting by msnv2 > {}: {}'.format(len(df), msnv2threshold, len(df[df['modified_namesim_v2'] > msnv2threshold])))\n",
    "                    print('Could not save df of size {}. When limiting by msnv2 > {}: {}. Took highest 1.04M rows (by msnv2)'.format(len(df), msnv2threshold, len(df[df['modified_namesim_v2'] > msnv2threshold])))\n",
    "            else:\n",
    "                df.to_excel(writer, sheet_name = sheet , index = False)\n",
    "            \n",
    "            #workbook = writer.book\n",
    "            worksheet = writer.sheets[sheet]\n",
    "            \n",
    "            cols = list(df.columns)\n",
    "            \n",
    "            if not df.empty:\n",
    "                highlight_column(worksheet, 'modified_namesim_v2', 'fff0b8ff', cols)\n",
    "                highlight_column(worksheet, 'dobsim', 'ffbed0e4', cols)\n",
    "                if 'dobsim_2' in cols:\n",
    "                    highlight_column(worksheet, 'dobsim_2', 'ffbed0e4', cols)\n",
    "                highlight_column(worksheet, 'dodsim', 'ffb5ff61', cols)\n",
    "            \n",
    "                #Addresses:\n",
    "                addsim_letter = get_column_letter(cols.index('addsim')+1)\n",
    "                #arrestlocsim_letter = get_column_letter(cols.index('arrestlocsim')-1)\n",
    "                cross_addsim_letter = get_column_letter(cols.index('cross_addsim')+1)\n",
    "\n",
    "                addresses_range = addsim_letter + str(2) + ':' + cross_addsim_letter + str(len(df)+1)#as it would be shown in excel also add one at end to skip column name row\n",
    "                condrule = CellIsRule(operator = 'greaterThan', formula = ['89.9'], fill=PatternFill(start_color = 'FFFFC7CE', end_color = 'FFFFC7CE', fill_type='solid'))\n",
    "                worksheet.conditional_formatting.add(addresses_range, condrule)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW ADDITION: forming \"combo remove df\" with incorrect known combinations:\n",
    "def combo_remover(df, combos_to_remove):\n",
    "    prel = len(df)\n",
    "    df=df.copy()\n",
    "    print()\n",
    "    #checking\n",
    "    if 'combinedid' not in df.columns:\n",
    "        df['combinedid'] = df[dfunids[0]]+\"_\"+df[matchtotarget]\n",
    "    if not dfunids[0] in df.columns:\n",
    "        raise Exception('Could not find {} column'.format(dfunids[0]))\n",
    "    if not matchtotarget in df.columns:\n",
    "        raise Exception('Could not find {} column'.format(matchtotarget))\n",
    "    \n",
    "    print(df['combinedid'].iloc[0])\n",
    "    print(df.columns)\n",
    "    \n",
    "    df = df[~df['combinedid'].isin(combos_to_remove)]\n",
    "    df = df.drop(columns = ['combinedid'])\n",
    "    print('Dropped {} known bad matches.'.format(prel-len(df)))\n",
    "    return df\n",
    "global combos_to_remove\n",
    "combos_to_remove = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\combos_to_remove_deathcert.csv\", sep = ',', header = 0)\n",
    "combos_to_remove = list(combos_to_remove['combinedid'].values)\n",
    "#usage: df = combo_remover(df, combos_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPED:\n",
      "\n",
      "df: 347113 to 347112 (1)\n",
      "matchto: 72138 to 72123 (15)\n",
      "            count\n",
      "tempname         \n",
      "norsubject      1\n",
      "          count\n",
      "tempname       \n",
      "johndoe      11\n",
      "janedoe       4\n"
     ]
    }
   ],
   "source": [
    "#Dropping john does and jane does\n",
    "def namecombiner(fn, mn, ln):\n",
    "    if not pd.isna(fn) and not pd.isna(ln):\n",
    "        return ''.join([fn.lower().replace(' ', ''), ln.lower().replace(\" \", '')])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "### Trying with middle name first\n",
    "df['tempname'] = df.apply(lambda x : namecombiner(x[dffn], x[dfmn], x[dfln]), axis = 1)\n",
    "predf = len(df)\n",
    "matchto['tempname'] = matchto.apply(lambda x : namecombiner(x[matchtofn], x[matchtomn], x[matchtoln]), axis = 1)\n",
    "\n",
    "##out of curiousity:\n",
    "# print(df['tempname'].value_counts().to_frame())\n",
    "# print(matchto['tempname'].value_counts().to_frame())\n",
    "# print('\\n')\n",
    "\n",
    "prematchto = len(matchto)\n",
    "\n",
    "excludelist = ['doejohn', 'doejane', 'male', 'female', 'john', 'johndoe', 'maleunknown', 'novsubject', 'norsubject',\n",
    "               'jane', 'janejoe', 'doejoe', 'ukn', 'uknukn', 'janedoe', 'unkunk', 'unknownmaleunknownmale', 'nolsubject',\n",
    "               'maleunknown', 'unknownmale', 'unknown', 'femaleunknown', 'unknownfemale', 'unkmale', 'jondoe',\n",
    "               'maleuknown', 'uknownmale', 'femaleuknown', 'uknownfemale', 'unknownunknown', 'johnunknown', 'unkownmale',\n",
    "               'undetermineddoe', 'janedoeu', 'johnjanedoe', 'skeletaldoe', 'doe', 'creekjdoe', 'janejohndoe', 'jhondoe',\n",
    "               'humanremains', 'nonhumanremains', 'nonhumanbones', 'nonhumanbone', 'bonesnonhuman', 'bonenonhuman',\n",
    "               'bone', 'humanskull', 'remainsnonhuman', 'samecaseas(0123210)teachingspecimen', 'humantooth',\n",
    "               'cremainsunidentified', 'unidentifiedcremains', 'longbonenonhuman', 'nonhumanlongbone', 'unidentifiedhumanremains',\n",
    "               'remainsunidentifiedhuman', 'malefetus', 'skeletonteaching', 'teachingskeleton', 'femalefetus']\n",
    "\n",
    "baddf = df[df['tempname'].isin(excludelist)]\n",
    "badmatchto = matchto[matchto['tempname'].isin(excludelist)]\n",
    "\n",
    "df = df[~df['tempname'].isin(excludelist)]\n",
    "matchto = matchto[~matchto['tempname'].isin(excludelist)]\n",
    "#matchto = matchto[(matchto['tempname'] != 'doejohn') & (matchto['tempname'] != 'doejane')]\n",
    "print('DROPPED:\\n')\n",
    "print('df: {} to {} ({})'.format(predf, len(df), abs(len(df)-predf)))\n",
    "print('matchto: {} to {} ({})'.format(prematchto, len(matchto), abs(len(matchto)-prematchto)))\n",
    "\n",
    "\n",
    "print(baddf['tempname'].value_counts().to_frame())\n",
    "#print(df['tempname'].value_counts(dropna=False)[df['tempname'].value_counts(dropna=False) > 5 ])\n",
    "#print(matchto['tempname'].value_counts(dropna=False)[matchto['tempname'].value_counts(dropna=False) > 5 ])\n",
    "print(badmatchto['tempname'].value_counts().to_frame())\n",
    "\n",
    "df = df.drop(columns = 'tempname')\n",
    "matchto = matchto.drop(columns = 'tempname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See below:\n",
      "df:\n",
      "  GENDER   count\n",
      "0      2  201747\n",
      "1      1  145365\n",
      "Precleaning entries: ['1', '2']\n",
      "\n",
      "matchto:\n",
      "  F19  count\n",
      "0   2  38546\n",
      "1   1  33570\n",
      "Precleaning entries: ['M', 'F', 'U', 'X']\n",
      "Last run at 2025-07-18 15:22:02.812748\n"
     ]
    }
   ],
   "source": [
    "## Checking gender columns:\n",
    "\n",
    "tdf = df[dfsex].value_counts().to_frame().reset_index()\n",
    "\n",
    "tm = matchto[matchtosex].value_counts().to_frame().reset_index()\n",
    "\n",
    "def gendertype(df, df_sex_col):\n",
    "    if 'M' in list(df[df_sex_col].values) or 'F' in list(df[df_sex_col].values) or 'Female' in list(df[df_sex_col].values) or 'Male' in list(df[df_sex_col].values):\n",
    "        df_type = 'alpha'\n",
    "    elif '1' in list(df[df_sex_col].values) or '2' in list(df[df_sex_col].values):\n",
    "        df_type = 'numeric'\n",
    "    elif 1 in list(df[df_sex_col].values) or 2 in list(df[df_sex_col].values):\n",
    "        df_type = 'numeric'\n",
    "    else:\n",
    "        raise Exception('Error deciding gender column nomenclature. Please check source file. Should be M/F or 1/2')\n",
    "        \n",
    "\n",
    "\n",
    "    return df_type\n",
    "print('See below:\\ndf:\\n{}\\nPrecleaning entries: {}\\n\\nmatchto:\\n{}\\nPrecleaning entries: {}'.format(tdf,preclean_gender_df,tm, preclean_gender_matchto))\n",
    "if 'blankcol' not in dfsex.lower() and 'blankcol' not in matchtosex.lower():\n",
    "    if gendertype(tdf, dfsex) != gendertype(tm, matchtosex):\n",
    "        raise Exception('Potential error in gender nomenclature, see above. Mismatch between alpha and numeric styles.')\n",
    "\n",
    "    del tdf, tm\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not find MAX(CONTACT_DATE) in df columns\n",
      "Last run at 2025-07-18 15:22:02.826188\n"
     ]
    }
   ],
   "source": [
    "### Special step for last encounter date parsing\n",
    "if 'MAX(CONTACT_DATE)' in df.columns:\n",
    "    DTfix('df', df, ['MAX(CONTACT_DATE)'])\n",
    "else:\n",
    "    print('Did not find MAX(CONTACT_DATE) in df columns')\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guillermo  vazquez - 2\n",
      "deborah beth goldin - 3\n",
      "natalie c do - 3\n",
      "pate braga stevens - 3\n",
      "michael norwin byron - 3\n",
      "bobby  caldwell - 2\n",
      "lila  barkhordarian - 2\n",
      "victoria nadinecristina berenbau - 3\n",
      "brian howard berkowitz - 3\n",
      "jaenam jay coe - 3\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ernesto  hernandezchavez - 2\n",
      "blake joseph armas - 3\n",
      "howard j berenson - 3\n",
      "ira cary fischer - 3\n",
      "john harry lopezdevictoria - 3\n",
      "vivian ifeoma laniyi - 3\n",
      "marlene  cedeno - 2\n",
      "rogelio  vargasaguilar - 2\n",
      "retha marie casillas - 3\n",
      "ora mae knowell - 3\n",
      "Last run at 2025-07-18 15:22:09.079239\n"
     ]
    }
   ],
   "source": [
    "### Tagging for name component presence\n",
    "def nametag(x, fncol, lncol, mncol):\n",
    "    count=0\n",
    "    if not pd.isna(x[fncol]) and x[fncol] != '' and x[fncol] != '-' and x[fncol] != ' ':\n",
    "        count=count+1\n",
    "    if not pd.isna(x[lncol]) and x[lncol] != '' and x[lncol] != '-' and x[lncol] != ' ':\n",
    "        count=count+1\n",
    "    if not pd.isna(x[mncol]) and x[mncol] != '' and x[mncol] != '-' and x[mncol] != ' ':\n",
    "        count=count+1\n",
    "    \n",
    "    return count\n",
    "\n",
    "df['nametag_df'] = df.apply(lambda x : nametag(x, dffn, dfln, dfmn), axis = 1)\n",
    "matchto['nametag_matchto'] = matchto.apply(lambda x : nametag(x, matchtofn, matchtoln, matchtomn), axis = 1)\n",
    "#20 sec run time for 23-24\n",
    "#2 min 42 sec run time for 16-22\n",
    "\n",
    "df.index = range(0,len(df))\n",
    "matchto.index = range(0,len(matchto))\n",
    "\n",
    "# Checking a couple entries for nametag column\n",
    "# Use if you want to double check\n",
    "import random\n",
    "ranlist = [random.randint(0,len(df)) for i in range(0,10)]\n",
    "for k in ranlist:\n",
    "    print(df[dffn].iloc[k], df[dfmn].iloc[k], df[dfln].iloc[k], '-', df['nametag_df'].iloc[k])\n",
    "print('-'*200)\n",
    "ranlist2 = [random.randint(0,len(matchto)) for i in range(0,10)]\n",
    "for k in ranlist2:\n",
    "    print(matchto[matchtofn].iloc[k], matchto[matchtomn].iloc[k], matchto[matchtoln].iloc[k], '-', matchto['nametag_matchto'].iloc[k])\n",
    "\n",
    "del ranlist, ranlist2, k\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Testing last encounter date stuff #### Deprecated 5/20/2025 - Priya suggests not dropping these as erroneous MAX_ENC_DATEs are commonplace.\n",
    "# maxcol = 'MAX_ENC_DATE'\n",
    "# if maxcol in df.columns:\n",
    "#     def OMG(x):\n",
    "#         if not pd.isna(x[maxcol]) and not pd.isna(x[dfdod]):\n",
    "#             if x[maxcol] <= x[dfdod]:\n",
    "#                 return 1\n",
    "#             else:\n",
    "#                 return 0 #bad\n",
    "#         else:\n",
    "#             return 'Error'\n",
    "\n",
    "#     df['LED'] = df.apply(lambda x : OMG(x), axis = 1)\n",
    "#     print(df['LED'].value_counts().to_frame())\n",
    "\n",
    "#     zero = df[df['LED'] == 0]\n",
    "#     df = df[df['LED'] != 0]\n",
    "#     df = df.drop(columns = ['LED'])\n",
    "# else:\n",
    "#     print('Could not find \"{}\" in df columns'.format(maxcol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CREATING GOOD OUTPUT FOR MANUAL ADJUD SECTIONS:\n",
    "good_final = pd.DataFrame()\n",
    "dropvar = 1\n",
    "do = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TEMPORARY - REMOVING GOOD MATCHES FROM OLD MATCH RUN (finished 12/2/2024 =D)\n",
    "# tout = pd.read_excel(r\"C:\\Users\\MathiasM\\OneDrive - Cedars-Sinai Health System\\Documents\\good_full.xlsx\")\n",
    "# print(tout.shape)\n",
    "# tout['F1'] = tout['F1'].apply(lambda x : str(x))\n",
    "# print(df.shape, matchto.shape)\n",
    "# df = df[~df[dfunids[0]].isin(tout[dfunids[0]].values)]\n",
    "# matchto = matchto[~matchto[matchtotarget].isin([tout[matchtotarget].values])]\n",
    "# print(df.shape, matchto.shape)\n",
    "###########\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading important functions for matching (JUST FUNCTION LOADING)\n",
    "import jellyfish\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os \n",
    "from termcolor import cprint\n",
    "def trying(s1, s2):\n",
    "    if pd.isna(s1):\n",
    "        s1 = ''\n",
    "    if pd.isna(s2):\n",
    "        s2 = ''\n",
    "\n",
    "    if len(s1) == 1 or len(s2) == 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        if len(s1) > len(s2):\n",
    "            diff = s1.replace(s2, '')\n",
    "            if len(diff) == len(s1): #subset not taken, so they are not a type1?\n",
    "                diff = np.nan\n",
    "        elif len(s1) < len(s2):\n",
    "            diff = s2.replace(s1, '')\n",
    "            if len(diff) == len(s2): #subset not taken, so they are not a type1?\n",
    "                diff = np.nan\n",
    "        elif s1 == s2:\n",
    "            diff = np.nan\n",
    "        else: #if s1 and s2 are the same length, but different names I guess?\n",
    "            diff = np.nan\n",
    "    \n",
    "        return diff\n",
    "\n",
    "def removing(name_piece, list_to_remove):\n",
    "    for item in list_to_remove:\n",
    "        #a=name_piece\n",
    "        name_piece = name_piece.replace(item, '')\n",
    "        #print(a,'-', item, '=',name_piece)\n",
    "        \n",
    "    return name_piece.replace(' ', '')\n",
    "\n",
    "###### VERSION 2 WITH NEW SPLITTING TECHNIQUE IN PLACE:\n",
    "\n",
    "def inlist(missinglist, list_to_compare):\n",
    "    outlist = list()\n",
    "    for missing in missinglist:\n",
    "        for name_piece in list_to_compare: #consider using any(item for name in name thing again here? Test in another chunk tho fr fr)\n",
    "            if missing in name_piece:\n",
    "                outlist.append(missing)\n",
    "    \n",
    "    return outlist ###Figure this out tomorrow - want to show which list (mlist or dlist) we should ADD the vfn, vmn, vln values to, don't want to add to BOTH, just to the one it came from so we can have a better idea!\n",
    "\n",
    "\n",
    "def fnmnln2(dfn, dmn, dln, mfn, mmn, mln):\n",
    "    #dfn, dmn, dln, mfn, mmn, mln\n",
    "    vfn = trying(dfn, mfn)\n",
    "    vmn = trying(dmn, mmn)\n",
    "    vln = trying(dln, mln)\n",
    "    #global missinglist\n",
    "    # print(dfn, mfn,'|', vfn)\n",
    "    # print(dmn, mmn,'|', vmn)\n",
    "    # print(dln, mln,'|', vln)\n",
    "    \n",
    "    \n",
    "    if not pd.isna(dmn) and not pd.isna(mmn) and dmn != '' and mmn != '':\n",
    "        if len(dmn) == 1 or len(mmn) == 1:\n",
    "            dmn = dmn[0]\n",
    "            mmn = mmn[0]\n",
    "    \n",
    "    #global missinglist\n",
    "    missinglist = [i for i in list(set([vfn, vmn, vln])) if not pd.isna(i)]\n",
    "    missinglist_m = inlist(missinglist, [mfn, mmn, mln])\n",
    "    missinglist_d = inlist(missinglist, [dfn, dmn, dln])\n",
    "    #print(missinglist)\n",
    "    #global tdc, outdict\n",
    "    tdc = pd.DataFrame(columns = ['diff'], data =[i for i in [vfn, vmn, vln] if not pd.isna(i)])\n",
    "    for p in list(set([i for i in ([removing(dfn,missinglist), removing(dmn,missinglist),removing(dln,missinglist), removing(mfn,missinglist), removing(mmn,missinglist),removing(mln,missinglist)]) if i != ''])):\n",
    "        tdc.insert(tdc.shape[1], p, None)\n",
    "        tdc[p] = tdc.apply(lambda x : calc_distances_jw(p,x['diff']), axis = 1)\n",
    "    #print(tdc)\n",
    "    outdict = {}\n",
    "    if not tdc.empty:\n",
    "        for p in range(0,len(tdc)):\n",
    "            pp = tdc.iloc[p].to_dict()\n",
    "            name1=pp['diff']\n",
    "            del pp['diff']\n",
    "        #print(name1, [k for k,v in pp.items() if v == max(pp.values())][0])\n",
    "        if max(pp.values()) > 95:\n",
    "            outdict[name1] = [k for k,v in pp.items() if v == max(pp.values())][0] #see if there is another piece of any other name section which has a similarity score over 95. 100%s should not occur because of 'removing' application\n",
    "        \n",
    "        \n",
    "\n",
    "        if len(outdict.keys()) > 0: #if outdict is not empty - this means there is a likely typo/substitution that needs to be made. Can either replace with key or value, should be the same, I think?\n",
    "            print('Initiating swap:',outdict)\n",
    "            for p in outdict.keys():\n",
    "                dfn = dfn.replace(outdict[p], p) #removing value and replacing with dict key\n",
    "                dmn = dmn.replace(outdict[p], p)\n",
    "                dln = dln.replace(outdict[p], p)\n",
    "\n",
    "                mfn = mfn.replace(outdict[p], p)\n",
    "                mmn = mmn.replace(outdict[p], p)\n",
    "                mln = mln.replace(outdict[p], p)\n",
    "        \n",
    "\n",
    "    # print('Split df:', dsplit:=''.join(sorted([i for i in ([removing(dfn,missinglist), removing(dmn,missinglist),removing(dln,missinglist)]+missinglist) if i != ''])))\n",
    "    # print('Split mt:', msplit:=''.join(sorted([i for i in ([removing(mfn,missinglist), removing(mmn,missinglist),removing(mln,missinglist)]+missinglist) if i != ''])))\n",
    "\n",
    "    # dsplit = list(set([i for i in ([removing(dfn,missinglist), removing(dmn,missinglist),removing(dln,missinglist)]+missinglist) if i != '']))\n",
    "    # msplit = list(set([i for i in ([removing(mfn,missinglist), removing(mmn,missinglist),removing(mln,missinglist)]+missinglist) if i != '']))\n",
    "    #global dsplit, msplit\n",
    "    dsplit = list(set([i for i in ([removing(dfn,missinglist), removing(dmn,missinglist),removing(dln,missinglist)]) if i != '']+missinglist_d+list(outdict.keys())))\n",
    "    msplit = list(set([i for i in ([removing(mfn,missinglist), removing(mmn,missinglist),removing(mln,missinglist)]) if i != '']+missinglist_m+list(outdict.keys())))\n",
    "\n",
    "    #missinglist2 = [i for i in dsplit if i not in msplit] + [i for i in msplit if i not in dsplit]\n",
    "\n",
    "    #print('Split df:', dsplit, '-->', ''.join(sorted(dsplit)))\n",
    "    #print('Split mt:', msplit, '-->', ''.join(sorted(msplit)))\n",
    "    dsplit_joined = ''.join(sorted(dsplit))\n",
    "    msplit_joined = ''.join(sorted(msplit))\n",
    "\n",
    "    #print('missinglist2:',missinglist2)\n",
    "\n",
    "    if dsplit_joined == msplit_joined:\n",
    "        return dsplit_joined, msplit_joined, 'type1a*'\n",
    "\n",
    "    #if vfn == vmn: #if piece of name is present in other part of name: e.g 'ann' in: mary annfrances brooks  +  maryann frances brooks\n",
    "        #return ''.join([dfn.replace(vfn, ''), dmn.replace(vfn, ''), dln.replace(vfn, ''), vfn]), ''.join([mfn.replace(vfn, ''), mmn.replace(vfn, ''), mln, vfn]), 'type1a'\n",
    "    #elif vfn == vln:\n",
    "        #return ''.join([dfn.replace(vfn, ''), dmn.replace(vfn, ''), dln.replace(vfn, ''), vfn]), ''.join([mfn.replace(vfn, ''), mmn.replace(vfn, ''), mln.replace(vfn, ''), vfn]), 'type1b'\n",
    "    #elif vln == vmn:\n",
    "       #return ''.join([dfn.replace(vln, ''), dmn.replace(vln, ''), dln.replace(vln, ''), vln]), ''.join([mfn.replace(vln, ''), mmn.replace(vln, ''), mln.replace(vln, ''), vln]), 'type1c'\n",
    "    elif dfn == mfn and dln == mln and mmn != dmn and (mmn != '' and dmn != '') and pd.isna(vmn):\n",
    "        #Different middle name, but same fn and ln\n",
    "        return ''.join([dfn, dln, dmn]), ''.join([mfn, mln, mmn]), 'Diff_MN'\n",
    "    else: #other\n",
    "        #testing = [i for i in [dfn, dmn, dln] if i not in [mfn, mmn, mln] and i != ''] + [i for i in [mfn, mmn, mln] if i not in [dfn, dmn, dln] and i != ''] #getting names only present in one or the other\n",
    "        \n",
    "        #print(Counter([i for i in [dfn, dmn, dln] if i != '']))\n",
    "       #print(ditest:=Counter([i for i in (dsplit+msplit) if i != '']))\n",
    "        #print(testing:=[i for i,v in ditest.items() if v < 2])\n",
    "        ditest=Counter([i for i in (dsplit+msplit) if i != ''])\n",
    "        testing=[i for i,v in ditest.items() if v < 2]\n",
    "\n",
    "        if len(testing) < 2: #if we are only excluding ONE name (leaving two in tact, hopefully)\n",
    "            #print('to remove:',testing)\n",
    "            #return ''.join(sorted([removing(dfn, testing), removing(dmn, testing), removing(dln, testing)])), ''.join(sorted([removing(mfn, testing), removing(mmn, testing), removing(mln, testing)])), 'type4'\n",
    "            return ''.join(sorted([removing(i,testing) for i in dsplit])), ''.join(sorted([removing(i,testing) for i in msplit])), 'Type4'\n",
    "        else: #if we are excluding more than one name - likely two different people - I would hope?\n",
    "            #return ''.join([dfn, dmn, dln]), ''.join([mfn, mmn, mln]), 'Other'\n",
    "            return dsplit_joined, msplit_joined, 'Other'\n",
    "fnmnln_vec = np.vectorize(fnmnln2)\n",
    "\n",
    "to_ignore = ['pobox', 'pobx', '', None, np.isnan, 'unknown', 'unk']\n",
    "\n",
    "def calc_distances_jw(df_string, matchto_string): ###### CURRENT MATCHING MODULE\n",
    "    if not pd.isna(df_string) and not pd.isna(matchto_string) and df_string != '' and matchto_string != '' and df_string not in to_ignore and matchto_string not in to_ignore:\n",
    "        df_string=str(df_string)\n",
    "        matchto_string=str(matchto_string)\n",
    "        jelly_jaro_winkler = round(100*jellyfish.jaro_winkler_similarity(df_string, matchto_string),3)\n",
    "        return jelly_jaro_winkler\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def joining(list_of_items, sep): #v2 5/20/25\n",
    "    list_of_items2 = [i for i in list_of_items if not pd.isna(i)]\n",
    "    list_of_items2 = [str(i).replace('.0', '') for i in list_of_items2]\n",
    "    try:\n",
    "        if len(list_of_items2) > 0:\n",
    "            return sep.join(list_of_items2)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "            print(list_of_items, list_of_items2)\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def date_diffs(x, v): ##### CURRENT DAY distance module to use\n",
    "    if not pd.isna(x) and not pd.isna(v):\n",
    "        return abs((x-v).total_seconds()/86400) #number of seconds in one day (24*60*60)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def date_diffs_noabs(x, v): ##### CURRENT DAY distance module to use\n",
    "    if not pd.isna(x) and not pd.isna(v):\n",
    "    #if type(x) == datetime.date and type(v) == datetime.date:\n",
    "        return ((x-v).total_seconds()/86400) #number of seconds in one day (24*60*60)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# def modified_namesim(x):\n",
    "#     if x['nametag_match'] == 'not equal':\n",
    "#         return calc_distances_jw(joining([x[dffn], x[dfln]], 'SORT'), joining([x[matchtofn], x[matchtoln]], 'SORT'))\n",
    "#     else:\n",
    "#         #now check for middle name initialism\n",
    "#         if not pd.isna(x[dfmn]) and not pd.isna(x[matchtomn]) and x[dfmn] != '' and x[matchtomn] != '':\n",
    "#             #now two cases: if either middle name is an initial, trim the other one [0]., else compare dfname and matchtoname\n",
    "#             if len(x[dfmn]) == 1 or len(x[matchtomn]) == 1:\n",
    "#                 return calc_distances_jw(joining([x[dffn], x[dfmn][0], x[dfln]], 'SORT'), joining([x[matchtofn], x[matchtomn][0], x[matchtoln]], 'SORT'))\n",
    "#             else:\n",
    "#                 return calc_distances_jw(x['dfname'], x['matchtoname'])\n",
    "#         else: #same as above, if one in missing the middle name section - just in case\n",
    "#             return calc_distances_jw(joining([x[dffn], x[dfln]], 'SORT'), joining([x[matchtofn], x[matchtoln]], 'SORT'))\n",
    "\n",
    "#Took from EMS matching, not sure why 'SORT' is in this?\n",
    "def modified_namesim(x):\n",
    "    if x['nametag_match'] == 'not equal':\n",
    "        return calc_distances_jw(joining(sorted([x[dffn], x[dfln]]), ''), joining(sorted([x[matchtofn], x[matchtoln]]), ''))\n",
    "    else:\n",
    "        #now check for middle name initialism\n",
    "        if not pd.isna(x[dfmn]) and not pd.isna(x[matchtomn]) and x[dfmn] != '' and x[matchtomn] != '':\n",
    "            #now two cases: if either middle name is an initial, trim the other one [0]., else compare dfname and matchtoname\n",
    "            if len(x[dfmn]) == 1 or len(x[matchtomn]) == 1:\n",
    "                return calc_distances_jw(joining(sorted([x[dffn], x[dfmn][0], x[dfln]]), ''), joining(sorted([x[matchtofn], x[matchtomn][0] ,x[matchtoln]]), ''))\n",
    "            else:\n",
    "                return calc_distances_jw(x['dfname'], x['matchtoname'])\n",
    "        else: #same as above, if one in missing the middle name section - just in case\n",
    "            return calc_distances_jw(joining(sorted([x[dffn], x[dfln]]), ''), joining(sorted([x[matchtofn], x[matchtoln]]), ''))\n",
    "\n",
    "def address_clean(x): #fails for coordinates or any cell which is just numbers ##updated 5/20/25\n",
    "    x = str(x)\n",
    "    if not pd.isna(x):  \n",
    "        x=x.lower()\n",
    "        x=x.replace(' ', '')\n",
    "        x=x.replace('-', '')\n",
    "        x=x.replace('apt', '')\n",
    "        x=x.replace('room', '')\n",
    "        x=x.replace('suite', '')\n",
    "        x=x.replace('.','')\n",
    "        x=x.replace('avenue', 'ave')\n",
    "        x=x.replace('drive', 'dr')\n",
    "        x=x.replace('street', 'st')\n",
    "        x=x.replace('saint', 'st')\n",
    "        x=x.replace('boulevard', 'bl').replace('blvd', 'bl')\n",
    "        x=x.replace('heights', 'hgts')\n",
    "        x=x.replace('canyon', 'cyn')\n",
    "        x=x.replace('road', 'rd')\n",
    "        x=x.replace('court', 'ct')\n",
    "        x=x.replace('place', 'pl')\n",
    "        x=x.replace('trail', 'trl')\n",
    "        x=x.replace('parkway', 'pwky')\n",
    "        x=x.replace('park', 'pk')\n",
    "        x=x.replace('penthouse', '')\n",
    "        x=x.replace('circle', 'cir')\n",
    "        x=x.replace('#', '')\n",
    "        x=x.replace('unit', '')\n",
    "        x=x.replace('lane', 'ln')\n",
    "        x=x.replace('1/2', '').replace(\"1/4\", '')\n",
    "        x=x.replace('way', 'wy')\n",
    "        x=x.replace('south', 's')\n",
    "        x=x.replace('north', 'n')\n",
    "        x=x.replace('east', 'e')\n",
    "        x=x.replace('west', 'w')\n",
    "        x=x.replace(',', '')\n",
    "        x=x.replace('.', '')\n",
    "        x=x.replace(\"*nd\", '')\n",
    "        x=x.replace('*na', '')\n",
    "        x=x.replace('scn', '')\n",
    "\n",
    "        if x == '123defaultst':\n",
    "            return None\n",
    "        if x in ['123defaultst', 'transient', 'homeless', '<na>', 'uto']:\n",
    "            return None\n",
    "\n",
    "        if not pd.isna(x) and x != '' and x != 'none':\n",
    "            while len(x) > 0 and (str(x)[-1]).isdigit(): #take of ending numbers\n",
    "                x=x[0:-1]\n",
    "            #now do special checks:\n",
    "            #if x[-3:] == 'ave':\n",
    "                #x=x[0:-3]\n",
    "                #return x\n",
    "            #else:\n",
    "            return x\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def fullsim(indf): \n",
    "    if indf.empty:\n",
    "        print('Empty df passed.')\n",
    "        return indf\n",
    "    #Can probably remove some of these formations and put them \n",
    "    #external to the function so that they are not called each time\n",
    "\n",
    "    #Cleaning middle names\n",
    "    #indf = indf[[dffn,dfmn,dfln,matchtofn,matchtomn,matchtoln, dfsex, matchtosex,'nametag_df', 'nametag_matchto', dfdob, matchtodob, matchtodod, matchtodoa, dfdod, matchtoarrestloc_a, matchtoarrestloc_b, dfarrestloc_a, dfarrestloc_b, 'PAT_ID', matchtotarget, 'ADD_LINE_1',matchtoaddress, matchtoaddress2]]\n",
    "    indf[dfmn] = indf[dfmn].apply(lambda x : x.replace('.', '') if not pd.isna(x) else '')\n",
    "    indf[matchtomn] = indf[matchtomn].apply(lambda x : x.replace('.', '') if not pd.isna(x) else '')\n",
    "\n",
    "    #Forming FULL name column\n",
    "    print(\"Forming FULL name columns\", datetime.datetime.now())\n",
    "    indf.loc[:,'dfname'] = indf.apply(lambda x : joining([x[dffn], x[dfmn], x[dfln]], ''), axis = 1)\n",
    "    indf.loc[:,'matchtoname'] = indf.apply(lambda x : joining([x[matchtofn], x[matchtomn], x[matchtoln]], ''), axis = 1)\n",
    "   \n",
    "    #Forming address columns\n",
    "    print(\"Forming address columns\", datetime.datetime.now())\n",
    "    try:\n",
    "        indf.loc[:,'nametag_match'] = indf.apply(lambda x : 'equal' if x['nametag_df'] == x['nametag_matchto'] else 'not equal', axis = 1)\n",
    "    except:\n",
    "        pass\n",
    "    indf.loc[:,'matchtoaddress'] = indf.apply(lambda x : joining(([x[matchtoaddress], x[matchtoaddress2]]), ''), axis = 1) #forming combined address\n",
    "\n",
    "    indf.loc[:,'matchtoarrestloc'] = indf.apply(lambda x : joining(([x[matchtoarrestloc_a], x[matchtoarrestloc_b]]), ''), axis = 1)\n",
    "    indf.loc[:,'dfarrestloc'] =  indf.apply(lambda x : joining(([x[dfarrestloc_a], x[dfarrestloc_b]]), ''), axis = 1)\n",
    "\n",
    "    #Cleaning addresses\n",
    "    print(\"Cleaning addresses\", datetime.datetime.now())\n",
    "    indf.loc[:,'matchtoaddress'] = indf['matchtoaddress'].apply(lambda x : address_clean(x))\n",
    "\n",
    "    indf.loc[:,dfaddress] = indf[dfaddress].apply(lambda x : address_clean(x))\n",
    "    indf.loc[:,'matchtoarrestloc'] = indf['matchtoarrestloc'].apply(lambda x : address_clean(x))\n",
    "    indf.loc[:,'dfarrestloc'] = indf['dfarrestloc'].apply(lambda x : address_clean(x))\n",
    "\n",
    "    #Calculating similarity scores - testing .loc now\n",
    "    print(\"Calculating similarity scores\", datetime.datetime.now())\n",
    "    indf.loc[:,'dobsim'] = indf.apply(lambda x : calc_distances_jw(x[dfdob], x[matchtodob]), axis = 1)\n",
    "    print('     dobsim done')\n",
    "    indf.loc[:,'namesim'] = indf.apply(lambda x : calc_distances_jw(x['dfname'], x['matchtoname']), axis = 1)\n",
    "    print('     namesim done')\n",
    "    indf.loc[:,'modified_namesim'] = indf.apply(lambda x : modified_namesim(x), axis = 1) #this one has all the \"initiating swaps\" stuff\n",
    "    #indf.loc[:,'modified_namesim'] = ''\n",
    "    print('     modified_namesim done')\n",
    "\n",
    "    \n",
    "    print(\"Calculating modified namesim v2\", datetime.datetime.now())\n",
    "    indf.loc[:,'dname'], indf['mname'], indf['type'] = fnmnln_vec(indf[dffn], indf[dfmn], indf[dfln], indf[matchtofn], indf[matchtomn], indf[matchtoln])\n",
    "    print('     modified_namesim done')\n",
    "    indf.loc[:,'modified_namesim_v2'] = indf.apply(lambda x : calc_distances_jw(x['dname'], x['mname']), axis = 1)\n",
    "    print('     modified_namesim_v2 done')\n",
    "\n",
    "    print(\"Calculating address similarity scores\", datetime.datetime.now())\n",
    "    indf.loc[:,'addsim'] = indf.apply(lambda x : calc_distances_jw(x[matchtoaddress], x[dfaddress]), axis = 1)\n",
    "    print('     addsim done')\n",
    "    indf.loc[:,'arrestlocsim'] = indf.apply(lambda x : calc_distances_jw(x['dfarrestloc'], x['matchtoarrestloc']), axis = 1)\n",
    "    print('     arrestlocsim done')\n",
    "    indf.loc[:,'cross_addsim'] = indf.apply(lambda x : calc_distances_jw(x[dfaddress], x['matchtoarrestloc']), axis = 1)\n",
    "    print('     cross_addsim done')\n",
    "\n",
    "    print(\"Calculating auxiliary date similarities\", datetime.datetime.now())\n",
    "    indf.loc[:,'dodsim'] = indf.apply(lambda x : calc_distances_jw(x[dfdod], x[matchtodod]), axis = 1)\n",
    "    print('     dodsim done')\n",
    "    indf.loc[:,'doddoasim'] = indf.apply(lambda x : calc_distances_jw(x[matchtodoa], x[dfdod]), axis = 1)\n",
    "    print('     doddoasim done')\n",
    "    if 'MAX_ENC_DATE' in indf.columns:\n",
    "        indf.loc[:,'doa_maxenc_sim'] = indf.apply(lambda x : calc_distances_jw(x[matchtodoa], x['MAX_ENC_DATE']), axis = 1)\n",
    "        print('     doa_maxenc_sim done')\n",
    "        indf.loc[:,'doa_maxenc_diff'] = indf.apply(lambda x : date_diffs_noabs(x[matchtodoa], x['MAX_ENC_DATE']), axis = 1)\n",
    "        print('     doa_maxenc_diff done')\n",
    "    else:\n",
    "        indf.loc[:,'doa_maxenc_sim'] = 0 #should be None? 0 is probably fine\n",
    "        print('     doa_maxenc_diff done')\n",
    "        indf.loc[:,'doa_maxenc_diff'] = 0\n",
    "        print('     doa_maxenc_sim done')\n",
    "    #indf['ssnsim'] = indf.apply(lambda x : calc_distances_jw(x[dfssn], x[matchtossn]), axis = 1)\n",
    "\n",
    "    #Calculating date differences\n",
    "    print(\"Calculating date differences\", datetime.datetime.now())\n",
    "    indf.loc[:,'dob_diff'] = indf.apply(lambda x : date_diffs(x[dfdob], x[matchtodob]), axis = 1)\n",
    "    print('     dob_diff done')\n",
    "    indf.loc[:,'dod_diff'] = indf.apply(lambda x : date_diffs(x[dfdod], x[matchtodod]), axis = 1)\n",
    "    print('     dod_diff done')\n",
    "    indf.loc[:,'doddoa_diff'] = indf.apply(lambda x : date_diffs_noabs(x[dfdod], x[matchtodoa]), axis = 1)\n",
    "    print('     dobdoa_diff done')\n",
    "\n",
    "    \n",
    "    ### EXPERIMENTAL\n",
    "    prel = len(indf)\n",
    "    indf = indf[indf['modified_namesim_v2'] > 40]\n",
    "    print('Dropped {} cases: {} to {} modified_namesim_v2 < 40'.format(prel-len(indf), prel, len(indf)))\n",
    "\n",
    "    prel = len(indf)\n",
    "    indf = indf.drop_duplicates()\n",
    "    print('Dropped {} duplicates: {} to {}'.format(prel-len(indf), prel, len(indf)))\n",
    "\n",
    "\n",
    "    if 'Remove' not in list(indf.columns):\n",
    "        indf.insert(indf.shape[1], 'Remove', None)\n",
    "\n",
    "    #if 'matchtossn' in list(locals()) and 'dfssn' in list(locals()): ### locals wont work here bc this is a function!!! \n",
    "        try:#if matchtossn in indf.columns and dfssn in indf.columns:\n",
    "            print(\"Calculating SSN similarities\", datetime.datetime.now())\n",
    "            indf.loc[:,'ssnsim'] = indf.apply(lambda x : calc_distances_jw(x[matchtossn], x[dfssn]), axis = 1)\n",
    "            manualreview = indf[['Remove', dffn, dfmn, dfln, matchtofn, matchtomn, matchtoln, dfunids[0], matchtotarget, \n",
    "                         dfsex, matchtosex, 'dfname', 'matchtoname', 'nametag_match', \n",
    "                         dfdob, matchtodob, \n",
    "                         dfdod, matchtodod, matchtodoa, \n",
    "                         dfaddress, 'matchtoaddress', 'matchtoarrestloc', 'dfarrestloc', \n",
    "                         'namesim', 'modified_namesim', 'dname', 'mname', 'modified_namesim_v2', \n",
    "                         matchtossn, dfssn, #####\n",
    "                         'dobsim', 'dob_diff',\n",
    "                         'dodsim', 'dod_diff', \n",
    "                         'doddoasim', 'doddoa_diff', 'doa_maxenc_sim', 'doa_maxenc_diff', \n",
    "                         'addsim', 'arrestlocsim', 'cross_addsim', \n",
    "                         'ssnsim']]\n",
    "            return manualreview\n",
    "        except Exception as e:\n",
    "            indf.loc[:,'ssnsim'] = None #Should be None? or is 0 okay\n",
    "            cprint('Could not calculate ssnsim due to: {}'.format(e), 'light_red')\n",
    "            manualreview = indf[['Remove', dffn, dfmn, dfln, matchtofn, matchtomn, matchtoln, dfunids[0], matchtotarget, \n",
    "                         dfsex, matchtosex, 'dfname', 'matchtoname', 'nametag_match', \n",
    "                         dfdob, matchtodob, \n",
    "                         dfdod, matchtodod, matchtodoa, \n",
    "                         dfaddress, 'matchtoaddress', 'matchtoarrestloc', 'dfarrestloc', \n",
    "                         'namesim', 'modified_namesim', 'dname', 'mname', 'modified_namesim_v2', \n",
    "                         'dobsim', 'dob_diff',\n",
    "                         'dodsim', 'dod_diff', \n",
    "                         'doddoasim', 'doddoa_diff', 'doa_maxenc_sim', 'doa_maxenc_diff', \n",
    "                         'addsim', 'arrestlocsim', 'cross_addsim',\n",
    "                         'ssnsim']]\n",
    "            return manualreview\n",
    "            \n",
    "            \n",
    "    #else:\n",
    "        #print('No SSN available')\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "                         \n",
    "\n",
    "   \n",
    "\n",
    "def partsim(indf):\n",
    "\n",
    "    indf.loc[:,'dname'], indf.loc[:,'mname'], indf.loc[:'type'] = fnmnln_vec(indf[dffn], indf[dfmn], indf[dfln], indf[matchtofn], indf[matchtomn], indf[matchtoln])\n",
    "    indf.loc[:,'modified_namesim_v2'] = indf.apply(lambda x : calc_distances_jw(x['dname'], x['mname']), axis = 1)\n",
    "\n",
    "    #manualreview = indf[[dffn, dfmn, dfln, matchtofn, matchtomn, matchtoln, 'PAT_ID', matchtotarget, 'dfname', 'matchtoname', 'nametag_match', dfdob, matchtodob, dfdod, matchtodod, dfssn, matchtossn, 'ADD_LINE_1', 'matchtoaddress', 'namesim', 'modified_namesim', 'dname', 'mname', 'modified_namesim_v2', 'dobsim', 'dob_diff','dodsim', 'dod_diff', 'addsim', 'ssnsim']]\n",
    "    return indf\n",
    "\n",
    "def partsim_lite(indf): #5/20/2025\n",
    "    indf2 = indf.copy()\n",
    "    indf2['namesim'] = indf2.apply(lambda x : calc_distances_jw(x['dfname'], x['matchtoname']), axis = 1)\n",
    "    return indf2\n",
    "\n",
    "def recombine(path_with_gbm):\n",
    "    good=pd.read_excel(path_with_gbm, sheet_name = 'good')\n",
    "    mid=pd.read_excel(path_with_gbm, sheet_name = 'mid')\n",
    "    try:\n",
    "        bad=pd.read_excel(path_with_gbm, sheet_name = 'bad')\n",
    "    except:\n",
    "        bad=pd.DataFrame()\n",
    "\n",
    "    full = pd.concat([good,mid,bad])\n",
    "\n",
    "    #full = full[full['Remove'] != 1] #recall 1 means bad match, so we want to keep it! So here we are taking those to remove (good matches), \n",
    "                                     #which are NOT marked with 1. We are removing CORRECT matches\n",
    "    good_full = full[full['Remove'] != 1]\n",
    "    bad_full = full[full['Remove'] == 1]\n",
    "    return good_full, bad_full\n",
    "\n",
    "def recombine_general(path_with_gbm, df, matchto, drop_pat_ids, good_final, combos_to_remove):\n",
    "    if os.path.isfile(path_with_gbm):\n",
    "        preldf = len(df) #\n",
    "        prelmatchto = len(matchto) #\n",
    "        \n",
    "        if '.xlsx' in path_with_gbm:\n",
    "            good=pd.read_excel(path_with_gbm, sheet_name = 'good')\n",
    "            mid=pd.read_excel(path_with_gbm, sheet_name = 'mid')\n",
    "            try:\n",
    "                bad=pd.read_excel(path_with_gbm, sheet_name = 'bad')\n",
    "            except:\n",
    "                bad=pd.DataFrame()\n",
    "\n",
    "            full = pd.concat([good,mid,bad])\n",
    "        elif '.csv' in path_with_gbm:\n",
    "            full = pd.read_csv(path_with_gbm, sep = ',', header = 0)\n",
    "        \n",
    "        good_full = full[full['Remove'] != 1]\n",
    "\n",
    "        if not good_full.empty:\n",
    "            good_final = pd.concat([good_final, good_full])\n",
    "        #global bad_full\n",
    "        bad_full = full[full['Remove'] == 1]\n",
    "\n",
    "        bad_full[matchtotarget] = bad_full[matchtotarget].apply(lambda x : str(x))\n",
    "        good_full[matchtotarget] = good_full[matchtotarget].apply(lambda x : str(x))\n",
    "\n",
    "        bad_full['combinedid'] = bad_full[dfunids[0]].astype(str)+\"_\"+bad_full[matchtotarget].astype(str)\n",
    "        bad_full = bad_full[~bad_full['combinedid'].isin(combos_to_remove)] #remove combinedids that are already present in combos_to_remove, avoids dupes and deprecates dedupe.py\n",
    "        bad_full['combinedid'].to_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\combos_to_remove_deathcert.csv\", mode = 'a', sep = ',', header = False, index = False)\n",
    "        \n",
    "        \n",
    "        combos_to_remove = list(set(combos_to_remove + list(bad_full['combinedid'].values)))\n",
    "\n",
    "        if drop_pat_ids == 1:\n",
    "            df = df.copy()\n",
    "            df = df[~df[dfunids[0]].isin(good_full[dfunids[0]])][df_columns]\n",
    "        \n",
    "        matchto = matchto.copy()\n",
    "        matchto = matchto[~matchto[matchtotarget].isin(good_full[matchtotarget].values)]\n",
    "\n",
    "        print('Dropping used from df:', preldf, len(df), (preldf-len(df)))\n",
    "        print('Dropping used from matchto:', prelmatchto, len(matchto), (prelmatchto-len(matchto)))\n",
    "        print('Number of total good matches: {}\\nNumber of bad matches added to remove list: {}'.format(len(good_final), len(bad_full)))\n",
    "\n",
    "        return df, matchto, good_final, combos_to_remove\n",
    "    else:\n",
    "        print(f'Could not find {path_with_gbm}')\n",
    "        return df, matchto, good_final, combos_to_remove\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347112 -> 268637\n",
      "72123 -> 68264\n",
      "Found 884 matches\n",
      "\n",
      "Z2782431_3052025094388\n",
      "Index(['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'GENDER', 'ADD_LINE_1',\n",
      "       'ADD_LINE_2', 'CITY', 'STATE_C', 'HOME_PHONE', 'SSN', 'MAX_ENC_DATE',\n",
      "       'DEATH_DATE', 'Tag', 'BlankCol_df', 'First Name', 'Middle Name',\n",
      "       'Last Name', 'BlankCol_df11', 'BlankCol_df22', 'First Name_f2',\n",
      "       'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1',\n",
      "       'Last Name_f1', 'ADD_LINE_1_first', 'nametag_df', 'F31', 'F8', 'F20',\n",
      "       'F3', 'F4', 'F5', 'F1', 'F55', 'F56', 'F57', 'F19',\n",
      "       'BlankCol_matchto13', 'F132', 'F133', 'F134', 'F3_f2', 'F5_f2', 'F3_f3',\n",
      "       'F5_f3', 'F3_f1', 'F5_f1', 'F55_first', 'nametag_matchto',\n",
      "       'combinedid'],\n",
      "      dtype='object')\n",
      "Dropped 0 known bad matches.\n",
      "Forming FULL name columns 2025-07-18 15:24:45.151629\n",
      "Forming address columns 2025-07-18 15:24:45.169060\n",
      "Cleaning addresses 2025-07-18 15:24:45.197354\n",
      "Calculating similarity scores 2025-07-18 15:24:45.208023\n",
      "     dobsim done\n",
      "     namesim done\n",
      "     modified_namesim done\n",
      "Calculating modified namesim v2 2025-07-18 15:24:45.241632\n",
      "     modified_namesim done\n",
      "     modified_namesim_v2 done\n",
      "Calculating address similarity scores 2025-07-18 15:24:46.118637\n",
      "     addsim done\n",
      "     arrestlocsim done\n",
      "     cross_addsim done\n",
      "Calculating auxiliary date similarities 2025-07-18 15:24:46.138120\n",
      "     dodsim done\n",
      "     doddoasim done\n",
      "     doa_maxenc_sim done\n",
      "     doa_maxenc_diff done\n",
      "Calculating date differences 2025-07-18 15:24:46.161706\n",
      "     dob_diff done\n",
      "     dod_diff done\n",
      "     dobdoa_diff done\n",
      "Dropped 0 cases: 884 to 884 modified_namesim_v2 < 40\n",
      "Dropped 0 duplicates: 884 to 884\n",
      "Calculating SSN similarities 2025-07-18 15:24:46.192823\n",
      "bad: 13\n",
      "good: 791\n",
      "mid: 80\n",
      "total: 884\n",
      "Beginning save (2025-07-18 15:24:46.204583)\n",
      "Last run at 2025-07-18 15:24:46.976559\n"
     ]
    }
   ],
   "source": [
    "#testing smaller df ssn-wise matching -- works but SSN values are not good for matching -- too many errors unfort\n",
    "\n",
    "def tag(x, tag1):\n",
    "    if not pd.isna(x[matchtotarget]):\n",
    "        return tag1\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "if 'matchtossn' in locals() and 'dfssn' in locals():\n",
    "    outssn = pd.DataFrame()\n",
    "\n",
    "    dfsubset_ssn = df[~pd.isna(df[dfssn])]\n",
    "    dfsubset_ssn.index = range(len(dfsubset_ssn))\n",
    "    matchtosubset_ssn = matchto[~pd.isna(matchto[matchtossn])]\n",
    "    matchtosubset_ssn.index = range(len(matchtosubset_ssn))\n",
    "\n",
    "    print(len(df), '->', len(dfsubset_ssn))\n",
    "    print(len(matchto), '->', len(matchtosubset_ssn))\n",
    "\n",
    "    prel=len(df)\n",
    "    dfsubset_ssn = dfsubset_ssn.merge(matchtosubset_ssn[matchto_columns+['nametag_matchto']], how = 'left', left_on=[dfssn], right_on=[matchtossn], suffixes=('_df', '_matchto'))\n",
    "    #dfsubset_ssn = dfsubset_ssn.merge(matchtosubset_ssn[[matchtotarget, matchtossn]], how = 'left', left_on=[dfssn], right_on=[matchtossn], suffixes=('_df', '_matchto'))\n",
    "    dfsubset_ssn['Tag'] = dfsubset_ssn.apply(lambda x : tag(x, 'SSN'), axis = 1)\n",
    "    #print(df.columns)\n",
    "    print('Found {} matches'.format(len(dfsubset_ssn[~pd.isna(dfsubset_ssn[matchtotarget])])))\n",
    "\n",
    "    #Save non blank matchtotarget merged rows to output DF\n",
    "    outssn = pd.concat([outssn, dfsubset_ssn[~pd.isna(dfsubset_ssn[matchtotarget])]], ignore_index=True)\n",
    "    #remove matched rows and then remove excess columns from merging!\n",
    "\n",
    "    # dfsubset_ssn = df[pd.isna(df[matchtotarget])][df_columns] #need a new way to remove these potential matches from main df...\n",
    "    # I suggest using PAT_IDs - but check that they are unique!\n",
    "    #todrop = outssn['PAT_ID'].to_list() #nice\n",
    "    #df = df[~df['PAT_ID'].isin(outssn['PAT_ID'].to_list())] #nice\n",
    "#################################################################################################################### WIP WIP WIP WIP WIP WIP WIP WIP WIP\n",
    "    #Printing delta\n",
    "    #print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))\n",
    "\n",
    "    outssn = combo_remover(outssn, combos_to_remove)\n",
    "    manualreview = fullsim(outssn) #calculating all scores for potential matches\n",
    "\n",
    "\n",
    "    out_good = manualreview[(manualreview['namesim'] > 90) & \n",
    "                            ((manualreview['dobsim'] > 95) | (manualreview['dob_diff'] <= 365)) & \n",
    "                            ((pd.isna(manualreview[dfdod])) | (manualreview['dodsim'] > 90) | (manualreview['dod_diff'] <= 10))] #good\n",
    "                            #if dfdod is null, base this on dobsim  -- > keep for now, monitor for real set\n",
    "                            #if dobsim is not 100, also include in GOOD if dob_diff < 365\n",
    "    #out_mid # dfdob = 100, namesim > 80 | modified_namesim > 80, dod == 100 | dod == 0\n",
    "\n",
    "    out_mid = manualreview[(~(manualreview['dfname'].isin(out_good['dfname'].values))) \n",
    "                        & \n",
    "                        (\n",
    "                            ((manualreview['dobsim'] == 100) & \n",
    "                            ((manualreview['namesim'] > 75) | (manualreview['modified_namesim_v2'] > 75)) & \n",
    "                            (manualreview['dodsim'].isin([0,100])))\n",
    "                        |\n",
    "                            (((manualreview['dobsim'] > 95) | (manualreview['dob_diff'] <= 365)) & \n",
    "                            ((manualreview['dodsim'] > 90) | (manualreview['dod_diff'] <= 10)) &\n",
    "                            (manualreview['addsim'] > 90))\n",
    "                        )\n",
    "                        ]\n",
    "\n",
    "    out_bad = manualreview[~(manualreview['dfname'].isin(out_mid['dfname'])) & ~(manualreview['dfname'].isin(out_good['dfname']))] #left over (mid?)\n",
    "\n",
    "    print('bad: {}\\ngood: {}\\nmid: {}'.format(len(out_bad), len(out_good), len(out_mid)))\n",
    "    print('total:',len(out_bad) + len(out_good) + len(out_mid))\n",
    "\n",
    "\n",
    "    # with pd.ExcelWriter(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\d25_ssn_match_review_round1.xlsx\") as writer:\n",
    "    #     out_good.to_excel(writer, index = False, sheet_name = 'good')\n",
    "    #     out_mid.to_excel(writer, index = False, sheet_name = 'mid')\n",
    "    #     out_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "    formatted_save_wc(r\"{}\\DC_ssn_match_review_{}.xlsx\".format(basepath, savemod), [out_good, out_mid, out_bad])\n",
    "\n",
    "\n",
    "    del manualreview, out_good, out_mid, out_bad, outssn\n",
    "\n",
    "    try:\n",
    "        del dfsubset_ssn, matchtosubset_ssn #saving memory downstream\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print('No SSN columns detected, skipping this chunk.')\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:475: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full[matchtotarget] = bad_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:476: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good_full[matchtotarget] = good_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full['combinedid'] = bad_full[dfunids[0]].astype(str)+\"_\"+bad_full[matchtotarget].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping used from df: 347112 346235 877\n",
      "Dropping used from matchto: 72123 71246 877\n",
      "Number of total good matches: 877\n",
      "Number of bad matches added to remove list: 7\n"
     ]
    }
   ],
   "source": [
    "if dropvar == 1:\n",
    "    df, matchto, good_final, combos_to_remove = recombine_general(r\"{}\\DC_ssn_match_review_{}_use.xlsx\".format(basepath, savemod),\n",
    "                                                                  df, matchto, drop_pat_ids, good_final, combos_to_remove)\n",
    "else:\n",
    "    print('Not dropping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just testing\n",
    "# print(df[dfdob].iloc[0], type(df[dfdob].iloc[0]), len(df[~pd.isna(df[dfdob])]))\n",
    "# print(df[dfdod].iloc[0], type(df[dfdod].iloc[0]), len(df[~pd.isna(df[dfdod])]))\n",
    "# print(matchto[matchtodob].iloc[0], type(matchto[matchtodob].iloc[0]), len(matchto[~pd.isna(matchto[matchtodob])]))\n",
    "# print(matchto[matchtodod].iloc[0], type(matchto[matchtodod].iloc[0]), len(matchto[~pd.isna(matchto[matchtodod])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'GENDER', 'ADD_LINE_1', 'ADD_LINE_2', 'CITY', 'STATE_C', 'HOME_PHONE', 'SSN', 'MAX_ENC_DATE', 'DEATH_DATE', 'Tag', 'BlankCol_df', 'First Name', 'Middle Name', 'Last Name', 'BlankCol_df11', 'BlankCol_df22', 'First Name_f2', 'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1', 'Last Name_f1', 'ADD_LINE_1_first', 'nametag_df']\n",
      "Found 35 matches\n",
      "35\n",
      "Forming FULL name columns 2025-07-18 15:42:39.167680\n",
      "Forming address columns 2025-07-18 15:42:39.169638\n",
      "Cleaning addresses 2025-07-18 15:42:39.172689\n",
      "Calculating similarity scores 2025-07-18 15:42:39.173785\n",
      "     dobsim done\n",
      "     namesim done\n",
      "     modified_namesim done\n",
      "Calculating modified namesim v2 2025-07-18 15:42:39.176785\n",
      "     modified_namesim done\n",
      "     modified_namesim_v2 done\n",
      "Calculating address similarity scores 2025-07-18 15:42:39.214239\n",
      "     addsim done\n",
      "     arrestlocsim done\n",
      "     cross_addsim done\n",
      "Calculating auxiliary date similarities 2025-07-18 15:42:39.218015\n",
      "     dodsim done\n",
      "     doddoasim done\n",
      "     doa_maxenc_sim done\n",
      "     doa_maxenc_diff done\n",
      "Calculating date differences 2025-07-18 15:42:39.221363\n",
      "     dob_diff done\n",
      "     dod_diff done\n",
      "     dobdoa_diff done\n",
      "Dropped 0 cases: 35 to 35 modified_namesim_v2 < 40\n",
      "Dropped 0 duplicates: 35 to 35\n",
      "Calculating SSN similarities 2025-07-18 15:42:39.228733\n",
      "25 8 2\n",
      "Beginning save (2025-07-18 15:42:39.232630)\n"
     ]
    }
   ],
   "source": [
    "#0. DOB, DOD full match with fuzzy similarity for manual review\n",
    "if do == 1:\n",
    "    def tag(x, tag1):\n",
    "        #print(x[matchtotarget])\n",
    "        if not pd.isna(x[matchtotarget]):\n",
    "            return tag1\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def tag_index(x, tag_index): ##### IMPLEMENT THIS! Want an integer tag with the description!\n",
    "        if not pd.isna(x[matchtotarget]):\n",
    "            return int(tag_index)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "\n",
    "\n",
    "    print(df_columns)\n",
    "    prel = len(df)\n",
    "    df.index = range(0,len(df)); matchto.index = range(0,len(matchto))\n",
    "    tempdf = df.merge(matchto[matchto_columns+['nametag_matchto']], how = 'left', left_on=[dfdob, dfdod], right_on=[matchtodob, matchtodod], suffixes=('_df', '_matchto')) \n",
    "    #df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, manual'), axis = 1)\n",
    "    #df['TagIndex'] = df.apply(lambda x : tag_index(x, 2), axis=1)\n",
    "    #print(df.columns)\n",
    "    print('Found {} matches'.format(len(tempdf[~pd.isna(tempdf[matchtotarget])])))\n",
    "\n",
    "    #Save non blank matchtotarget merged rows to output DF\n",
    "    out = pd.DataFrame()\n",
    "    out = pd.concat([out, tempdf[~pd.isna(tempdf[matchtotarget])]], ignore_index=True)\n",
    "    #remove matched rows and then remove excess columns from merging!\n",
    "\n",
    "    print(len(out))\n",
    "    out = combo_remover(out, combos_to_remove)\n",
    "    manualreview_out = fullsim(out)\n",
    "\n",
    "    #good\n",
    "    dobdod_good = manualreview_out[\n",
    "                                    (manualreview_out['namesim'] == 100) \n",
    "                                | \n",
    "                                    ((manualreview_out['namesim'] > 90) & ((manualreview_out['addsim'] > 90) | (manualreview_out['ssnsim'] > 85)))\n",
    "                                ]\n",
    "    #mid\n",
    "    dobdod_mid = manualreview_out[\n",
    "                                    (~manualreview_out['dfname'].isin(dobdod_good['dfname'].values)) &\n",
    "                                    ((manualreview_out['namesim'] > 75) | (manualreview_out['modified_namesim_v2'] > 75))\n",
    "                                ]\n",
    "\n",
    "    #everything else\n",
    "    dobdod_bad = manualreview_out[(~manualreview_out['dfname'].isin(dobdod_good['dfname'].values)) & (~manualreview_out['dfname'].isin(dobdod_mid['dfname'].values))]\n",
    "\n",
    "    print(len(dobdod_good), len(dobdod_mid), len(dobdod_bad))\n",
    "\n",
    "    # with pd.ExcelWriter(r\"{}\\d25_dobdod_match_review_{}.xlsx\".format(basepath, savemod)) as writer:\n",
    "    #     dobdod_good.to_excel(writer, index = False, sheet_name = 'good')\n",
    "    #     dobdod_mid.to_excel(writer, index = False, sheet_name = 'mid')\n",
    "    #     dobdod_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "    formatted_save_wc(r\"{}\\DC_dobdod_match_review_{}.xlsx\".format(basepath, savemod), [dobdod_good, dobdod_mid, dobdod_bad])\n",
    "\n",
    "    del manualreview_out, dobdod_good, dobdod_bad, dobdod_mid\n",
    "else:\n",
    "    print('skipping')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:475: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full[matchtotarget] = bad_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:476: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good_full[matchtotarget] = good_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full['combinedid'] = bad_full[dfunids[0]].astype(str)+\"_\"+bad_full[matchtotarget].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping used from df: 346235 346201 34\n",
      "Dropping used from matchto: 71246 71212 34\n",
      "Number of total good matches: 911\n",
      "Number of bad matches added to remove list: 0\n"
     ]
    }
   ],
   "source": [
    "#Removing manually adjudicated records\n",
    "if dropvar == 1:\n",
    "    \n",
    "    df, matchto, good_final, combos_to_remove = recombine_general(r\"{}\\DC_dobdod_match_review_{}_use.xlsx\".format(basepath, savemod),\n",
    "                                                                  df, matchto, drop_pat_ids, good_final, combos_to_remove)\n",
    "else:\n",
    "    print('Not dropping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'GENDER', 'ADD_LINE_1', 'ADD_LINE_2', 'CITY', 'STATE_C', 'HOME_PHONE', 'SSN', 'MAX_ENC_DATE', 'DEATH_DATE', 'Tag', 'BlankCol_df', 'First Name', 'Middle Name', 'Last Name', 'BlankCol_df11', 'BlankCol_df22', 'First Name_f2', 'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1', 'Last Name_f1', 'ADD_LINE_1_first', 'nametag_df']\n",
      "Found 20681 matches\n",
      "346201 to 346201 difference: 0\n",
      "20681\n",
      "\n",
      "Z579822_3052024290385\n",
      "Index(['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'GENDER', 'ADD_LINE_1',\n",
      "       'ADD_LINE_2', 'CITY', 'STATE_C', 'HOME_PHONE', 'SSN', 'MAX_ENC_DATE',\n",
      "       'DEATH_DATE', 'Tag', 'BlankCol_df', 'First Name', 'Middle Name',\n",
      "       'Last Name', 'BlankCol_df11', 'BlankCol_df22', 'First Name_f2',\n",
      "       'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1',\n",
      "       'Last Name_f1', 'ADD_LINE_1_first', 'nametag_df', 'F31', 'F8', 'F20',\n",
      "       'F3', 'F4', 'F5', 'F1', 'F55', 'F56', 'F57', 'F19',\n",
      "       'BlankCol_matchto13', 'F132', 'F133', 'F134', 'F3_f2', 'F5_f2', 'F3_f3',\n",
      "       'F5_f3', 'F3_f1', 'F5_f1', 'F55_first', 'nametag_matchto',\n",
      "       'combinedid'],\n",
      "      dtype='object')\n",
      "Dropped 1 known bad matches.\n",
      "Forming FULL name columns 2025-07-18 15:50:41.128487\n",
      "Forming address columns 2025-07-18 15:50:41.403084\n",
      "Cleaning addresses 2025-07-18 15:50:41.843920\n",
      "Calculating similarity scores 2025-07-18 15:50:42.015799\n",
      "     dobsim done\n",
      "     namesim done\n",
      "     modified_namesim done\n",
      "Calculating modified namesim v2 2025-07-18 15:50:42.629018\n",
      "     modified_namesim done\n",
      "     modified_namesim_v2 done\n",
      "Calculating address similarity scores 2025-07-18 15:51:10.315326\n",
      "     addsim done\n",
      "     arrestlocsim done\n",
      "     cross_addsim done\n",
      "Calculating auxiliary date similarities 2025-07-18 15:51:10.719875\n",
      "     dodsim done\n",
      "     doddoasim done\n",
      "     doa_maxenc_sim done\n",
      "     doa_maxenc_diff done\n",
      "Calculating date differences 2025-07-18 15:51:11.209957\n",
      "     dob_diff done\n",
      "     dod_diff done\n",
      "     dobdoa_diff done\n",
      "Dropped 1767 cases: 20680 to 18913 modified_namesim_v2 < 40\n",
      "Dropped 0 duplicates: 18913 to 18913\n",
      "Calculating SSN similarities 2025-07-18 15:51:11.659415\n",
      "0 2 17483\n",
      "Beginning save (2025-07-18 15:51:11.805394)\n"
     ]
    }
   ],
   "source": [
    "##### DOD straight match with fuzzy matching for everything else\n",
    "if do == 1:\n",
    "  print(df_columns)\n",
    "  prel = len(df)\n",
    "  df.index = range(0,len(df)); matchto.index = range(0,len(matchto))\n",
    "  tempdf = df.merge(matchto[matchto_columns+['nametag_matchto']], how = 'left', left_on=[dfdod], right_on=[matchtodod], suffixes=('_df', '_matchto')) \n",
    "\n",
    "\n",
    "  print('Found {} matches'.format(len(tempdf[~pd.isna(tempdf[matchtotarget])])))\n",
    "\n",
    "  #Save non blank matchtotarget merged rows to output DF\n",
    "  out = pd.DataFrame() #remake out df - hopefully cut down on memory?\n",
    "  out = pd.concat([out, tempdf[~pd.isna(tempdf[matchtotarget])]], ignore_index=True)\n",
    "  #remove matched rows and then remove excess columns from merging!\n",
    "\n",
    "\n",
    "  #Printing delta\n",
    "  print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))\n",
    "\n",
    "  print(len(out))\n",
    "\n",
    "\n",
    "  #break\n",
    "  out = combo_remover(out, combos_to_remove)\n",
    "  manualreview_dod = fullsim(out)\n",
    "\n",
    "  #good\n",
    "  dod_good = manualreview_dod[\n",
    "                                  (manualreview_dod['namesim'] == 100) \n",
    "                                | \n",
    "                                  ((manualreview_dod['namesim'] > 90) & ( (manualreview_dod['addsim'] > 90) | (manualreview_dod['ssnsim'] > 85) | (manualreview_dod['dobsim'] > 90)))\n",
    "                                ]\n",
    "  #mid\n",
    "  dod_mid = manualreview_dod[\n",
    "                                  (~manualreview_dod['dfname'].isin(dod_good['dfname'].values)) &\n",
    "                                  (((manualreview_dod['namesim'] > 75) | (manualreview_dod['modified_namesim_v2'] > 75)) & ((manualreview_dod['dobsim'] > 90) | (manualreview_dod['ssnsim'] > 85) | (manualreview_dod['addsim'] > 90)))\n",
    "                                ]\n",
    "\n",
    "  #everything else\n",
    "  dod_bad = manualreview_dod[(~manualreview_dod['dfname'].isin(dod_good['dfname'].values)) & (~manualreview_dod['dfname'].isin(dod_mid['dfname'].values))]\n",
    "\n",
    "  print(len(dod_good), len(dod_mid), len(dod_bad))\n",
    "\n",
    "  # with pd.ExcelWriter(r'{}\\DC_dod_match_review_{}.xlsx'.format(basepath, savemod)) as writer:\n",
    "  #     #manualreview_dod.to_excel(writer, index = False, sheet_name='full')\n",
    "  #     dod_good.to_excel(writer, index = False, sheet_name = 'good')\n",
    "  #     dod_mid.to_excel(writer, index = False, sheet_name = 'mid')\n",
    "  #     if len(dod_bad) < 1048576:\n",
    "  #       dod_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "  #     else:\n",
    "  #        dod_bad.iloc[0:10**6].to_excel(writer, index = False, sheet_name = 'bad')\n",
    "  formatted_save_wc(r'{}\\DC_dod_match_review_{}.xlsx'.format(basepath, savemod), [dod_good, dod_mid, dod_bad])\n",
    "\n",
    "  # print(manualreview_dod[matchtotarget].value_counts().to_frame())\n",
    "  # print(len(list(manualreview_dod[matchtotarget].unique())))\n",
    "\n",
    "  del dod_good, dod_mid, dod_bad, manualreview_dod, out\n",
    "\n",
    "else:\n",
    "  print(\"skipping\")\n",
    "  ### Possible problem with these sets - duplicated matches - i.e paul f tompkins being matched with several other people. \n",
    "  #Solution: make list of unique F1 (or other ID), then loop through those, filter to tempdf, take highest match score for that person\n",
    "\n",
    "\n",
    "  #before removing blanks: Found 2022395 matches\n",
    "  #after removing blanks: Found 271877 matches\n",
    "  #splits: 12 12 250394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:464: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full = pd.concat([good,mid,bad])\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:475: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full[matchtotarget] = bad_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:476: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good_full[matchtotarget] = good_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full['combinedid'] = bad_full[dfunids[0]].astype(str)+\"_\"+bad_full[matchtotarget].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping used from df: 346201 346200 1\n",
      "Dropping used from matchto: 71212 71211 1\n",
      "Number of total good matches: 912\n",
      "Number of bad matches added to remove list: 17484\n"
     ]
    }
   ],
   "source": [
    "#Removing good matches from pool, returning bad matches to pool.\n",
    "if dropvar == 1:\n",
    "    df, matchto, good_final, combos_to_remove = recombine_general(r'{}\\DC_dod_match_review_{}_use.xlsx'.format(basepath, savemod),\n",
    "                                                                  df, matchto, drop_pat_ids, good_final, combos_to_remove)\n",
    "else:\n",
    "    print('Not dropping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2025-07-18 15:56:12.008644\n",
      "['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'GENDER', 'ADD_LINE_1', 'ADD_LINE_2', 'CITY', 'STATE_C', 'HOME_PHONE', 'SSN', 'MAX_ENC_DATE', 'DEATH_DATE', 'Tag', 'BlankCol_df', 'First Name', 'Middle Name', 'Last Name', 'BlankCol_df11', 'BlankCol_df22', 'First Name_f2', 'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1', 'Last Name_f1', 'ADD_LINE_1_first', 'nametag_df']\n",
      "Found 721655 matches\n",
      "Index(['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'GENDER', 'ADD_LINE_1',\n",
      "       'ADD_LINE_2', 'CITY', 'STATE_C', 'HOME_PHONE', 'SSN', 'MAX_ENC_DATE',\n",
      "       'DEATH_DATE', 'Tag', 'BlankCol_df', 'First Name', 'Middle Name',\n",
      "       'Last Name', 'BlankCol_df11', 'BlankCol_df22', 'First Name_f2',\n",
      "       'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1',\n",
      "       'Last Name_f1', 'ADD_LINE_1_first', 'nametag_df'],\n",
      "      dtype='object')\n",
      "346200 to 346200 difference: 0\n",
      "721655\n",
      "Merging done at 2025-07-18 15:56:13.520509\n",
      "\n",
      "Z677372_3052025114873\n",
      "Index(['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'GENDER', 'ADD_LINE_1',\n",
      "       'ADD_LINE_2', 'CITY', 'STATE_C', 'HOME_PHONE', 'SSN', 'MAX_ENC_DATE',\n",
      "       'DEATH_DATE', 'Tag', 'BlankCol_df', 'First Name', 'Middle Name',\n",
      "       'Last Name', 'BlankCol_df11', 'BlankCol_df22', 'First Name_f2',\n",
      "       'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1',\n",
      "       'Last Name_f1', 'ADD_LINE_1_first', 'nametag_df', 'F31', 'F8', 'F20',\n",
      "       'F3', 'F4', 'F5', 'F1', 'F55', 'F56', 'F57', 'F19',\n",
      "       'BlankCol_matchto13', 'F132', 'F133', 'F134', 'F3_f2', 'F5_f2', 'F3_f3',\n",
      "       'F5_f3', 'F3_f1', 'F5_f1', 'F55_first', 'nametag_matchto',\n",
      "       'combinedid'],\n",
      "      dtype='object')\n",
      "Dropped 3 known bad matches.\n",
      "Starting dfname and matchtoname formation\n",
      "Done with dfname and matchtoname formation. Starting partsim_lite\n",
      "Done with partsim application, starting filtering\n",
      "Partsim application done 2025-07-18 15:56:34.206645.\n",
      "721652 to 48952 (-672700)\n",
      "Forming FULL name columns 2025-07-18 15:56:34.240722\n",
      "Forming address columns 2025-07-18 15:56:34.981891\n",
      "Cleaning addresses 2025-07-18 15:56:36.102812\n",
      "Calculating similarity scores 2025-07-18 15:56:36.532288\n",
      "     dobsim done\n",
      "     namesim done\n",
      "     modified_namesim done\n",
      "Calculating modified namesim v2 2025-07-18 15:56:38.134846\n",
      "Initiating swap: {'harndorp': 'hartendorp'}\n",
      "     modified_namesim done\n",
      "     modified_namesim_v2 done\n",
      "Calculating address similarity scores 2025-07-18 15:57:45.722715\n",
      "     addsim done\n",
      "     arrestlocsim done\n",
      "     cross_addsim done\n",
      "Calculating auxiliary date similarities 2025-07-18 15:57:46.968711\n",
      "     dodsim done\n",
      "     doddoasim done\n",
      "     doa_maxenc_sim done\n",
      "     doa_maxenc_diff done\n",
      "Calculating date differences 2025-07-18 15:57:48.278608\n",
      "     dob_diff done\n",
      "     dod_diff done\n",
      "     dobdoa_diff done\n",
      "Dropped 12 cases: 48952 to 48940 modified_namesim_v2 < 40\n",
      "Dropped 0 duplicates: 48940 to 48940\n",
      "Calculating SSN similarities 2025-07-18 15:57:49.513685\n",
      "Fullsim application done 2025-07-18 15:57:49.933569\n",
      "69 2331 45842\n",
      "Beginning save (2025-07-18 15:57:49.959015)\n"
     ]
    }
   ],
   "source": [
    "#### DOB matching with fuzzy on stuff afterwards:\n",
    "\n",
    "if do == 1:\n",
    "    import datetime\n",
    "\n",
    "    print('Starting at {}'.format(datetime.datetime.now()))\n",
    "    print(df_columns)\n",
    "    prel = len(df)\n",
    "    df.index = range(0,len(df)); matchto.index = range(0,len(matchto))\n",
    "    tempdf = df.merge(matchto[matchto_columns+['nametag_matchto']], how = 'left', left_on=[dfdob], right_on=[matchtodob], suffixes=('_df', '_matchto')) \n",
    "    #df['Tag'] = df.apply(lambda x : tag(x, 'DOB, manual'), axis = 1)\n",
    "    #df['TagIndex'] = df.apply(lambda x : tag_index(x, 0.60), axis=1)\n",
    "    #print(df.columns)\n",
    "    print('Found {} matches'.format(len(tempdf[~pd.isna(tempdf[matchtotarget])])))\n",
    "\n",
    "    #Save non blank matchtotarget merged rows to output DF\n",
    "    out = pd.DataFrame() #remake out df - hopefully cut down on memory?\n",
    "    out = pd.concat([out, tempdf[~pd.isna(tempdf[matchtotarget])]], ignore_index=True)\n",
    "    #remove matched rows and then remove excess columns from merging!\n",
    "    print(df.columns)\n",
    "    #df = df[~df[dfunids[0]].isin(out[dfunids[0]].values)][df_columns]\n",
    "    #matchto = matchto[~matchto[matchtotarget].isin(out[matchtotarget].values)] #remove matched ones from matchto\n",
    "\n",
    "    #Printing delta\n",
    "    print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))\n",
    "\n",
    "    print(len(out))\n",
    "    print('Merging done at {}'.format(datetime.datetime.now()))\n",
    "    ## new\n",
    "    ########## TESTING\n",
    "\n",
    "    #manualreview_dob = pd.read_csv(r'C:\\Users\\MathiasM\\OneDrive - Cedars-Sinai Health System\\Documents\\dob_match_review.csv', sep = ',', header = 0)\n",
    "    #print(len(manualreview_dob))\n",
    "\n",
    "    out = combo_remover(out, combos_to_remove)\n",
    "    print('Starting dfname and matchtoname formation')\n",
    "    out['dfname'] = out.apply(lambda x : joining(sorted([x[dffn], x[dfmn], x[dfln]]), ''), axis = 1)\n",
    "    out['matchtoname'] = out.apply(lambda x : joining(sorted([x[matchtofn], x[matchtomn], x[matchtoln]]), ''), axis = 1)\n",
    "    print('Done with dfname and matchtoname formation. Starting partsim_lite')\n",
    "\n",
    "    filtered_dob = partsim_lite(out)\n",
    "    print('Done with partsim application, starting filtering')\n",
    "    filtered_dob = filtered_dob[filtered_dob['namesim'] > 62]\n",
    "\n",
    "    print('Partsim application done {}.\\n{} to {} (-{})'.format(datetime.datetime.now(), len(out), len(filtered_dob), len(out)-len(filtered_dob)))\n",
    "    \n",
    "    manualreview_dob = fullsim(filtered_dob)\n",
    "\n",
    "    print('Fullsim application done {}'.format(datetime.datetime.now()))\n",
    "\n",
    "    dob_good = manualreview_dob[\n",
    "                                    (manualreview_dob['namesim'] == 100) \n",
    "                                | \n",
    "                                    ((manualreview_dob['namesim'] > 90) & ( (manualreview_dob['addsim'] > 90) | (manualreview_dob['ssnsim'] > 90) | (manualreview_dob['dodsim'] > 90) ))\n",
    "                                ]\n",
    "    #mid\n",
    "    dob_mid = manualreview_dob[\n",
    "                                    (~manualreview_dob['dfname'].isin(dob_good['dfname'].values)) &\n",
    "                                    (((manualreview_dob['namesim'] > 75) | (manualreview_dob['modified_namesim_v2'] > 75)) & ((manualreview_dob['dodsim'] > 90) | (manualreview_dob['dodsim'] == 0) | (manualreview_dob['ssnsim'] > 90) | (manualreview_dob['addsim'] > 90)))\n",
    "                                ]\n",
    "\n",
    "    #everything else\n",
    "    dob_bad = manualreview_dob[(~manualreview_dob['dfname'].isin(dob_good['dfname'].values)) & (~manualreview_dob['dfname'].isin(dob_mid['dfname'].values))]\n",
    "\n",
    "    print(len(dob_good), len(dob_mid), len(dob_bad))\n",
    "\n",
    "    # with pd.ExcelWriter(r'{}\\DC_dob_match_review_{}.xlsx'.format(basepath, savemod)) as writer:\n",
    "    #     #manualreview_dod.to_excel(writer, index = False, sheet_name='full')\n",
    "    #     dob_good.to_excel(writer, index = False, sheet_name = 'good')\n",
    "    #     dob_mid.to_excel(writer, index = False, sheet_name = 'mid')\n",
    "    #     if len(dob_bad) < 1040000:\n",
    "    #         dob_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "    #     else:\n",
    "    #         dob_bad = dob_bad[dob_bad['namesim'] > 50]\n",
    "    #         if len(dob_bad) < 1040000:\n",
    "    #             dob_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "    #         else:\n",
    "    #             dob_bad.iloc[0:10**6].to_excel(writer, index=False, sheet_name='bad')\n",
    "    formatted_save_wc(r'{}\\DC_dob_match_review_{}.xlsx'.format(basepath, savemod), [dob_good, dob_mid, dob_bad])\n",
    "    \n",
    "    del dob_good, dob_mid, dob_bad, manualreview_dob\n",
    "\n",
    "else:\n",
    "    print('Skipping DOB match.')\n",
    "\n",
    "    #with pd.ExcelWriter(r'C:\\Users\\MathiasM\\OneDrive - Cedars-Sinai Health System\\Documents\\dob_match_review.xlsx') as writer:\n",
    "        #manualreview_dob.to_excel(writer, index = False, sheet_name='full')\n",
    "\n",
    "#BEFORE REMOVING BLANK NAMES: Found 4,427,451 matches\n",
    "#AFTER REMOVING BLANK NAMES: Found 17,423,944 matches ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### TEMPORARY RE-RUN BECAUSE OF WEIRD FAILURE\n",
    "# filt_dob = filtered_dob[filtered_dob['modified_namesim_v2'] > 62]\n",
    "\n",
    "# print('Partsim application done {}.\\n{} to {} (-{})'.format(datetime.datetime.now(), len(out), len(filt_dob), len(out)-len(filt_dob)))\n",
    "\n",
    "# manualreview_dob = fullsim(filt_dob)\n",
    "\n",
    "# print('Fullsim application done {}'.format(datetime.datetime.now()))\n",
    "\n",
    "# dob_good = manualreview_dob[\n",
    "#                                 (manualreview_dob['namesim'] == 100) \n",
    "#                             | \n",
    "#                                 ((manualreview_dob['namesim'] > 90) & ( (manualreview_dob['addsim'] > 90) | (manualreview_dob['ssnsim'] > 90) | (manualreview_dob['dodsim'] > 90) ))\n",
    "#                             ]\n",
    "# #mid\n",
    "# dob_mid = manualreview_dob[\n",
    "#                                 (~manualreview_dob['dfname'].isin(dob_good['dfname'].values)) &\n",
    "#                                 (((manualreview_dob['namesim'] > 75) | (manualreview_dob['modified_namesim_v2'] > 75)) & ((manualreview_dob['dodsim'] > 90) | (manualreview_dob['dodsim'] == 0) | (manualreview_dob['ssnsim'] > 90) | (manualreview_dob['addsim'] > 90)))\n",
    "#                             ]\n",
    "\n",
    "# #everything else\n",
    "# dob_bad = manualreview_dob[(~manualreview_dob['dfname'].isin(dob_good['dfname'].values)) & (~manualreview_dob['dfname'].isin(dob_mid['dfname'].values))]\n",
    "\n",
    "# print(len(dob_good), len(dob_mid), len(dob_bad))\n",
    "\n",
    "# with pd.ExcelWriter(r'C:\\Users\\MathiasM\\OneDrive - Cedars-Sinai Health System\\Documents\\d23_half24_dob_match_review_filtered_round1.xlsx') as writer:\n",
    "#     #manualreview_dod.to_excel(writer, index = False, sheet_name='full')\n",
    "#     dob_good.to_excel(writer, index = False, sheet_name = 'good')\n",
    "#     dob_mid.to_excel(writer, index = False, sheet_name = 'mid')\n",
    "#     if len(dob_bad) < 1040000:\n",
    "#         dob_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "#     else:\n",
    "#         dob_bad.iloc[0:10**6].to_excel(writer, index=False, sheet_name='bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:464: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full = pd.concat([good,mid,bad])\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:475: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full[matchtotarget] = bad_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:476: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good_full[matchtotarget] = good_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full['combinedid'] = bad_full[dfunids[0]].astype(str)+\"_\"+bad_full[matchtotarget].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping used from df: 346200 346098 102\n",
      "Dropping used from matchto: 71211 71109 102\n",
      "Number of total good matches: 1014\n",
      "Number of bad matches added to remove list: 48140\n"
     ]
    }
   ],
   "source": [
    "#Removing good matches from pool, returning bad matches to pool.\n",
    "if dropvar == 1:\n",
    "    df, matchto, good_final, combos_to_remove = recombine_general(r'{}\\DC_dob_match_review_{}_use.xlsx'.format(basepath, savemod),\n",
    "                                                                  df, matchto, drop_pat_ids, good_final, combos_to_remove)\n",
    "else:\n",
    "    print('Not dropping')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1497 matches\n",
      "Index(['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'GENDER', 'ADD_LINE_1',\n",
      "       'ADD_LINE_2', 'CITY', 'STATE_C', 'HOME_PHONE', 'SSN', 'MAX_ENC_DATE',\n",
      "       'DEATH_DATE', 'Tag', 'BlankCol_df', 'First Name', 'Middle Name',\n",
      "       'Last Name', 'BlankCol_df11', 'BlankCol_df22', 'First Name_f2',\n",
      "       'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1',\n",
      "       'Last Name_f1', 'ADD_LINE_1_first', 'nametag_df', 'tempname'],\n",
      "      dtype='object')\n",
      "346098 to 346098 difference: 0\n",
      "1497\n",
      "Forming FULL name columns 2025-07-18 16:16:16.153453\n",
      "Forming address columns 2025-07-18 16:16:16.181433\n",
      "Cleaning addresses 2025-07-18 16:16:16.218953\n",
      "Calculating similarity scores 2025-07-18 16:16:16.233253\n",
      "     dobsim done\n",
      "     namesim done\n",
      "     modified_namesim done\n",
      "Calculating modified namesim v2 2025-07-18 16:16:16.287279\n",
      "     modified_namesim done\n",
      "     modified_namesim_v2 done\n",
      "Calculating address similarity scores 2025-07-18 16:16:17.490447\n",
      "     addsim done\n",
      "     arrestlocsim done\n",
      "     cross_addsim done\n",
      "Calculating auxiliary date similarities 2025-07-18 16:16:17.524831\n",
      "     dodsim done\n",
      "     doddoasim done\n",
      "     doa_maxenc_sim done\n",
      "     doa_maxenc_diff done\n",
      "Calculating date differences 2025-07-18 16:16:17.562654\n",
      "     dob_diff done\n",
      "     dod_diff done\n",
      "     dobdoa_diff done\n",
      "Dropped 0 cases: 1497 to 1497 modified_namesim_v2 < 40\n",
      "Dropped 0 duplicates: 1497 to 1497\n",
      "Calculating SSN similarities 2025-07-18 16:16:17.604006\n",
      "1 15 1464\n",
      "Beginning save (2025-07-18 16:16:17.619254)\n"
     ]
    }
   ],
   "source": [
    "##### dname/mname straight match with fuzzy matching for everything else\n",
    "if do == 1:\n",
    "    def namecombiner(fn, mn, ln):\n",
    "        if not pd.isna(fn) and not pd.isna(ln) and not pd.isna(mn):\n",
    "            return ''.join(sorted([fn, mn, ln]))\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    ### Trying with middle name first\n",
    "    df['tempname'] = df.apply(lambda x : namecombiner(x[dffn], x[dfmn], x[dfln]), axis = 1)\n",
    "    matchto['tempname'] = matchto.apply(lambda x : namecombiner(x[matchtofn], x[matchtomn], x[matchtoln]), axis = 1)\n",
    "\n",
    "    prel = len(df)\n",
    "    df.index = range(0,len(df)); matchto.index = range(0,len(matchto))\n",
    "    tempdf = df.merge(matchto[matchto_columns+['nametag_matchto','tempname']], how = 'left', left_on=['tempname'], right_on=['tempname'], suffixes=('_df', '_matchto')) \n",
    "    #df['Tag'] = df.apply(lambda x : tag(x, 'name (no mn), manual'), axis = 1)\n",
    "    #df['TagIndex'] = df.apply(lambda x : tag_index(x, 2), axis=1)\n",
    "    #print(df.columns)\n",
    "    print('Found {} matches'.format(len(tempdf[~pd.isna(tempdf[matchtotarget])])))\n",
    "\n",
    "    #Save non blank matchtotarget merged rows to output DF\n",
    "    out = pd.DataFrame() #remake out df - hopefully cut down on memory?\n",
    "    out = pd.concat([out, tempdf[~pd.isna(tempdf[matchtotarget])]], ignore_index=True)\n",
    "    #remove matched rows and then remove excess columns from merging!\n",
    "    print(df.columns)\n",
    "    #df = df[~df[dfunids[0]].isin(out[dfunids[0]].values)][df_columns]\n",
    "    #matchto = matchto[~matchto[matchtotarget].isin(out[matchtotarget].values)] #remove matched ones from matchto\n",
    "\n",
    "    #Printing delta\n",
    "    print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))\n",
    "\n",
    "    print(len(out))\n",
    "\n",
    "    ## new\n",
    "    #out = out[[dffn,dfmn,dfln,matchtofn,matchtomn,matchtoln, 'nametag_df', 'nametag_matchto', 'F132', 'F133', dfdob, matchtodob, matchtodod, dfdod, dfssn, matchtossn, 'PAT_ID', matchtotarget, 'ADD_LINE_1']]\n",
    "    ## \n",
    "    # manualreview_dod_pre = partsim(out)\n",
    "    # filtered = (manualreview_dod_pre[manualreview_dod_pre['modified_namesim_v2'] > 30])\n",
    "    # print('fuark',len(manualreview_dod_pre))\n",
    "    # print('quark',len(filtered))\n",
    "\n",
    "    # with pd.ExcelWriter(r'C:\\Users\\MathiasM\\OneDrive - Cedars-Sinai Health System\\Documents\\dod_match_temp.xlsx') as writer:\n",
    "    #     filtered.to_excel(writer, index = False, sheet_name='full')\n",
    "\n",
    "\n",
    "    manualreview_name = fullsim(out)\n",
    "    ## Removing those where MAX_ENC_DATE is later than F20\n",
    "    # print('Removing those where MAX_ENC_DATE is later than F20') #bc Death Cert matching has accurate dates ?\n",
    "    # print(len(manualreview_name))\n",
    "    # #have to convert datestring to date first I guess lol! MAX_ENC_DATE isn't the right format:\n",
    "    # manualreview_name['MAX_ENC_DATE'] = pd.to_datetime(manualreview_name['MAX_ENC_DATE'], errors='coerce')\n",
    "    # manualreview_name = manualreview_name[~(manualreview_name['MAX_ENC_DATE'] > manualreview_name['F20'])] #take cases where MAX_ENC_DATE is less than F20 (see urnary ~)\n",
    "    # print(len(manualreview_name))\n",
    "\n",
    "    #good\n",
    "    name_good = manualreview_name[\n",
    "                                    #(manualreview_name['namesim'] == 100)\n",
    "                                #| \n",
    "                                    (((manualreview_name['addsim'] > 90) | (manualreview_name['ssnsim'] > 95)) & ((manualreview_name['dobsim'] > 80) | (manualreview_name['dodsim'] > 90) | (manualreview_name['dobsim'] == 0)))\n",
    "                                ]\n",
    "    #mid\n",
    "    name_mid = manualreview_name[\n",
    "                                    (~manualreview_name[dfunids[0]].isin(name_good[dfunids[0]].values)) &\n",
    "                                    (((manualreview_name['dobsim'] > 90) & (manualreview_name['dob_diff'] <= 365)) | (manualreview_name['ssnsim'] > 95) | (manualreview_name['dodsim'] > 90) | (manualreview_name['dobsim'] == 0))\n",
    "                                ]\n",
    "\n",
    "    #everything else\n",
    "    name_bad = manualreview_name[(~manualreview_name['dfname'].isin(name_good['dfname'].values)) & (~manualreview_name['dfname'].isin(name_mid['dfname'].values))]\n",
    "\n",
    "    print(len(name_good), len(name_mid), len(name_bad))\n",
    "\n",
    "    #then do first name and last name only, then if there are a good amount of matches - try just last name -- see filtering scheme (filter w/ DOB first pass)\n",
    "\n",
    "    # with pd.ExcelWriter(r'{}\\DC_name_match_review_{}.xlsx'.format(basepath, savemod)) as writer:\n",
    "    #     manualreview_name.to_excel(writer, index = False, sheet_name='full')\n",
    "    #     name_good.to_excel(writer, index = False, sheet_name = 'good')\n",
    "    #     name_mid.to_excel(writer, index = False, sheet_name = 'mid')\n",
    "    #     name_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "    formatted_save_wc(r'{}\\DC_name_match_review_{}.xlsx'.format(basepath, savemod), [name_good, name_mid, name_bad])\n",
    "\n",
    "    del manualreview_name, name_good, name_mid, name_bad, out\n",
    "    # print(manualreview_name[matchtotarget].value_counts().to_frame())\n",
    "    # print(len(list(manualreview_name[matchtotarget].unique())))\n",
    "\n",
    "    #del manualreview_name #dod_good, dod_mid, dod_bad, manualreview_dod\n",
    "else:\n",
    "    print(\"skipping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:464: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full = pd.concat([good,mid,bad])\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:475: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full[matchtotarget] = bad_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:476: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good_full[matchtotarget] = good_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\NguyenD29\\AppData\\Local\\Temp\\ipykernel_25072\\4016639393.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full['combinedid'] = bad_full[dfunids[0]].astype(str)+\"_\"+bad_full[matchtotarget].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping used from df: 346098 346094 4\n",
      "Dropping used from matchto: 71109 71105 4\n",
      "Number of total good matches: 1018\n",
      "Number of bad matches added to remove list: 1476\n"
     ]
    }
   ],
   "source": [
    "#Removing good matches from pool, returning bad matches to pool.\n",
    "if dropvar == 1:\n",
    "    df, matchto, good_final, combos_to_remove = recombine_general(r'{}\\DC_name_match_review_{}_use.xlsx'.format(basepath, savemod),\n",
    "                                                                  df, matchto, drop_pat_ids, good_final, combos_to_remove)\n",
    "else:\n",
    "    print('Not dropping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8706431 matches\n",
      "\n",
      "Z677372_3052024290241\n",
      "Index(['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'GENDER', 'ADD_LINE_1',\n",
      "       'ADD_LINE_2', 'CITY', 'STATE_C', 'HOME_PHONE', 'SSN', 'MAX_ENC_DATE',\n",
      "       'DEATH_DATE', 'Tag', 'BlankCol_df', 'First Name', 'Middle Name',\n",
      "       'Last Name', 'BlankCol_df11', 'BlankCol_df22', 'First Name_f2',\n",
      "       'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1',\n",
      "       'Last Name_f1', 'ADD_LINE_1_first', 'nametag_df', 'F31', 'F8', 'F20',\n",
      "       'F3', 'F4', 'F5', 'F1', 'F55', 'F56', 'F57', 'F19',\n",
      "       'BlankCol_matchto13', 'F132', 'F133', 'F134', 'F3_f2', 'F5_f2', 'F3_f3',\n",
      "       'F5_f3', 'F3_f1', 'F5_f1', 'F55_first', 'nametag_matchto',\n",
      "       'combinedid'],\n",
      "      dtype='object')\n",
      "Dropped 1622 known bad matches.\n",
      "Creating dfname, matchtoname\n",
      "Starting partsim_lite on 8704809 records\n",
      "8704809\n",
      "Starting fullsim for 1357809 records\n",
      "Forming FULL name columns 2025-07-22 13:11:44.450906\n",
      "Forming address columns 2025-07-22 13:12:01.333548\n",
      "Cleaning addresses 2025-07-22 13:12:27.319748\n",
      "Calculating similarity scores 2025-07-22 13:12:37.629318\n",
      "     dobsim done\n",
      "     namesim done\n",
      "     modified_namesim done\n",
      "Calculating modified namesim v2 2025-07-22 13:13:12.908305\n",
      "Initiating swap: {'elizabh': 'elizabeth'}\n",
      "Initiating swap: {'kathere': 'katherine'}\n",
      "Initiating swap: {'kevinasieb': 'kevinmacasieb'}\n",
      "Initiating swap: {'hyunk': 'hyun'}\n",
      "Initiating swap: {'marymichelle': 'maryannmichelle'}\n",
      "Initiating swap: {'elisabta': 'elisabetta'}\n",
      "Initiating swap: {'hyunk': 'hyun'}\n",
      "     modified_namesim done\n",
      "     modified_namesim_v2 done\n",
      "Calculating address similarity scores 2025-07-22 13:34:44.667170\n",
      "     addsim done\n",
      "     arrestlocsim done\n",
      "     cross_addsim done\n",
      "Calculating auxiliary date similarities 2025-07-22 13:35:06.912096\n",
      "     dodsim done\n",
      "     doddoasim done\n",
      "     doa_maxenc_sim done\n",
      "     doa_maxenc_diff done\n",
      "Calculating date differences 2025-07-22 13:35:31.773588\n",
      "     dob_diff done\n",
      "     dod_diff done\n",
      "     dobdoa_diff done\n",
      "Dropped 33 cases: 1357809 to 1357776 modified_namesim_v2 < 40\n",
      "Dropped 0 duplicates: 1357776 to 1357776\n",
      "Calculating SSN similarities 2025-07-22 13:35:58.163017\n",
      "0 7632 1073271\n",
      "Beginning save (2025-07-22 13:36:07.301902)\n",
      "Could not save df of size 1073271. When limiting by msnv2 > 75: 1053656. Took highest 1.04M rows (by msnv2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected <class 'openpyxl.worksheet.cell_range.MultiCellRange'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\descriptors\\base.py:59\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(expected_type, value)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     value = \u001b[43mexpected_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\worksheet\\cell_range.py:433\u001b[39m, in \u001b[36mMultiCellRange.__init__\u001b[39m\u001b[34m(self, ranges)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ranges, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     ranges = [\u001b[43mCellRange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ranges.split()]\n\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m.ranges = \u001b[38;5;28mset\u001b[39m(ranges)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\worksheet\\cell_range.py:59\u001b[39m, in \u001b[36mCellRange.__init__\u001b[39m\u001b[34m(self, range_string, min_col, min_row, max_col, max_row, title)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.max_col = max_col\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_row\u001b[49m = max_row\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.title = title\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\descriptors\\base.py:112\u001b[39m, in \u001b[36mMin.__set__\u001b[39m\u001b[34m(self, instance, value)\u001b[39m\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mMin value is \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m'\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.min))\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__set__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\descriptors\\base.py:91\u001b[39m, in \u001b[36mMax.__set__\u001b[39m\u001b[34m(self, instance, value)\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value > \u001b[38;5;28mself\u001b[39m.max:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mMax value is \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m'\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.max))\n\u001b[32m     92\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__set__\u001b[39m(instance, value)\n",
      "\u001b[31mValueError\u001b[39m: Max value is 1048576",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(name_good), \u001b[38;5;28mlen\u001b[39m(name_mid), \u001b[38;5;28mlen\u001b[39m(name_bad))\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m#then do first name and last name only, then if there are a good amount of matches - try just last name -- see filtering scheme (filter w/ DOB first pass)\u001b[39;00m\n\u001b[32m     67\u001b[39m \n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# with pd.ExcelWriter(r'{}\\DC_LASTname_match_review_{}.xlsx'.format(basepath, savemod)) as writer:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m \u001b[38;5;66;03m#     else:     \u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m#         name_bad.to_excel(writer, index = False, sheet_name = 'bad')\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[43mformatted_save_wc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDC_LASTname_match_review_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[33;43m.xlsx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavemod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mname_good\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_bad\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m## 91 minute run time for fullsim portion\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m name_good, name_bad, name_mid, manualreview_lastname, filtered_ln\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mformatted_save_wc\u001b[39m\u001b[34m(savepath, list_of_dfs)\u001b[39m\n\u001b[32m     53\u001b[39m addresses_range = addsim_letter + \u001b[38;5;28mstr\u001b[39m(\u001b[32m2\u001b[39m) + \u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m + cross_addsim_letter + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)+\u001b[32m1\u001b[39m)\u001b[38;5;66;03m#as it would be shown in excel also add one at end to skip column name row\u001b[39;00m\n\u001b[32m     54\u001b[39m condrule = CellIsRule(operator = \u001b[33m'\u001b[39m\u001b[33mgreaterThan\u001b[39m\u001b[33m'\u001b[39m, formula = [\u001b[33m'\u001b[39m\u001b[33m89.9\u001b[39m\u001b[33m'\u001b[39m], fill=PatternFill(start_color = \u001b[33m'\u001b[39m\u001b[33mFFFFC7CE\u001b[39m\u001b[33m'\u001b[39m, end_color = \u001b[33m'\u001b[39m\u001b[33mFFFFC7CE\u001b[39m\u001b[33m'\u001b[39m, fill_type=\u001b[33m'\u001b[39m\u001b[33msolid\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mworksheet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconditional_formatting\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddresses_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondrule\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\formatting\\formatting.py:71\u001b[39m, in \u001b[36mConditionalFormattingList.add\u001b[39m\u001b[34m(self, range_string, cfRule)\u001b[39m\n\u001b[32m     69\u001b[39m cf = range_string\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(range_string, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     cf = \u001b[43mConditionalFormatting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrange_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cfRule, Rule):\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOnly instances of openpyxl.formatting.rule.Rule may be added\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\formatting\\formatting.py:29\u001b[39m, in \u001b[36mConditionalFormatting.__init__\u001b[39m\u001b[34m(self, sqref, pivot, cfRule, extLst)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqref=(), pivot=\u001b[38;5;28;01mNone\u001b[39;00m, cfRule=(), extLst=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msqref\u001b[49m = sqref\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m.pivot = pivot\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m.cfRule = cfRule\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\descriptors\\base.py:71\u001b[39m, in \u001b[36mConvertible.__set__\u001b[39m\u001b[34m(self, instance, value)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__set__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance, value):\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mself\u001b[39m.allow_none \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     70\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.allow_none):\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m         value = \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexpected_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__set__\u001b[39m(instance, value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\descriptors\\base.py:61\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(expected_type, value)\u001b[39m\n\u001b[32m     59\u001b[39m         value = expected_type(value)\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mexpected \u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mstr\u001b[39m(expected_type))\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[31mTypeError\u001b[39m: expected <class 'openpyxl.worksheet.cell_range.MultiCellRange'>"
     ]
    }
   ],
   "source": [
    "#### NAME MATCHING, JUST ON LAST NAME (I hope its not too big - likely will need to use partsim_lite for this)\n",
    "#do = 1\n",
    "if do == 1:\n",
    "    del tempdf\n",
    "    prel = len(df)\n",
    "    df.index = range(0,len(df)); matchto.index = range(0,len(matchto))\n",
    "    tempdf = df.merge(matchto[matchto_columns+['nametag_matchto']], how = 'left', left_on=[dfln], right_on=[matchtoln], suffixes=('_df', '_matchto')) \n",
    "    #df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, manual'), axis = 1)\n",
    "    #df['TagIndex'] = df.apply(lambda x : tag_index(x, 2), axis=1)\n",
    "    #print(df.columns)\n",
    "    print('Found {} matches'.format(len(tempdf[~pd.isna(tempdf[matchtotarget])])))\n",
    "\n",
    "    #Save non blank matchtotarget merged rows to output DF\n",
    "    out = pd.DataFrame()\n",
    "    out = pd.concat([out, tempdf[~pd.isna(tempdf[matchtotarget])]], ignore_index=True)\n",
    "    #remove matched rows and then remove excess columns from merging!\n",
    "\n",
    "    #df = df[pd.isna(df[matchtotarget])][df_columns] #remove matched ones from df [OLD method]\n",
    "    #df = df[~df[dfunids[0]].isin(out[dfunids[0]].values)][df_columns]\n",
    "    #print(len(matchto), 'bef')\n",
    "    #matchto = matchto[~matchto[matchtotarget].isin(out[matchtotarget].values)] #remove matched ones from matchto\n",
    "    #print(len(matchto), 'aft')\n",
    "    #Printing delta\n",
    "    #print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))\n",
    "\n",
    "    # print(len(out), 'Starting MAX_ENC_DATE mismatch drop')\n",
    "    # #out['MAX_ENC_DATE'] = pd.to_datetime(out['MAX_ENC_DATE'], errors='coerce')\n",
    "    # out = out[~(out['MAX_ENC_DATE'] > out['F20'])] #take cases where MAX_ENC_DATE is less than F20 (see urnary ~)\n",
    "    # print(len(out))\n",
    "    out = combo_remover(out, combos_to_remove)\n",
    "    print('Creating dfname, matchtoname')\n",
    "    out['dfname'] = out.apply(lambda x : joining(sorted([x[dffn], x[dfmn], x[dfln]]), ''), axis = 1)\n",
    "    out['matchtoname'] = out.apply(lambda x : joining(sorted([x[matchtofn], x[matchtomn], x[matchtoln]]), ''), axis = 1)\n",
    "\n",
    "    print('Starting partsim_lite on {} records'.format(len(out)))\n",
    "    filtered_ln = partsim_lite(out)\n",
    "    print(len(filtered_ln))\n",
    "\n",
    "    try:\n",
    "        del out\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    filtered_ln = filtered_ln[filtered_ln['namesim'] > 75]\n",
    "    print('Starting fullsim for {} records'.format(len(filtered_ln)))\n",
    "\n",
    "    manualreview_lastname = fullsim(filtered_ln)\n",
    "\n",
    "    name_good = manualreview_lastname[\n",
    "                                    (manualreview_lastname['ssnsim'] > 98)\n",
    "                                | \n",
    "                                    (((manualreview_lastname['addsim'] > 90) | (manualreview_lastname['arrestlocsim'] > 90) | (manualreview_lastname['cross_addsim'] > 90)) & \n",
    "                                    ((manualreview_lastname['dobsim'] > 90) & ((manualreview_lastname['dodsim'] > 90) | (manualreview_lastname['dobsim'] == 0))))\n",
    "                                ]\n",
    "    #mid\n",
    "    name_mid = manualreview_lastname[\n",
    "                                    (~manualreview_lastname[dfunids[0]].isin(name_good[dfunids[0]].values)) &\n",
    "                                    (((manualreview_lastname['dobsim'] > 90) & (manualreview_lastname['dob_diff'] <= 365)) | (manualreview_lastname['addsim'] > 95) | (manualreview_lastname['cross_addsim'] > 95) | (manualreview_lastname['arrestlocsim'] > 95) | (manualreview_lastname['dodsim'] > 90) | (manualreview_lastname['dobsim'] == 0))\n",
    "                                ]\n",
    "\n",
    "    #everything else\n",
    "    name_bad = manualreview_lastname[(~manualreview_lastname['dfname'].isin(name_good['dfname'].values)) & (~manualreview_lastname['dfname'].isin(name_mid['dfname'].values))]\n",
    "\n",
    "    print(len(name_good), len(name_mid), len(name_bad))\n",
    "\n",
    "    #then do first name and last name only, then if there are a good amount of matches - try just last name -- see filtering scheme (filter w/ DOB first pass)\n",
    "\n",
    "    # with pd.ExcelWriter(r'{}\\DC_LASTname_match_review_{}.xlsx'.format(basepath, savemod)) as writer:\n",
    "    #     #manualreview_lastname.to_excel(writer, index = False, sheet_name='full')\n",
    "    #     name_good.to_excel(writer, index = False, sheet_name = 'good')\n",
    "    #     name_mid.to_excel(writer, index = False, sheet_name = 'mid')\n",
    "    #     if len(name_bad) > 1048576:\n",
    "    #         name_bad = name_bad[name_bad['modified_namesim_v2'] > 85]\n",
    "    #         print('Cutting down name_bad by filtering mns_v2: {}'.format(len(name_bad)))\n",
    "    #         if len(name_bad) > 1048576:\n",
    "    #             name_bad.iloc[0:10**6].to_excel(writer, index = False, sheet_name = 'bad')\n",
    "    #         else:\n",
    "    #             name_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "    #     else:     \n",
    "    #         name_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "    formatted_save_wc(r'{}\\DC_LASTname_match_review_{}.xlsx'.format(basepath, savemod), [name_good, name_mid, name_bad])\n",
    "    ## 91 minute run time for fullsim portion\n",
    "    del name_good, name_bad, name_mid, manualreview_lastname, filtered_ln\n",
    "    #manualreview_out = fullsim(filtered_ln)\n",
    "    #found 54,418,818 matches lol\n",
    "    #about 20 minute run time to partsim, still yields 16 MILLION matche though\n",
    "else:\n",
    "    print('not running. Check do var.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'formatted_save_wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mformatted_save_wc\u001b[49m(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\\\u001b[39m\u001b[33mDC_LASTname_match_review_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.xlsx\u001b[39m\u001b[33m'\u001b[39m.format(basepath, savemod), [name_good, name_mid, name_bad])\n",
      "\u001b[31mNameError\u001b[39m: name 'formatted_save_wc' is not defined"
     ]
    }
   ],
   "source": [
    "formatted_save_wc(r'{}\\DC_LASTname_match_review_{}.xlsx'.format(basepath, savemod), [name_good, name_mid, name_bad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_27844\\4016639393.py:464: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full = pd.concat([good,mid,bad])\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_27844\\4016639393.py:475: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full[matchtotarget] = bad_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_27844\\4016639393.py:476: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good_full[matchtotarget] = good_full[matchtotarget].apply(lambda x : str(x))\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_27844\\4016639393.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_full['combinedid'] = bad_full[dfunids[0]].astype(str)+\"_\"+bad_full[matchtotarget].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping used from df: 347105 347102 3\n",
      "Dropping used from matchto: 78196 78193 3\n",
      "Number of total good matches: 1139\n",
      "Number of bad matches added to remove list: 0\n"
     ]
    }
   ],
   "source": [
    "if dropvar == 1:\n",
    "    df, matchto, good_final, combos_to_remove = recombine_general(r'{}\\DC_LASTname_match_review_{}_use.xlsx'.format(basepath, savemod),\n",
    "                                                                  df, matchto, drop_pat_ids, good_final, combos_to_remove)\n",
    "\n",
    "else:\n",
    "    print('Not dropping')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not running. Check do var.\n"
     ]
    }
   ],
   "source": [
    "#### NAME MATCHING, JUST ON FIRST NAME (I hope its not too big - likely will need to use partsim_lite for this)\n",
    "\n",
    "if do == 1:\n",
    "    try:\n",
    "        del tempdf, out\n",
    "        print(\"deleted tempdf and out vars\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    prel = len(df)\n",
    "    df.index = range(0,len(df)); matchto.index = range(0,len(matchto))\n",
    "\n",
    "    #tempdf = df.merge(matchto[matchto_columns+['nametag_matchto']], how = 'left', left_on=[dffn], right_on=[matchtofn], suffixes=('_df', '_matchto'))  # NOTE\n",
    "\n",
    "    #columns I need for partsim_lite and identification: \n",
    "    #from df: dfunids[0], dffn, dfmn, dfln\n",
    "    #from matchto: matchtotarget, matchtofn, matchtomn, matchtoln\n",
    "    #try merging with just these and see if you have the memory for it\n",
    "\n",
    "    #tempdf = df[[dfunids[0], dffn, dfmn, dfln, 'MAX_ENC_DATE']].merge(matchto[[matchtotarget, matchtofn, matchtomn, matchtoln, 'F20']], how = 'left', left_on = [dffn], right_on = [matchtofn], suffixes=('_df', '_matchto'))\n",
    "    tempdf = df.merge(matchto, how = 'left', left_on = [dffn], right_on = [matchtofn], suffixes=('_df', '_matchto'))\n",
    "    print(tempdf.shape)\n",
    "    #tempdf.to_csv(r'Z:\\Marco Mathias\\OSCAR\\Match names\\firstname_merge.csv', sep = ',', header = True, index = False)\n",
    "    \n",
    "    \n",
    "    #df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, manual'), axis = 1)\n",
    "    #df['TagIndex'] = df.apply(lambda x : tag_index(x, 2), axis=1)\n",
    "    #print(df.columns)\n",
    "    print('Found {} matches'.format(len(tempdf[~pd.isna(tempdf[matchtotarget])])))\n",
    "\n",
    "    #Save non blank matchtotarget merged rows to output DF\n",
    "    out = pd.DataFrame()\n",
    "    out = pd.concat([out, tempdf[~pd.isna(tempdf[matchtotarget])]], ignore_index=True)\n",
    "    #remove matched rows and then remove excess columns from merging!\n",
    "\n",
    "    try:\n",
    "        del tempdf\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #df = df[pd.isna(df[matchtotarget])][df_columns] #remove matched ones from df [OLD method]\n",
    "    #df = df[~df[dfunids[0]].isin(out[dfunids[0]].values)][df_columns]\n",
    "    #print(len(matchto), 'bef')\n",
    "    #matchto = matchto[~matchto[matchtotarget].isin(out[matchtotarget].values)] #remove matched ones from matchto\n",
    "    #print(len(matchto), 'aft')\n",
    "    #Printing delta\n",
    "    #print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))\n",
    "\n",
    "    # print(len(out), 'Starting MAX_ENC_DATE mismatch drop')\n",
    "    # #out['MAX_ENC_DATE'] = pd.to_datetime(out['MAX_ENC_DATE'], errors='coerce')\n",
    "    # out = out[~(out['MAX_ENC_DATE'] > out['F20'])] #take cases where MAX_ENC_DATE is less than F20 (see urnary ~)\n",
    "    # print(len(out))\n",
    "    out = combo_remover(out, combos_to_remove)\n",
    "    print('Creating dfname, matchtoname')\n",
    "    out['dfname'] = out.apply(lambda x : joining(sorted([x[dffn], x[dfmn], x[dfln]]), ''), axis = 1)\n",
    "    out['matchtoname'] = out.apply(lambda x : joining(sorted([x[matchtofn], x[matchtomn], x[matchtoln]]), ''), axis = 1)\n",
    "\n",
    "    print('Starting partsim_lite on {} records'.format(len(out)))\n",
    "    filtered_ln = partsim_lite(out)\n",
    "    print(len(filtered_ln))\n",
    "\n",
    "    try:\n",
    "        del out\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    filtered_ln = filtered_ln[filtered_ln['namesim'] > 79]\n",
    "    print('Starting fullsim for {} records'.format(len(filtered_ln)))\n",
    "\n",
    "    manualreview_lastname = fullsim(filtered_ln)\n",
    "\n",
    "    name_good = manualreview_lastname[\n",
    "                                    (manualreview_lastname['ssnsim'] > 98)\n",
    "                                | \n",
    "                                    (((manualreview_lastname['addsim'] > 90) | (manualreview_lastname['arrestlocsim'] > 90) | (manualreview_lastname['cross_addsim'] > 90)) & \n",
    "                                    ((manualreview_lastname['dobsim'] > 90) & ((manualreview_lastname['dodsim'] > 90) | (manualreview_lastname['dobsim'] == 0))))\n",
    "                                ]\n",
    "    #mid\n",
    "    name_mid = manualreview_lastname[\n",
    "                                    (~manualreview_lastname[dfunids[0]].isin(name_good[dfunids[0]].values)) &\n",
    "                                    (((manualreview_lastname['dobsim'] > 90) & (manualreview_lastname['dob_diff'] <= 365)) | (manualreview_lastname['addsim'] > 95) | (manualreview_lastname['cross_addsim'] > 95) | (manualreview_lastname['arrestlocsim'] > 95) | (manualreview_lastname['dodsim'] > 90) | (manualreview_lastname['dobsim'] == 0))\n",
    "                                ]\n",
    "\n",
    "    #everything else\n",
    "    name_bad = manualreview_lastname[(~manualreview_lastname['dfname'].isin(name_good['dfname'].values)) & (~manualreview_lastname['dfname'].isin(name_mid['dfname'].values))]\n",
    "\n",
    "    print(len(name_good), len(name_mid), len(name_bad))\n",
    "\n",
    "    #then do first name and last name only, then if there are a good amount of matches - try just last name -- see filtering scheme (filter w/ DOB first pass)\n",
    "\n",
    "    # with pd.ExcelWriter(r'{}\\DC_FIRSTname_match_review_{}.xlsx'.format(basepath, savemod)) as writer:\n",
    "    #     #manualreview_lastname.to_excel(writer, index = False, sheet_name='full')\n",
    "    #     name_good.to_excel(writer, index = False, sheet_name = 'good')\n",
    "    #     name_mid.to_excel(writer, index = False, sheet_name = 'mid')\n",
    "    #     if len(name_bad) > 1048576:\n",
    "    #         name_bad = name_bad[name_bad['modified_namesim_v2'] > 85]\n",
    "    #         print('Cutting down name_bad by filtering mns_v2: {}'.format(len(name_bad)))\n",
    "    #         if len(name_bad) > 1048576:\n",
    "    #             name_bad.iloc[0:10**6].to_excel(writer, index = False, sheet_name = 'bad')\n",
    "    #         else:\n",
    "    #             name_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "    #     else:     \n",
    "    #         name_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "    formatted_save_wc(r'{}\\DC_FIRSTname_match_review_{}.xlsx'.format(basepath, savemod), [name_good, name_mid, name_bad])\n",
    "    ## 91 minute run time for fullsim portion\n",
    "    del name_good, name_bad, name_mid, manualreview_lastname, filtered_ln\n",
    "    #manualreview_out = fullsim(filtered_ln)\n",
    "    #found 54,418,818 matches lol\n",
    "    #about 20 minute run time to partsim, still yields 16 MILLION matches though\n",
    "else:\n",
    "    print('not running. Check do var.')\n",
    "\n",
    "    #271 million... = 16 GB of data :|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find Z:\\Marco Mathias\\OSCAR\\Match names\\DC_FIRSTname_match_review_25redo_use.xlsx\n"
     ]
    }
   ],
   "source": [
    "if dropvar == 1:\n",
    "    df, matchto, good_final, combos_to_remove = recombine_general(r'{}\\DC_FIRSTname_match_review_{}_use.xlsx'.format(basepath, savemod),\n",
    "                                                                  df, matchto, drop_pat_ids, good_final, combos_to_remove)\n",
    "    # tempadjudicated_good, tempadjudicated_bad = recombine(r)\n",
    "\n",
    "\n",
    "    # preldf = len(df) #\n",
    "    # prelmatchto = len(matchto) #\n",
    "    # df = df[~df[dfunids[0]].isin(tempadjudicated_good[dfunids[0]].astype(str).values)][df_columns]\n",
    "    # matchto = matchto[~matchto[matchtotarget].isin(tempadjudicated_good[matchtotarget].astype(str).values)] #fixing dtypes, was causing error.\n",
    "\n",
    "    # good_final = pd.concat([good_final, tempadjudicated_good])\n",
    "\n",
    "    # print('Dropping used from df:', preldf, len(df), (preldf-len(df)))\n",
    "    # print('Dropping used from matchto:', prelmatchto, len(matchto), (prelmatchto-len(matchto)))\n",
    "    # print('Number of total good matches: {}'.format(len(good_final)))\n",
    "\n",
    "    # del tempadjudicated_good, tempadjudicated_bad\n",
    "else:\n",
    "    print('Not dropping')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "# #### NOTE CONTINUATION OF ABOVE CHUNK - JUST DELETE THIS AFTER IT IS DONE RUNNING (if it ever finishes lol)\n",
    "# try:\n",
    "#     del filtered_dob\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# #print('Found {} matches'.format(len(tempdf[~pd.isna(tempdf[matchtotarget])])))\n",
    "\n",
    "# #Save non blank matchtotarget merged rows to output DF\n",
    "# #out = pd.DataFrame() #NOTE\n",
    "# #out = pd.concat([out, tempdf[~pd.isna(tempdf[matchtotarget])]], ignore_index=True) #NOTE\n",
    "# #remove matched rows and then remove excess columns from merging!\n",
    "\n",
    "\n",
    "\n",
    "# #df = df[pd.isna(df[matchtotarget])][df_columns] #remove matched ones from df [OLD method]\n",
    "# #df = df[~df[dfunids[0]].isin(out[dfunids[0]].values)][df_columns]\n",
    "# #print(len(matchto), 'bef')\n",
    "# #matchto = matchto[~matchto[matchtotarget].isin(out[matchtotarget].values)] #remove matched ones from matchto\n",
    "# #print(len(matchto), 'aft')\n",
    "# #Printing delta\n",
    "# #print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))\n",
    "\n",
    "# ### NOTE THIS IS ALL TEMPORARY!!!\n",
    "# #out = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\firstname_merge.csv\", sep = ',', header = 0, dtype = str, engine = 'pyarrow')\n",
    "# do = 0\n",
    "# if do == 1:\n",
    "#     count = 1\n",
    "#     interim = pd.DataFrame()\n",
    "#     out = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\firstname_merge.csv\", sep = ',', header = 0, dtype = {dffn:str, dfmn:str, dfln:str, matchtofn:str, matchtomn:str, matchtoln:str}, engine = 'python', chunksize = 5000000) #chunksize of 5M means we will have 54 to loop thru\n",
    "#     for chunk in out:\n",
    "#         chunk = chunk.merge(df[['PAT_ID', 'MAX_ENC_DATE']], how = 'left', on = ['PAT_ID'])\n",
    "#         #print('Starting merge 2')\n",
    "#         chunk = chunk.merge(matchto[['F1', 'F20']], how = 'left', on = ['F1'])\n",
    "\n",
    "#         chunk[dffn] = chunk[dffn].apply(lambda x : str(x))\n",
    "#         chunk[dfmn] = chunk[dfmn].apply(lambda x : str(x))\n",
    "#         chunk[dfln] = chunk[dfln].apply(lambda x : str(x))\n",
    "\n",
    "#         chunk[matchtofn] = chunk[matchtofn].apply(lambda x : str(x))\n",
    "#         chunk[matchtomn] = chunk[matchtomn].apply(lambda x : str(x))\n",
    "#         chunk[matchtoln] = chunk[matchtoln].apply(lambda x : str(x))\n",
    "\n",
    "#         #print('Starting MAX_ENC_DATE mismatch drop')\n",
    "#         #print(len(chunk))\n",
    "#         #out['MAX_ENC_DATE'] = pd.to_datetime(out['MAX_ENC_DATE'], errors='coerce')\n",
    "#         chunk = chunk[~(chunk['MAX_ENC_DATE'] > chunk['F20'])] #take cases where MAX_ENC_DATE is less than F20 (see urnary ~)\n",
    "#         #print(len(chunk))\n",
    "#         #print(type(chunk['F20'].iloc[0]), type(chunk['MAX_ENC_DATE'].iloc[0]))\n",
    "#         #print(chunk[dffn].dtype, chunk[dfmn].dtype, chunk[dfln].dtype)\n",
    "#     # out = combo_remover(out, combos_to_remove)\n",
    "#         #print('Creating dfname, matchtoname')\n",
    "#         #print(len(chunk))\n",
    "#         chunk['dfname'] = chunk.apply(lambda x : joining(sorted([x[dffn], x[dfmn], x[dfln]]), ''), axis = 1)\n",
    "#         chunk['matchtoname'] = chunk.apply(lambda x : joining(sorted([x[matchtofn], x[matchtomn], x[matchtoln]]), ''), axis = 1)\n",
    "        \n",
    "#         #print('Starting partsim_lite on {} records'.format(len(out)))\n",
    "#         filtered_ln = partsim_lite(chunk)\n",
    "#         #print(len(filtered_ln))\n",
    "\n",
    "#     # try:\n",
    "#     #     del out\n",
    "#     # except:\n",
    "#     #     pass\n",
    "\n",
    "#         filtered_ln = filtered_ln[filtered_ln['namesim'] > 80]\n",
    "#         print(count, len(filtered_ln), datetime.datetime.now())\n",
    "\n",
    "#         if count == 1:\n",
    "#             filtered_ln.to_csv(r'Z:\\Marco Mathias\\OSCAR\\Match names\\firstname_merge_postpartsim.csv', sep = ',', header = True, index = False)\n",
    "#         else:\n",
    "        \n",
    "#             filtered_ln.to_csv(r'Z:\\Marco Mathias\\OSCAR\\Match names\\firstname_merge_postpartsim.csv', sep = ',', header = False, index = False, mode = 'a')\n",
    "#         count = count + 1\n",
    "#     #### NOW YOU NEED TO MERGE BACK THE DF AND MATCHTO COLUMNS REQUIRED FOR FULLSIM TO RUN!!!!!\n",
    "#     #raise Exception('Finished with important part')\n",
    "#     filtered_ln = filtered_ln.merge(df, how = 'left', on = [dfunids[0]])\n",
    "#     print(filtered_ln.shape)\n",
    "#     filtered_ln = filtered_ln.merge(matchto[matchto_columns+['nametag_matchto']], how = 'left', on = [matchtotarget])\n",
    "#     print(filtered_ln.shape)\n",
    "#     #raise Exception('ewwow')\n",
    "\n",
    "\n",
    "#     manualreview_lastname = fullsim(filtered_ln)\n",
    "\n",
    "#     name_good = manualreview_lastname[\n",
    "#                                     (manualreview_lastname['ssnsim'] > 98)\n",
    "#                                 | \n",
    "#                                     (((manualreview_lastname['addsim'] > 90) | (manualreview_lastname['arrestlocsim'] > 90) | (manualreview_lastname['cross_addsim'] > 90)) & \n",
    "#                                     ((manualreview_lastname['dobsim'] > 90) & ((manualreview_lastname['dodsim'] > 90) | (manualreview_lastname['dobsim'] == 0))))\n",
    "#                                 ]\n",
    "#     #mid\n",
    "#     name_mid = manualreview_lastname[\n",
    "#                                     (~manualreview_lastname[dfunids[0]].isin(name_good[dfunids[0]].values)) &\n",
    "#                                     (((manualreview_lastname['dobsim'] > 90) & (manualreview_lastname['dob_diff'] <= 365)) | (manualreview_lastname['addsim'] > 95) | (manualreview_lastname['cross_addsim'] > 95) | (manualreview_lastname['arrestlocsim'] > 95) | (manualreview_lastname['dodsim'] > 90) | (manualreview_lastname['dobsim'] == 0))\n",
    "#                                 ]\n",
    "\n",
    "#     #everything else\n",
    "#     name_bad = manualreview_lastname[(~manualreview_lastname['dfname'].isin(name_good['dfname'].values)) & (~manualreview_lastname['dfname'].isin(name_mid['dfname'].values))]\n",
    "\n",
    "#     print(len(name_good), len(name_mid), len(name_bad))\n",
    "\n",
    "#     #then do first name and last name only, then if there are a good amount of matches - try just last name -- see filtering scheme (filter w/ DOB first pass)\n",
    "\n",
    "#     with pd.ExcelWriter(r'Z:\\Marco Mathias\\OSCAR\\Match names\\d23_half24_FIRSTname_match_review_round1.xlsx') as writer:\n",
    "#         #manualreview_lastname.to_excel(writer, index = False, sheet_name='full')\n",
    "#         name_good.to_excel(writer, index = False, sheet_name = 'good')\n",
    "#         name_mid.to_excel(writer, index = False, sheet_name = 'mid')\n",
    "#         if len(name_bad) > 1048576:\n",
    "#             name_bad = name_bad[name_bad['modified_namesim_v2'] > 85]\n",
    "#             print('Cutting down name_bad by filtering mns_v2: {}'.format(len(name_bad)))\n",
    "#             if len(name_bad) > 1048576:\n",
    "#                 name_bad.iloc[0:10**6].to_excel(writer, index = False, sheet_name = 'bad')\n",
    "#             else:\n",
    "#                 name_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "#         else:     \n",
    "#             name_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "#     ## 91 minute run time for fullsim portion\n",
    "#     del name_good, name_bad, name_mid, manualreview_lastname, filtered_ln, tempdf\n",
    "#     #manualreview_out = fullsim(filtered_ln)\n",
    "#     #found 54,418,818 matches to yield 2 potential matches lol\n",
    "#     #about 20 minute run time to partsim, still yields 16 MILLION matche though\n",
    "\n",
    "\n",
    "#     #271 million... \n",
    "# else:\n",
    "#     print('skipping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "# if do == 1:\n",
    "#     filtered_ln = pd.read_csv(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\firstname_merge_postpartsim.csv\", sep = ',', header = 0)\n",
    "#     filtered_ln['F1'] = filtered_ln['F1'].apply(lambda x : str(x))\n",
    "#     print(filtered_ln.shape)\n",
    "#     filtered_ln = filtered_ln.merge(df[[i for i in df.columns if i not in filtered_ln.columns] + [dfunids[0]]], how = 'left', on = [dfunids[0]])\n",
    "#     print(filtered_ln.shape)\n",
    "#     filtered_ln = filtered_ln.merge(matchto[[i for i in matchto.columns if i not in filtered_ln.columns] + [matchtotarget]], how = 'left', on = [matchtotarget])\n",
    "#     print(filtered_ln.shape)\n",
    "\n",
    "\n",
    "#     #Replacing nan values in name columns! argh!\n",
    "#     print('Fixing nan values')\n",
    "#     filtered_ln[dffn] = filtered_ln[dffn].fillna(value = '')\n",
    "#     filtered_ln[dfmn] = filtered_ln[dfmn].fillna(value = '')\n",
    "#     filtered_ln[dfln] = filtered_ln[dfln].fillna(value = '')\n",
    "\n",
    "#     filtered_ln[matchtofn] = filtered_ln[matchtofn].fillna(value = '')\n",
    "#     filtered_ln[matchtomn] = filtered_ln[matchtomn].fillna(value = '')\n",
    "#     filtered_ln[matchtoln] = filtered_ln[matchtoln].fillna(value = '')\n",
    "\n",
    "\n",
    "#     #raise Exception('ewwow')\n",
    "\n",
    "#     print(filtered_ln.columns)\n",
    "\n",
    "\n",
    "#     manualreview_lastname = fullsim(filtered_ln)\n",
    "\n",
    "#     name_good = manualreview_lastname[\n",
    "#                                     (manualreview_lastname['ssnsim'] > 98)\n",
    "#                                 | \n",
    "#                                     (((manualreview_lastname['addsim'] > 90) | (manualreview_lastname['arrestlocsim'] > 90) | (manualreview_lastname['cross_addsim'] > 90)) & \n",
    "#                                     ((manualreview_lastname['dobsim'] > 90) & ((manualreview_lastname['dodsim'] > 90) | (manualreview_lastname['dobsim'] == 0))))\n",
    "#                                 ]\n",
    "#     #mid\n",
    "#     name_mid = manualreview_lastname[\n",
    "#                                     (~manualreview_lastname[dfunids[0]].isin(name_good[dfunids[0]].values)) &\n",
    "#                                     (((manualreview_lastname['dobsim'] > 90) & (manualreview_lastname['dob_diff'] <= 365)) | (manualreview_lastname['addsim'] > 95) | (manualreview_lastname['cross_addsim'] > 95) | (manualreview_lastname['arrestlocsim'] > 95) | (manualreview_lastname['dodsim'] > 90) | (manualreview_lastname['dobsim'] == 0))\n",
    "#                                 ]\n",
    "\n",
    "#     #everything else\n",
    "#     name_bad = manualreview_lastname[(~manualreview_lastname['dfname'].isin(name_good['dfname'].values)) & (~manualreview_lastname['dfname'].isin(name_mid['dfname'].values))]\n",
    "\n",
    "#     print(len(name_good), len(name_mid), len(name_bad))\n",
    "\n",
    "#     #then do first name and last name only, then if there are a good amount of matches - try just last name -- see filtering scheme (filter w/ DOB first pass)\n",
    "\n",
    "#     with pd.ExcelWriter(r'Z:\\Marco Mathias\\OSCAR\\Match names\\d23_half24_FIRSTname_match_review_round1.xlsx') as writer:\n",
    "#         #manualreview_lastname.to_excel(writer, index = False, sheet_name='full')\n",
    "#         name_good.to_excel(writer, index = False, sheet_name = 'good')\n",
    "#         name_mid.to_excel(writer, index = False, sheet_name = 'mid')\n",
    "#         if len(name_bad) > 1048576:\n",
    "#             name_bad = name_bad[name_bad['modified_namesim_v2'] > 85]\n",
    "#             print('Cutting down name_bad by filtering mns_v2: {}'.format(len(name_bad)))\n",
    "#             if len(name_bad) > 1048576:\n",
    "#                 name_bad.iloc[0:10**6].to_excel(writer, index = False, sheet_name = 'bad')\n",
    "#             else:\n",
    "#                 name_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "#         else:     \n",
    "#             name_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "#     ## 91 minute run time for fullsim portion\n",
    "#     del name_good, name_bad, name_mid, manualreview_lastname, filtered_ln, tempdf\n",
    "#     #manualreview_out = fullsim(filtered_ln)\n",
    "#     #found 54,418,818 matches to yield 2 potential matches lol\n",
    "#     #about 20 minute run time to partsim, still yields 16 MILLION matche though\n",
    "\n",
    "# else:\n",
    "#     print(\"skipping\")\n",
    "#     #271 million... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 149064 matches\n",
      "Index(['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'GENDER', 'ADD_LINE_1',\n",
      "       'ADD_LINE_2', 'CITY', 'HOME_PHONE', 'SSN', 'MAX_ENC_DATE', 'DEATH_DATE',\n",
      "       'Tag', 'BlankCol_df', 'First Name', 'Middle Name', 'Last Name',\n",
      "       'BlankCol_df11', 'BlankCol_df22', 'First Name_f2', 'Last Name_f2',\n",
      "       'First Name_f3', 'Last Name_f3', 'First Name_f1', 'Last Name_f1',\n",
      "       'ADD_LINE_1_first', 'nametag_df'],\n",
      "      dtype='object')\n",
      "347113 to 347102 difference: 11\n",
      "149064\n",
      "Forming FULL name columns 2025-06-06 10:19:17.189880\n",
      "Forming address columns 2025-06-06 10:19:19.630489\n",
      "Cleaning addresses 2025-06-06 10:19:23.192862\n",
      "Calculating similarity scores 2025-06-06 10:19:24.714911\n",
      "     dobsim done\n",
      "     namesim done\n",
      "     modified_namesim done\n",
      "Calculating modified namesim v2 2025-06-06 10:19:29.907043\n",
      "Initiating swap: {'guadupe': 'guadalupe'}\n",
      "     modified_namesim done\n",
      "     modified_namesim_v2 done\n",
      "Calculating address similarity scores 2025-06-06 10:23:22.938596\n",
      "     addsim done\n",
      "     arrestlocsim done\n",
      "     cross_addsim done\n",
      "Calculating auxiliary date similarities 2025-06-06 10:23:25.807082\n",
      "     dodsim done\n",
      "     doddoasim done\n",
      "     doa_maxenc_sim done\n",
      "     doa_maxenc_diff done\n",
      "Calculating date differences 2025-06-06 10:23:29.347343\n",
      "     dob_diff done\n",
      "     dod_diff done\n",
      "     dobdoa_diff done\n",
      "Dropped 14803 cases: 149064 to 134261 modified_namesim_v2 < 40\n",
      "Dropped 0 duplicates: 134261 to 134261\n",
      "Calculating SSN similarities 2025-06-06 10:23:32.818204\n",
      "4 5019 54822\n",
      "Beginning save (2025-06-06 10:23:33.883317)\n"
     ]
    }
   ],
   "source": [
    "## ADDRESS MATCH (see below for formation - may need to edit / functionalize better\n",
    "if do == 1:\n",
    "  matchto['matchtoaddress'] = matchto.apply(lambda x : joining(([x[matchtoaddress], x[matchtoaddress2]]), ''), axis = 1) #forming combined address\n",
    "  matchto['matchtoaddress'] = matchto['matchtoaddress'].apply(lambda x : address_clean(x))\n",
    "  df['ADD_LINE_1'] = df['ADD_LINE_1'].apply(lambda x : address_clean(x))\n",
    "\n",
    "  df.index = range(0,len(df)); matchto.index = range(0,len(matchto))\n",
    "  tempdf = df.merge(matchto[matchto_columns+['nametag_matchto', 'matchtoaddress']], how = 'left', left_on=['ADD_LINE_1'], right_on=['matchtoaddress'], suffixes=('_df', '_matchto')) \n",
    "  #df['Tag'] = df.apply(lambda x : tag(x, 'name (no mn), manual'), axis = 1)\n",
    "  #df['TagIndex'] = df.apply(lambda x : tag_index(x, 2), axis=1)\n",
    "  #print(df.columns)\n",
    "  print('Found {} matches'.format(len(tempdf[~pd.isna(tempdf[matchtotarget])])))\n",
    "\n",
    "  #Save non blank matchtotarget merged rows to output DF\n",
    "  out = pd.DataFrame() #remake out df - hopefully cut down on memory?\n",
    "  out = pd.concat([out, tempdf[~pd.isna(tempdf[matchtotarget])]], ignore_index=True)\n",
    "  #remove matched rows and then remove excess columns from merging!\n",
    "  print(df.columns)\n",
    "  #df = df[~df[dfunids[0]].isin(out[dfunids[0]].values)][df_columns]\n",
    "  #matchto = matchto[~matchto[matchtotarget].isin(out[matchtotarget].values)] #remove matched ones from matchto\n",
    "\n",
    "  #Printing delta\n",
    "  print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))\n",
    "\n",
    "  print(len(out))\n",
    "\n",
    "  manualreview_address = fullsim(out)\n",
    "\n",
    "\n",
    "  #remove courtorder, courtorderdelayed, delayed\n",
    "  manualreview_address = manualreview_address[~manualreview_address[matchtoln].isin(['courtorderdelayed', 'courtorder', 'delayed'])]\n",
    "  #remove cases where both addresses are blank...\n",
    "  #manualreview_address = manualreview_address[(manualreview_address['ADD_LINE_1'] != '') & (manualreview_address['matchtoaddress'] != '')]\n",
    "  # manualreview_address = manualreview_address[manualreview_address['addsim'] == 100]\n",
    "  # print('manualreview_address size: {}'.format(len(manualreview_address)))\n",
    "\n",
    "  add_good = manualreview_address[\n",
    "                                  #(manualreview_name['namesim'] == 100)\n",
    "                                #| \n",
    "                                  (((manualreview_address['modified_namesim_v2'] > 90) | (manualreview_address['ssnsim'] > 95)) & ((manualreview_address['dobsim'] > 85) | (manualreview_address['dodsim'] > 90) | (manualreview_address['dobsim'] == 0)))\n",
    "                                ]\n",
    "  #mid\n",
    "  add_mid = manualreview_address[\n",
    "                                  (~manualreview_address[dfunids[0]].isin(add_good[dfunids[0]].values)) &\n",
    "                                  (((manualreview_address['dobsim'] > 90) & ((manualreview_address['modified_namesim_v2'] > 75))) | ((manualreview_address['dob_diff'] <= 365) | (manualreview_address['ssnsim'] > 95) | (manualreview_address['dodsim'] > 90) | (manualreview_address['dobsim'] == 0)))\n",
    "                                ]\n",
    "\n",
    "  #everything else\n",
    "  add_bad = manualreview_address[(~manualreview_address['dfname'].isin(add_good['dfname'].values)) & (~manualreview_address['dfname'].isin(add_mid['dfname'].values))]\n",
    "\n",
    "  print(len(add_good), len(add_mid), len(add_bad))\n",
    "\n",
    "\n",
    "  # with pd.ExcelWriter(r\"{}\\DC_address_match_review_{}.xlsx\".format(basepath, savemod)) as writer:\n",
    "  #     manualreview_address.to_excel(writer, index = False, sheet_name = 'full')\n",
    "  #     add_good.to_excel(writer, index = False, sheet_name = 'good')\n",
    "  #     add_mid.to_excel(writer, index = False, sheet_name = 'mid')\n",
    "  #     add_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "  formatted_save_wc(r\"{}\\DC_address_match_review_{}.xlsx\".format(basepath, savemod), [add_good, add_mid, add_bad])\n",
    "else:\n",
    "  print('skipping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_27844\\4016639393.py:464: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full = pd.concat([good,mid,bad])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping used from df: 347102 347102 0\n",
      "Dropping used from matchto: 78193 78193 0\n",
      "Number of total good matches: 1139\n",
      "Number of bad matches added to remove list: 158\n"
     ]
    }
   ],
   "source": [
    "if dropvar == 1:\n",
    "    df, matchto, good_final, combos_to_remove = recombine_general(r\"{}\\DC_address_match_review_{}_use.xlsx\".format(basepath, savemod),\n",
    "                                                              df, matchto, drop_pat_ids, good_final, combos_to_remove)\n",
    "else:\n",
    "    print('Not dropping')\n",
    "#     tempadjudicated_good, tempadjudicated_bad = recombine(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\d25_address_match_review_round1_use.xlsx\")\n",
    "\n",
    "\n",
    "#     preldf = len(df) #\n",
    "#     prelmatchto = len(matchto) #\n",
    "#     df = df[~df[dfunids[0]].isin(tempadjudicated_good[dfunids[0]].astype(str).values)][df_columns]\n",
    "#     matchto = matchto[~matchto[matchtotarget].isin(tempadjudicated_good[matchtotarget].astype(str).values)] #fixing dtypes, was causing error.\n",
    "\n",
    "#     good_final = pd.concat([good_final, tempadjudicated_good])\n",
    "\n",
    "#     print('Dropping used from df:', preldf, len(df), (preldf-len(df)))\n",
    "#     print('Dropping used from matchto:', prelmatchto, len(matchto), (prelmatchto-len(matchto)))\n",
    "#     print('Number of total good matches: {}'.format(len(good_final)))\n",
    "\n",
    "#     del tempadjudicated_good, tempadjudicated_bad\n",
    "# else:\n",
    "#     print('Not dropping')\n",
    "# r\"{}\\DC_address_match_review_{}.xlsx\".format(basepath, savemod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count\n",
      "PAT_ID         \n",
      "Z2348041      1\n",
      "Z1847851      1\n",
      "Z3319921      1\n",
      "Z1635390      1\n",
      "Z3784043      1\n",
      "...         ...\n",
      "Z706146       1\n",
      "Z1017239      1\n",
      "Z1422986      1\n",
      "Z2598951      1\n",
      "Z113700       1\n",
      "\n",
      "[1139 rows x 1 columns]\n",
      "1139\n"
     ]
    }
   ],
   "source": [
    "print(good_final[dfunids[0]].value_counts().to_frame())\n",
    "print(len(good_final))\n",
    "with pd.ExcelWriter(r\"{}\\DC_final_{}.xlsx\".format(basepath, savemod)) as writer:\n",
    "    good_final.to_excel(writer, index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'ADD_LINE_1', 'ADD_LINE_2', 'CITY', 'STATE_C', 'STATE', 'COUNTRY_C', 'COUNTRY', 'HOME_PHONE', 'SSN', 'DEATH_DATE', 'MAX(CONTACT_DATE)', 'Tag', 'gender', 'First Name', 'Middle Name', 'Last Name', 'First Name_f2', 'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1', 'Last Name_f1', 'ADD_LINE_1_first', 'nametag_df']\n",
      "Found 3374173 matches\n",
      "Index(['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'ADD_LINE_1', 'ADD_LINE_2',\n",
      "       'CITY', 'STATE_C', 'STATE', 'COUNTRY_C', 'COUNTRY', 'HOME_PHONE', 'SSN',\n",
      "       'DEATH_DATE', 'MAX(CONTACT_DATE)', 'Tag', 'gender', 'First Name',\n",
      "       'Middle Name', 'Last Name', 'First Name_f2', 'Last Name_f2',\n",
      "       'First Name_f3', 'Last Name_f3', 'First Name_f1', 'Last Name_f1',\n",
      "       'ADD_LINE_1_first', 'nametag_df', 'F8', 'F20', 'F3', 'F4', 'F5', 'F1',\n",
      "       'F3_f2', 'F5_f2', 'F3_f3', 'F5_f3', 'F3_f1', 'F5_f1', 'F31',\n",
      "       'F132_first', 'F132', 'F133', 'F134', 'F19', 'nametag_matchto',\n",
      "       'TagIndex'],\n",
      "      dtype='object')\n",
      "341828 bef\n",
      "19590 aft\n",
      "350897 to 34506 difference: 316391\n",
      "3374173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:236: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf[dfmn] = indf[dfmn].apply(lambda x : x.replace('.', ''))\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf[matchtomn] = indf[matchtomn].apply(lambda x : x.replace('.', ''))\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:240: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['dfname'] = indf.apply(lambda x : joining(sorted([x[dffn], x[dfmn], x[dfln]]), ''), axis = 1)\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:241: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['matchtoname'] = indf.apply(lambda x : joining(sorted([x[matchtofn], x[matchtomn], x[matchtoln]]), ''), axis = 1)\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:244: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['nametag_match'] = indf.apply(lambda x : 'equal' if x['nametag_df'] == x['nametag_matchto'] else 'not equal', axis = 1)\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['matchtoaddress'] = indf.apply(lambda x : joining(([x['F132'], x['F133']]), ''), axis = 1) #forming combined address\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:249: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['matchtoaddress'] = indf['matchtoaddress'].apply(lambda x : address_clean(x))\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:250: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['ADD_LINE_1'] = indf['ADD_LINE_1'].apply(lambda x : address_clean(x))\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:253: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['dobsim'] = indf.apply(lambda x : calc_distances_jw(x[dfdob], x[matchtodob]), axis = 1)\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:254: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['namesim'] = indf.apply(lambda x : calc_distances_jw(x['dfname'], x['matchtoname']), axis = 1)\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:255: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['modified_namesim'] = indf.apply(lambda x : modified_namesim(x), axis = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating swap: {'greensh': 'greentash'}\n",
      "Initiating swap: {'sanderswilams': 'sanderswilliams'}\n",
      "Initiating swap: {'garciacervtes': 'garciacervantes'}\n",
      "Initiating swap: {'joannelene': 'joannemaylene'}\n",
      "Initiating swap: {'rodriez': 'rodriguez'}\n",
      "Initiating swap: {'otacmons': 'otaclemons'}\n",
      "Initiating swap: {'timothyhn': 'timothyjohn'}\n",
      "Initiating swap: {'rodriguezares': 'rodriguezlinares'}\n",
      "Initiating swap: {'alejdro': 'alejandro'}\n",
      "Initiating swap: {'nicotti': 'nicoletti'}\n",
      "Initiating swap: {'rareztabares': 'ramireztabares'}\n",
      "Initiating swap: {'cathere': 'catherine'}\n",
      "Initiating swap: {'delcidmoras': 'delcidmorales'}\n",
      "Initiating swap: {'seguradoguez': 'seguradominguez'}\n",
      "Initiating swap: {'mclaughn': 'mclaughlin'}\n",
      "Initiating swap: {'fleishnlevy': 'fleishmanlevy'}\n",
      "Initiating swap: {'santosrtinez': 'santosmartinez'}\n",
      "Initiating swap: {'borzouh': 'borzouyeh'}\n",
      "Initiating swap: {'camarillodrade': 'camarilloandrade'}\n",
      "Initiating swap: {'rusciolli': 'rusciolelli'}\n",
      "Initiating swap: {'mathewales': 'mathewmorales'}\n",
      "Initiating swap: {'solorzo': 'solorzano'}\n",
      "Initiating swap: {'macugall': 'macdougall'}\n",
      "Initiating swap: {'soderndrobison': 'soderlundrobison'}\n",
      "Initiating swap: {'futtern': 'futterman'}\n",
      "Initiating swap: {'renschr': 'renschler'}\n",
      "Initiating swap: {'luaamath': 'luangamath'}\n",
      "Initiating swap: {'macasang': 'macalisang'}\n",
      "Initiating swap: {'herndez': 'hernandez'}\n",
      "Initiating swap: {'manennchi': 'manendanchi'}\n",
      "Initiating swap: {'vasquezmelr': 'vasquezmelchor'}\n",
      "Initiating swap: {'crisantocayeo': 'crisantocayetano'}\n",
      "Initiating swap: {'tolento': 'tolentino'}\n",
      "Initiating swap: {'schoover': 'schoonover'}\n",
      "Initiating swap: {'shibani': 'sheibani'}\n",
      "Initiating swap: {'sgaragno': 'sgaraglino'}\n",
      "Initiating swap: {'eganwir': 'eganwiler'}\n",
      "Initiating swap: {'mittman': 'mittleman'}\n",
      "Initiating swap: {'guadupe': 'guadalupe'}\n",
      "Initiating swap: {'boisselkle': 'boisselkottle'}\n",
      "Initiating swap: {'milngton': 'millington'}\n",
      "Initiating swap: {'kharitov': 'kharitonov'}\n",
      "Initiating swap: {'chaourakow': 'chaoulirakow'}\n",
      "Initiating swap: {'tahvian': 'tahvilian'}\n",
      "Initiating swap: {'cngcastillo': 'chongcastillo'}\n",
      "Initiating swap: {'wilamson': 'williamson'}\n",
      "Initiating swap: {'maryeln': 'maryellen'}\n",
      "Initiating swap: {'debisscp': 'debisschop'}\n",
      "Initiating swap: {'grotuse': 'grothouse'}\n",
      "Initiating swap: {'littjohn': 'littlejohn'}\n",
      "Initiating swap: {'chrispher': 'christopher'}\n",
      "Initiating swap: {'suleynov': 'suleymanov'}\n",
      "Initiating swap: {'cathere': 'catherine'}\n",
      "Initiating swap: {'radcffe': 'radcliffe'}\n",
      "Initiating swap: {'certenberdpips': 'certenberdphillips'}\n",
      "Initiating swap: {'stepnie': 'stephanie'}\n",
      "Initiating swap: {'harvicz': 'harvilicz'}\n",
      "Initiating swap: {'vochaisaree': 'vongchaisaree'}\n",
      "Initiating swap: {'benayao': 'benayahoo'}\n",
      "Initiating swap: {'huntingn': 'huntington'}\n",
      "Initiating swap: {'chingaryan': 'chilingaryan'}\n",
      "Initiating swap: {'burrwilams': 'burrwilliams'}\n",
      "Initiating swap: {'stepnie': 'stephanie'}\n",
      "Initiating swap: {'gonzazcorona': 'gonzalezcorona'}\n",
      "Initiating swap: {'aleksdr': 'aleksandr'}\n",
      "Initiating swap: {'balvanedapaes': 'balvanedaparedes'}\n",
      "Initiating swap: {'krishurthy': 'krishnamurthy'}\n",
      "Initiating swap: {'washingtshepard': 'washingtonshepard'}\n",
      "Initiating swap: {'elizath': 'elizabeth'}\n",
      "Initiating swap: {'preissgrenier': 'preissgreninger'}\n",
      "Initiating swap: {'christe': 'christine'}\n",
      "Initiating swap: {'chevaer': 'chevalier'}\n",
      "Initiating swap: {'donaldn': 'donaldson'}\n",
      "Initiating swap: {'peraltadomiuez': 'peraltadominguez'}\n",
      "Initiating swap: {'domiuez': 'dominguez'}\n",
      "Initiating swap: {'jacquele': 'jacqueline'}\n",
      "Initiating swap: {'elizath': 'elizabeth'}\n",
      "Initiating swap: {'bakalck': 'bakalchuk'}\n",
      "Initiating swap: {'akinforin': 'akinfolarin'}\n",
      "Initiating swap: {'delaente': 'delafuente'}\n",
      "Initiating swap: {'selwitzbriga': 'selwitzbriglia'}\n",
      "Initiating swap: {'morasrios': 'moralesrios'}\n",
      "Initiating swap: {'hoshizakipetern': 'hoshizakipeterson'}\n",
      "Initiating swap: {'bereani': 'berelyani'}\n",
      "Initiating swap: {'gomezdelapn': 'gomezdelaplain'}\n",
      "Initiating swap: {'soleyny': 'soleymany'}\n",
      "Initiating swap: {'wilamson': 'williamson'}\n",
      "Initiating swap: {'bartsser': 'bartmasser'}\n",
      "Initiating swap: {'kanusulain': 'kanusulaiman'}\n",
      "Initiating swap: {'anne': 'annie'}\n",
      "Initiating swap: {'kaitlynsavah': 'kaitlynsavannah'}\n",
      "Initiating swap: {'harrisontell': 'harrisonantell'}\n",
      "Initiating swap: {'contrerasvez': 'contrerasvelez'}\n",
      "Initiating swap: {'yagubian': 'yaghoubian'}\n",
      "Initiating swap: {'maldonadernandez': 'maldonadohernandez'}\n",
      "Initiating swap: {'gorokvskaya': 'gorokhovskaya'}\n",
      "Initiating swap: {'soimanpour': 'soleimanpour'}\n",
      "Initiating swap: {'schlaerhelvey': 'schlangerhelvey'}\n",
      "Initiating swap: {'richardn': 'richardson'}\n",
      "Initiating swap: {'scarbugh': 'scarbrough'}\n",
      "Initiating swap: {'welshwrell': 'welshworrell'}\n",
      "Initiating swap: {'schindr': 'schindler'}\n",
      "Initiating swap: {'christe': 'christine'}\n",
      "Initiating swap: {'donaldn': 'donaldson'}\n",
      "Initiating swap: {'iacobels': 'iacobellis'}\n",
      "Initiating swap: {'bleckmn': 'bleckmann'}\n",
      "Initiating swap: {'cervtes': 'cervantes'}\n",
      "Initiating swap: {'mirandabrie': 'mirandabrianne'}\n",
      "Initiating swap: {'stigano': 'stigliano'}\n",
      "Initiating swap: {'mirandabrie': 'mirandabrianne'}\n",
      "Initiating swap: {'pattern': 'patterson'}\n",
      "Initiating swap: {'pfeffernn': 'pfeffermann'}\n",
      "Initiating swap: {'armelposkin': 'armelpoliskin'}\n",
      "Initiating swap: {'schilng': 'schilling'}\n",
      "Initiating swap: {'silvern': 'silverman'}\n",
      "Initiating swap: {'kathere': 'katherine'}\n",
      "Initiating swap: {'beyarsn': 'beyarslan'}\n",
      "Initiating swap: {'shavaan': 'shavalian'}\n",
      "Initiating swap: {'marceno': 'marcelino'}\n",
      "Initiating swap: {'kupusswy': 'kupusswamy'}\n",
      "Initiating swap: {'stepnie': 'stephanie'}\n",
      "Initiating swap: {'grajalessanval': 'grajalessandoval'}\n",
      "Initiating swap: {'chriopher': 'christopher'}\n",
      "Initiating swap: {'theorescu': 'theodorescu'}\n",
      "Initiating swap: {'serranocarna': 'serranocarmona'}\n",
      "Initiating swap: {'ziglboim': 'zighelboim'}\n",
      "Initiating swap: {'corneus': 'cornelius'}\n",
      "Initiating swap: {'marialrosario': 'mariadelrosario'}\n",
      "Initiating swap: {'harraue': 'harrangue'}\n",
      "Initiating swap: {'hailejohnn': 'hailejohnson'}\n",
      "Initiating swap: {'mclaughn': 'mclaughlin'}\n",
      "Initiating swap: {'stephie': 'stephanie'}\n",
      "Initiating swap: {'carringn': 'carrington'}\n",
      "Initiating swap: {'brilant': 'brilliant'}\n",
      "Initiating swap: {'johnsonfulr': 'johnsonfuller'}\n",
      "Initiating swap: {'kathere': 'katherine'}\n",
      "Initiating swap: {'cathere': 'catherine'}\n",
      "Initiating swap: {'jamesel': 'jamesjoel'}\n",
      "Initiating swap: {'kathene': 'katherine'}\n",
      "Initiating swap: {'franknjr': 'franklinjr'}\n",
      "Initiating swap: {'castello': 'castellano'}\n",
      "Initiating swap: {'silberkit': 'silberkleit'}\n",
      "Initiating swap: {'tecuaneymendez': 'tecuanhueymendez'}\n",
      "Initiating swap: {'moseleycafee': 'moseleymacafee'}\n",
      "Initiating swap: {'patlanrtinez': 'patlanmartinez'}\n",
      "Initiating swap: {'biscega': 'bisceglia'}\n",
      "Initiating swap: {'elkhuvich': 'elkhunovich'}\n",
      "Initiating swap: {'ngunpham': 'nguyenpham'}\n",
      "Initiating swap: {'kaitlynsavah': 'kaitlynsavannah'}\n",
      "Initiating swap: {'priscla': 'priscilla'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['dname'], indf['mname'], indf['type'] = fnmnln_vec(indf[dffn], indf[dfmn], indf[dfln], indf[matchtofn], indf[matchtomn], indf[matchtoln])\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['dname'], indf['mname'], indf['type'] = fnmnln_vec(indf[dffn], indf[dfmn], indf[dfln], indf[matchtofn], indf[matchtomn], indf[matchtoln])\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['dname'], indf['mname'], indf['type'] = fnmnln_vec(indf[dffn], indf[dfmn], indf[dfln], indf[matchtofn], indf[matchtomn], indf[matchtoln])\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:261: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['modified_namesim_v2'] = indf.apply(lambda x : calc_distances_jw(x['dname'], x['mname']), axis = 1)\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:263: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['addsim'] = indf.apply(lambda x : calc_distances_jw(x['matchtoaddress'], x['ADD_LINE_1']), axis = 1)\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:264: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['dodsim'] = indf.apply(lambda x : calc_distances_jw(x[dfdod], x[matchtodod]), axis = 1)\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:265: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['ssnsim'] = indf.apply(lambda x : calc_distances_jw(x[dfssn], x[matchtossn]), axis = 1)\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:268: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['dob_diff'] = indf.apply(lambda x : date_diffs(x[dfdob], x[matchtodob]), axis = 1)\n",
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_20436\\1034162498.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indf['dod_diff'] = indf.apply(lambda x : date_diffs(x[dfdod], x[matchtodod]), axis = 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This sheet is too large! Your sheet size is: 3374173, 30 Max sheet size is: 1048576, 16384",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mExcelWriter(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMathiasM\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdob_match_review.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmanualreview_dob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:2345\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   2332\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2334\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2335\u001b[0m     df,\n\u001b[0;32m   2336\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2343\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2344\u001b[0m )\n\u001b[1;32m-> 2345\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\formats\\excel.py:932\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    930\u001b[0m num_rows, num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_rows \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows \u001b[38;5;129;01mor\u001b[39;00m num_cols \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cols:\n\u001b[1;32m--> 932\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis sheet is too large! Your sheet size is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax sheet size is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m     )\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    938\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mValueError\u001b[0m: This sheet is too large! Your sheet size is: 3374173, 30 Max sheet size is: 1048576, 16384"
     ]
    }
   ],
   "source": [
    "#with pd.ExcelWriter(r'C:\\Users\\MathiasM\\OneDrive - Cedars-Sinai Health System\\Documents\\dob_match_review.xlsx') as writer:\n",
    "    #manualreview_dob.to_excel(writer, index = False, sheet_name='full')\n",
    "\n",
    "    #do DOB 1:1 matching, CUT down based on DOD (within 1 year / 365 days) [instead of namesim] - make partsim w/ dod_diff score\n",
    "    #Then on that subset - run similarity score\n",
    "\n",
    "    #saving as .csv file - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manualreview_dob.to_csv(r'C:\\Users\\MathiasM\\Documents\\dob_match_review.csv', sep =',' , header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3374173\n",
      "320 123 3366585\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This sheet is too large! Your sheet size is: 3366585, 30 Max sheet size is: 1048576, 16384",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m     dob_good\u001b[38;5;241m.\u001b[39mto_excel(writer, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, sheet_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgood\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     dob_mid\u001b[38;5;241m.\u001b[39mto_excel(writer, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, sheet_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mdob_bad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#  OG sheet size is: 3374173, 30 \u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Max sheet size is: 1048576, 16384\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#########\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:2345\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   2332\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2334\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2335\u001b[0m     df,\n\u001b[0;32m   2336\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2343\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2344\u001b[0m )\n\u001b[1;32m-> 2345\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\formats\\excel.py:932\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    930\u001b[0m num_rows, num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_rows \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows \u001b[38;5;129;01mor\u001b[39;00m num_cols \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cols:\n\u001b[1;32m--> 932\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis sheet is too large! Your sheet size is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax sheet size is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m     )\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    938\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mValueError\u001b[0m: This sheet is too large! Your sheet size is: 3366585, 30 Max sheet size is: 1048576, 16384"
     ]
    }
   ],
   "source": [
    "\n",
    "    #dob_bad.to_excel(writer, index = False, sheet_name = 'bad')\n",
    "\n",
    "#save bad as csv so we don't lose it \n",
    "\n",
    "\n",
    "#  OG sheet size is: 3374173, 30 \n",
    "# Max sheet size is: 1048576, 16384\n",
    "\n",
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'ADD_LINE_1', 'ADD_LINE_2', 'CITY', 'STATE_C', 'STATE', 'COUNTRY_C', 'COUNTRY', 'HOME_PHONE', 'SSN', 'DEATH_DATE', 'MAX(CONTACT_DATE)', 'Tag', 'gender', 'First Name', 'Middle Name', 'Last Name', 'First Name_f2', 'Last Name_f2', 'First Name_f3', 'Last Name_f3', 'First Name_f1', 'Last Name_f1', 'ADD_LINE_1_first']\n",
      "Found 2160 matches\n",
      "357186 to 355026 difference: 2160\n"
     ]
    }
   ],
   "source": [
    "### 2. new version of first: DOB, DOD, FN, LN (correct order) step matching using pd.merge\n",
    "\n",
    "out = pd.DataFrame()\n",
    "print(df_columns)\n",
    "prel = len(df)\n",
    "df.index = range(0,len(df)); matchto.index = range(0,len(matchto))\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn, dfln], right_on=[matchtodob, matchtodod, matchtofn, matchtoln], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN, LN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 2), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_31232\\507847455.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355026 to 355026 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#3. DOB, DOD, FN as LN, LN as FN (switched order) step matching using pd.merge\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dfln, dffn], right_on=[matchtodob, matchtodod, matchtofn, matchtoln], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN as LN, LN as FN (switched)'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 3), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1443 matches\n",
      "353891 to 353891 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#4. DOB, FN, LN, address_1 (if DOD is blank) ### need to add clause for DOD blank - testing in Untilted-4.ipynb now (dropping by MRN)\n",
    "df_blankdod = df[pd.isna(df[dfdod])] #form blankdod subdf (important to do it each time because things are dropped from df not df_blankdod or df_blankdob)\n",
    "prel = len(df_blankdod)\n",
    "#####\n",
    "df_blankdod = df_blankdod.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn, dfln, dfaddress_1], right_on=[matchtodob, matchtofn, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "#####\n",
    "\n",
    "df_blankdod['Tag'] = df_blankdod.apply(lambda x : tag(x, 'DOB, FN, LN, address_1 (if DOD is blank)'), axis = 1)\n",
    "df_blankdod['TagIndex'] = df_blankdod.apply(lambda x : tag_index(x, 4), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df_blankdod[~pd.isna(df_blankdod[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df_blankdod[~pd.isna(df_blankdod[matchtotarget])]], ignore_index=True)\n",
    "#Getting list of patients to drop\n",
    "exclu=df_blankdod[~pd.isna(df_blankdod[matchtotarget])][dfunids[0]].to_list()\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[~df[dfunids[0]].isin(exclu)][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df_blankdod), 'difference: {}'.format(prel-len(df_blankdod)))\n",
    "try:\n",
    "    del df_blankdod\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_31232\\1899714524.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat([out, df_blankdod[~pd.isna(df_blankdod[matchtotarget])]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352448 to 352448 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#5. DOB, FN as LN, LN as FN, address_1 (if DOD is blank) (switched)\n",
    "df_blankdod = df[pd.isna(df[dfdod])] #form blankdod subdf (important to do it each time because things are dropped from df not df_blankdod or df_blankdob)\n",
    "prel = len(df_blankdod)\n",
    "#####\n",
    "df_blankdod = df_blankdod.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfln, dffn, dfaddress_1], right_on=[matchtodob, matchtofn, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "#####\n",
    "\n",
    "df_blankdod['Tag'] = df_blankdod.apply(lambda x : tag(x, 'DOB, FN as LN, LN as FN, address_1 (if DOD is blank) (switched)'), axis = 1)\n",
    "df_blankdod['TagIndex'] = df_blankdod.apply(lambda x : tag_index(x, 5), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df_blankdod[~pd.isna(df_blankdod[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df_blankdod[~pd.isna(df_blankdod[matchtotarget])]], ignore_index=True)\n",
    "#Getting list of patients to drop\n",
    "exclu=df_blankdod[~pd.isna(df_blankdod[matchtotarget])][dfunids[0]].to_list()\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[~df[dfunids[0]].isin(exclu)][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df_blankdod), 'difference: {}'.format(prel-len(df_blankdod)))\n",
    "try:\n",
    "    del df_blankdod\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches\n",
      "0 to 0 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#6. DOD, FN, LN, address_1 (IF DOB IS BLANK)***\n",
    "df_blankDOB = df[pd.isna(df[dfdob])] #form blankdod subdf (important to do it each time because things are dropped from df not df_blankdod or df_blankdob)\n",
    "prel = len(df_blankDOB)\n",
    "#####\n",
    "df_blankDOB = df_blankDOB.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dffn, dfln, dfaddress_1], right_on=[matchtodod, matchtofn, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "#####\n",
    "\n",
    "df_blankDOB['Tag'] = df_blankDOB.apply(lambda x : tag(x, 'DOD, FN, LN, address_1 (if DOB is blank)'), axis = 1)\n",
    "df_blankDOB['TagIndex'] = df_blankDOB.apply(lambda x : tag_index(x, 6), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df_blankDOB[~pd.isna(df_blankDOB[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df_blankDOB[~pd.isna(df_blankDOB[matchtotarget])]], ignore_index=True)\n",
    "#Getting list of patients to drop\n",
    "exclu=df_blankDOB[~pd.isna(df_blankDOB[matchtotarget])][dfunids[0]].to_list()\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[~df[dfunids[0]].isin(exclu)][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df_blankDOB), 'difference: {}'.format(prel-len(df_blankDOB)))\n",
    "try:\n",
    "    del df_blankDOB\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches\n",
      "0 to 0 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#7. DOD, FN as LN, LN as FN, address_1 (IF DOB IS BLANK) (swithced) **\n",
    "df_blankDOB = df[pd.isna(df[dfdob])] #form blankdod subdf (important to do it each time because things are dropped from df not df_blankdod or df_blankdob)\n",
    "prel = len(df_blankDOB)\n",
    "#####\n",
    "df_blankDOB = df_blankDOB.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dffn, dfln, dfaddress_1], right_on=[matchtodod, matchtofn, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "#####\n",
    "\n",
    "df_blankDOB['Tag'] = df_blankDOB.apply(lambda x : tag(x, 'DOD, FN as LN, LN as FN, address_1 (IF DOB IS BLANK) (swithced)'), axis = 1)\n",
    "df_blankDOB['TagIndex'] = df_blankDOB.apply(lambda x : tag_index(x, 7), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df_blankDOB[~pd.isna(df_blankDOB[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df_blankDOB[~pd.isna(df_blankDOB[matchtotarget])]], ignore_index=True)\n",
    "#Getting list of patients to drop\n",
    "exclu=df_blankDOB[~pd.isna(df_blankDOB[matchtotarget])][dfunids[0]].to_list()\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[~df[dfunids[0]].isin(exclu)][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df_blankDOB), 'difference: {}'.format(prel-len(df_blankDOB)))\n",
    "try:\n",
    "    del df_blankDOB\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 matches\n",
      "353583 to 353571 difference: 12\n"
     ]
    }
   ],
   "source": [
    "#8. DOB, DOD, FN [FIRST 3], LN, address_1 (correct order)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn3, dfln, dfaddress_1], right_on=[matchtodob, matchtodod, matchtofn3, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN [FIRST 3], LN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 8), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 matches\n",
      "353571 to 353565 difference: 6\n"
     ]
    }
   ],
   "source": [
    "#9. DOB, DOD, FN [FIRST 1], LN, address_1 (correct order)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn1, dfln, dfaddress_1], right_on=[matchtodob, matchtodod, matchtofn1, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN [FIRST 1], LN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 9), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 matches\n",
      "353565 to 353531 difference: 34\n"
     ]
    }
   ],
   "source": [
    "#10. DOB, DOD, FN [FIRST 3], LN (correct order)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn3, dfln], right_on=[matchtodob, matchtodod, matchtofn3, matchtoln], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN [FIRST 3], LN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 10), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 matches\n",
      "353531 to 353517 difference: 14\n"
     ]
    }
   ],
   "source": [
    "#11. DOB, DOD, FN [FIRST 1], LN (correct order)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn1, dfln], right_on=[matchtodob, matchtodod, matchtofn1, matchtoln], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN [FIRST 1], LN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 11), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 matches\n",
      "353517 to 353505 difference: 12\n"
     ]
    }
   ],
   "source": [
    "#12. DOB, DOD, FN , LN [FIRST 3], address_1 (correct order)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn, dfln3, dfaddress_1], right_on=[matchtodob, matchtodod, matchtofn, matchtoln3, matchtoaddress_1], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN, LN [FIRST 3], address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 12), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 matches\n",
      "353505 to 353502 difference: 3\n"
     ]
    }
   ],
   "source": [
    "#13. DOB, DOD, FN , LN [FIRST 1], address_1 (correct order)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn, dfln1, dfaddress_1], right_on=[matchtodob, matchtodod, matchtofn, matchtoln1, matchtoaddress_1], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN, LN [FIRST 1], address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 13), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 matches\n",
      "353502 to 353479 difference: 23\n"
     ]
    }
   ],
   "source": [
    "#14. DOB, DOD, FN , LN [FIRST 3], address_1 (correct order)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn, dfln3], right_on=[matchtodob, matchtodod, matchtofn, matchtoln3], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN, LN [FIRST 3]'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 14), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 matches\n",
      "353479 to 353475 difference: 4\n"
     ]
    }
   ],
   "source": [
    "#15. DOB, DOD, FN , LN [FIRST 1] (correct order)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn, dfln1], right_on=[matchtodob, matchtodod, matchtofn, matchtoln1], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN, LN [FIRST 1]'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 15), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47 matches\n",
      "353475 to 353428 difference: 47\n"
     ]
    }
   ],
   "source": [
    "#16. DOB, FN, LN, address_1 (no blank DOD clause)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn, dfln, dfaddress_1], right_on=[matchtodob, matchtofn, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN, LN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 16), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_31232\\1317621410.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353428 to 353428 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#17. DOB, FN as LN, LN as FN, address_1 (no blank DOD clause) (switched)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfln, dffn, dfaddress_1], right_on=[matchtodob, matchtofn, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN as LN, LN as FN, address_1 (switched)'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 17), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 matches\n",
      "353428 to 353397 difference: 31\n"
     ]
    }
   ],
   "source": [
    "#18. DOB, FN [first 3], LN, address_1 (no blank DOD clause)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn3, dfln, dfaddress_1], right_on=[matchtodob, matchtofn3, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN [first 3], LN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 18), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 matches\n",
      "353397 to 353381 difference: 16\n"
     ]
    }
   ],
   "source": [
    "#19. DOB, FN [first 1], LN, address_1 (no blank DOD clause)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn1, dfln, dfaddress_1], right_on=[matchtodob, matchtofn1, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN [first 1], LN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 19), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2488 matches\n",
      "353381 to 350894 difference: 2487\n"
     ]
    }
   ],
   "source": [
    "#20. DOB, FN, LN\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn, dfln], right_on=[matchtodob, matchtofn, matchtoln], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN, LN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 20), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_31232\\2939475707.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350894 to 350894 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#21. DOB, FN as LN, LN as FN (switched)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfln, dffn], right_on=[matchtodob, matchtofn, matchtoln], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN as LN, LN as FN (switched)'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 21), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 matches\n",
      "350894 to 350833 difference: 61\n"
     ]
    }
   ],
   "source": [
    "#22. DOB, FN [first 3], LN\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn3, dfln], right_on=[matchtodob, matchtofn3, matchtoln], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN [first 3], LN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 22), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 128 matches\n",
      "350833 to 350705 difference: 128\n"
     ]
    }
   ],
   "source": [
    "#23. DOB, FN [first 1], LN\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn1, dfln], right_on=[matchtodob, matchtofn1, matchtoln], suffixes=('_df', '_matchto')) \n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN [first 1], LN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 23), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 matches\n",
      "350705 to 350694 difference: 11\n"
     ]
    }
   ],
   "source": [
    "#24. DOD, FN, LN, address_1 \n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dffn, dfln, dfaddress_1], right_on=[matchtodod, matchtofn, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN, LN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 24), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_31232\\997854028.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350694 to 350694 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#25. DOD, FN as LN, LN as FN, address_1 (switched)\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dfln, dffn, dfaddress_1], right_on=[matchtodod, matchtofn, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN as LN, LN as FN, address_1 (switched)'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 25), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 matches\n",
      "350694 to 350692 difference: 2\n"
     ]
    }
   ],
   "source": [
    "#26. DOD, FN [first 3], LN, address_1 \n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dffn3, dfln, dfaddress_1], right_on=[matchtodod, matchtofn3, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN [first 3], LN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 26), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_31232\\990614372.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350692 to 350692 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#27. DOD, FN [first 1], LN, address_1 \n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dffn1, dfln, dfaddress_1], right_on=[matchtodod, matchtofn1, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN [first 1], LN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 27), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 matches\n",
      "350692 to 350662 difference: 30\n"
     ]
    }
   ],
   "source": [
    "#28 DOD, FN, LN\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dffn, dfln], right_on=[matchtodod, matchtofn, matchtoln], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN, LN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 28), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 matches\n",
      "350662 to 350661 difference: 1\n"
     ]
    }
   ],
   "source": [
    "#29 DOD, FN as LN, LN as FN (switched)\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dfln, dffn], right_on=[matchtodod, matchtofn, matchtoln], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN as LN, LN as FN (switched)'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 29), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_31232\\1498782288.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350661 to 350661 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#30 DOD, FN [first 3], LN \n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dffn3, dfln], right_on=[matchtodod, matchtofn3, matchtoln], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN [first 3], LN '), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 30), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 matches\n",
      "350661 to 350658 difference: 3\n"
     ]
    }
   ],
   "source": [
    "#31 DOD, FN [first 3], LN \n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dffn1, dfln], right_on=[matchtodod, matchtofn1, matchtoln], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN [first 1], LN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 31), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 matches\n",
      "350658 to 350656 difference: 2\n"
     ]
    }
   ],
   "source": [
    "#32 DOB, DOD, LN, address_1\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dfln, dfaddress_1], right_on=[matchtodob, matchtodod, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, LN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 32), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 matches\n",
      "350656 to 350646 difference: 10\n"
     ]
    }
   ],
   "source": [
    "#33 DOB, DOD, LN\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dfln], right_on=[matchtodob, matchtodod, matchtoln], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, LN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 33), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 matches\n",
      "350646 to 350625 difference: 21\n"
     ]
    }
   ],
   "source": [
    "#34 DOB, FN, LN [first 3], address_1\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn, dfln3, dfaddress_1], right_on=[matchtodob, matchtofn, matchtoln3, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN, LN [first 3], address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 34), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 matches\n",
      "350625 to 350617 difference: 8\n"
     ]
    }
   ],
   "source": [
    "#35 DOB, FN, LN [first 1], address_1\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn, dfln1, dfaddress_1], right_on=[matchtodob, matchtofn, matchtoln1, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN, LN [first 1], address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 35), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_31232\\1750639845.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350617 to 350617 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#36 DOD, FN, LN [first 3], address_1\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dffn, dfln3, dfaddress_1], right_on=[matchtodod, matchtofn, matchtoln3, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN, LN [first 3], address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 36), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MathiasM\\AppData\\Local\\Temp\\ipykernel_31232\\1674374403.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350617 to 350617 difference: 0\n"
     ]
    }
   ],
   "source": [
    "#37 DOD, FN, LN [first 3], address_1\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dffn, dfln1, dfaddress_1], right_on=[matchtodod, matchtofn, matchtoln1, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN, LN [first 1], address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 37), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 matches\n",
      "350617 to 350609 difference: 8\n"
     ]
    }
   ],
   "source": [
    "#38 DOB, DOD, FN, address_1\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn, dfaddress_1], right_on=[matchtodob, matchtodod, matchtofn, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 38), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 matches\n",
      "350609 to 350586 difference: 23\n"
     ]
    }
   ],
   "source": [
    "#39 DOB, DOD, FN\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn], right_on=[matchtodob, matchtodod, matchtofn], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 39), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 matches\n",
      "350586 to 350522 difference: 64\n"
     ]
    }
   ],
   "source": [
    "#40 DOB, FN, LN [first 3]\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn, dfln3], right_on=[matchtodob, matchtofn, matchtoln3], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN, LN [first 3]'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 40), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 789 matches\n",
      "350522 to 349736 difference: 786\n"
     ]
    }
   ],
   "source": [
    "#41 DOB, FN, LN [first 1]\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn, dfln1], right_on=[matchtodob, matchtofn, matchtoln1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN, LN [first 1]'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 41), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 matches\n",
      "349736 to 349734 difference: 2\n"
     ]
    }
   ],
   "source": [
    "#42 DOB, DOD, address_1\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dfaddress_1], right_on=[matchtodob, matchtodod, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 42), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 matches\n",
      "349734 to 349721 difference: 13\n"
     ]
    }
   ],
   "source": [
    "#43 DOB, LN, address_1\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfln, dfaddress_1], right_on=[matchtodob, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, LN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 43), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 matches\n",
      "349721 to 349720 difference: 1\n"
     ]
    }
   ],
   "source": [
    "#43.25 DOB, DOD, FN3, LN3\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn3, dfln3], right_on=[matchtodob, matchtodod, matchtofn3, matchtoln3], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN [first 3], LN [first 3]'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 43.25), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 matches\n",
      "349720 to 349719 difference: 1\n"
     ]
    }
   ],
   "source": [
    "#43.5 DOB, DOD, FN1, LN1\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod, dffn1, dfln1], right_on=[matchtodob, matchtodod, matchtofn1, matchtoln1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, FN [first 1], LN [first 1]'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 43.5), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOD, FN as LN, LN as FN, address_1 [[#IF DOB IS BLANK qualifier]]***********\n",
    "\n",
    "# prel = len(df)\n",
    "# df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dfln, dffn, dfaddress_1], right_on=[matchtodod, matchtofn, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "# df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN as LN, LN as FN, address_1 (switched)'), axis = 1)\n",
    "# #print(df.columns)\n",
    "# print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "# #Save non blank matchtotarget merged rows to output DF\n",
    "# out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "# #remove matched rows and then remove excess columns from merging!\n",
    "# df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "# #Printing delta\n",
    "# print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now: DOD, FN [first 2], LN, address_1  [[#IF DOB IS BLANK qualifier]]\n",
    "\n",
    "# prel = len(df)\n",
    "# df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdod, dffn2, dfln, dfaddress_1], right_on=[matchtodod, matchtofn2, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "# df['Tag'] = df.apply(lambda x : tag(x, 'DOD, FN [first 2], LN, address_1'), axis = 1)\n",
    "# #print(df.columns)\n",
    "# print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "# #Save non blank matchtotarget merged rows to output DF\n",
    "# out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "# #remove matched rows and then remove excess columns from merging!\n",
    "# df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "# #Printing delta\n",
    "# print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now: DOB, DOD, LN, address_1 \n",
    "\n",
    "# prel = len(df)\n",
    "# df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod,dfln, dfaddress_1], right_on=[matchtodob, matchtodod, matchtoln, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "# df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD, LN, address_1'), axis = 1)\n",
    "# #print(df.columns)\n",
    "# print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "# #Save non blank matchtotarget merged rows to output DF\n",
    "# out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "# #remove matched rows and then remove excess columns from merging!\n",
    "# df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "# #Printing delta\n",
    "# print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 matches\n",
      "349719 to 349711 difference: 8\n"
     ]
    }
   ],
   "source": [
    "#45 DOB, DOD\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfdod], right_on=[matchtodob, matchtodod], suffixes=('_df', '_matchto')) #notice how dfln and dffn are switched\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, DOD'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 44), axis=1)\n",
    "\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1476 matches\n",
      "349711 to 348270 difference: 1441\n"
     ]
    }
   ],
   "source": [
    "#45 DOB, LN\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dfln], right_on=[matchtodob, matchtoln], suffixes=('_df', '_matchto')) #notice how dfln and dffn are switched\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, LN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 45), axis=1)\n",
    "\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 matches\n",
      "348270 to 348254 difference: 16\n"
     ]
    }
   ],
   "source": [
    "#46 DOB, FN, address_1\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn, dfaddress_1], right_on=[matchtodob, matchtofn, matchtoaddress_1], suffixes=('_df', '_matchto')) #now kith\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN, address_1'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 46), axis=1)\n",
    "#print(df.columns)\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12155 matches\n",
      "348254 to 337443 difference: 10811\n"
     ]
    }
   ],
   "source": [
    "#47 DOB, FN\n",
    "\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfdob, dffn], right_on=[matchtodob, matchtofn], suffixes=('_df', '_matchto')) #notice how dfln and dffn are switched\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'DOB, FN'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 47), axis=1)\n",
    "\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126752 matches\n",
      "337443 to 304666 difference: 32777\n"
     ]
    }
   ],
   "source": [
    "#48 FN, LN only\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dffn, dfln], right_on=[matchtofn, matchtoln], suffixes=('_df', '_matchto')) #notice how dfln and dffn are switched\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'FN, LN only'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 48), axis=1)\n",
    "\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 626 matches\n",
      "304666 to 304274 difference: 392\n"
     ]
    }
   ],
   "source": [
    "#49. FN as LN, LN as FN (switched)\n",
    "prel = len(df)\n",
    "df = df.merge(matchto[matchto_columns], how = 'left', left_on=[dfln, dffn], right_on=[matchtofn, matchtoln], suffixes=('_df', '_matchto')) #notice how dfln and dffn are switched\n",
    "df['Tag'] = df.apply(lambda x : tag(x, 'FN as LN, LN as FN (switched)'), axis = 1)\n",
    "df['TagIndex'] = df.apply(lambda x : tag_index(x, 49), axis=1)\n",
    "\n",
    "print('Found {} matches'.format(len(df[~pd.isna(df[matchtotarget])])))\n",
    "\n",
    "#Save non blank matchtotarget merged rows to output DF\n",
    "out = pd.concat([out, df[~pd.isna(df[matchtotarget])]], ignore_index=True)\n",
    "#remove matched rows and then remove excess columns from merging!\n",
    "df = df[pd.isna(df[matchtotarget])][df_columns]\n",
    "#Printing delta\n",
    "print(prel, 'to', len(df), 'difference: {}'.format(prel-len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PAT_ID', 'MRN', 'BIRTH_DATE', 'PAT_NAME', 'ADD_LINE_1', 'ADD_LINE_2',\n",
      "       'CITY', 'STATE_C', 'STATE', 'COUNTRY_C', 'COUNTRY', 'HOME_PHONE', 'SSN',\n",
      "       'DEATH_DATE', 'MAX(CONTACT_DATE)', 'Tag', 'gender', 'First Name',\n",
      "       'Middle Name', 'Last Name', 'First Name_f2', 'Last Name_f2',\n",
      "       'First Name_f3', 'Last Name_f3', 'First Name_f1', 'Last Name_f1',\n",
      "       'ADD_LINE_1_first', 'nametag_df', 'F8', 'F20', 'F3', 'F4', 'F5', 'F1',\n",
      "       'F3_f2', 'F5_f2', 'F3_f3', 'F5_f3', 'F3_f1', 'F5_f1', 'F132_first',\n",
      "       'F132', 'F133', 'F134', 'F19', 'TagIndex'],\n",
      "      dtype='object')\n",
      "<class 'str'> <class 'str'>\n",
      "145166\n",
      "Dropped 3339 matches due to gender mismatch. 145166 remaining\n"
     ]
    }
   ],
   "source": [
    "## Excluding matches where genders do NOT match\n",
    "print(out.columns)\n",
    "print(type(out[dfsex].iloc[0]), type(out[matchtosex].iloc[0]))\n",
    "\n",
    "def gendercheck(x):\n",
    "    if not pd.isna(x[dfsex]) and not pd.isna(x[matchtosex]):\n",
    "        if x[dfsex] != x[matchtosex]:\n",
    "            return 'Gender mismatch'\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "prel = len(out)\n",
    "out['gender_match']= out.apply(lambda x : gendercheck(x), axis = 1)\n",
    "out = out[out['gender_match'] != 'Gender mismatch']\n",
    "out=out.drop(columns = ['gender_match'])\n",
    "print(len(out))\n",
    "\n",
    "print('Dropped {} matches due to gender mismatch. {} remaining'.format(prel-len(out), len(out)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145166\n",
      "Dropped 0 matches due to nametag mismatch. 145166 remaining\n"
     ]
    }
   ],
   "source": [
    "## Excluding where nametag index doesn't match\n",
    "\n",
    "def nametagcheck(x):\n",
    "    if not pd.isna(x['nametag_df']) and not pd.isna(x['nametag_matchto']):\n",
    "        if x['nametag_df'] != x['nametag_matchto']:\n",
    "            return 'nametag mismatch'\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "prel = len(out)\n",
    "out['nametag_match']= out.apply(lambda x : gendercheck(x), axis = 1)\n",
    "out = out[out['nametag_match'] != 'Gender mismatch']\n",
    "out=out.drop(columns = ['nametag_match'])\n",
    "print(len(out))\n",
    "\n",
    "print('Dropped {} matches due to nametag mismatch. {} remaining'.format(prel-len(out), len(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 70624 matches due to nametag mismatch. 74542 remaining\n"
     ]
    }
   ],
   "source": [
    "# Dropping matches where last encounter date (df) is after date of death (matchto), if present\n",
    "\n",
    "if 'MAX(CONTACT_DATE)' in out.columns:\n",
    "    def lecheck(x):\n",
    "        led = x['MAX(CONTACT_DATE)']\n",
    "        dod = x[matchtodod]\n",
    "        if not pd.isna(led) and not pd.isna(dod):\n",
    "            #print(dod, led)\n",
    "            if dod < led + datetime.timedelta(days=7):\n",
    "                return 'Date_Error'\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "# print(df[dfdob].iloc[0],df['MAX(CONTACT_DATE)'].iloc[0],df['MAX(CONTACT_DATE)'].iloc[0] + datetime.timedelta(days=7))\n",
    "# if df[dfdob].iloc[3] < df['MAX(CONTACT_DATE)'].iloc[0] + datetime.timedelta(days=7):\n",
    "#     print('dob earlier')\n",
    "    prel = len(out)\n",
    "    out['datecheck'] = out.apply(lambda x : lecheck(x),axis=1)\n",
    "    \n",
    "    #now drop the Date_Error rows and record the difference\n",
    "    out = out[out['datecheck'] != 'Date_Error']\n",
    "\n",
    "    print('Dropped {} matches due to nametag mismatch. {} remaining'.format(prel-len(out), len(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "print(out[dfsex].iloc[0], out[matchtosex].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          count\n",
      "Tag                                            \n",
      "FN, LN only                               63279\n",
      "DOB, FN                                    4974\n",
      "DOB, FN, LN                                2415\n",
      "DOB, FN, LN, address_1 (if DOD is blank)   1362\n",
      "DOB, DOD, FN, LN                           1245\n",
      "DOB, FN, LN [first 1]                       351\n",
      "DOB, LN                                     312\n",
      "FN as LN, LN as FN (switched)               209\n",
      "DOB, FN, LN [first 3]                        54\n",
      "DOB, FN [first 3], LN                        52\n",
      "DOB, FN [first 1], LN                        48\n",
      "DOB, FN, LN, address_1                       35\n",
      "DOB, FN [first 3], LN, address_1             28\n",
      "DOB, FN, LN [first 3], address_1             18\n",
      "DOD, FN, LN                                  16\n",
      "DOB, FN [first 1], LN, address_1             15\n",
      "DOB, DOD, FN [FIRST 3], LN                   14\n",
      "DOB, FN, address_1                           13\n",
      "DOB, LN, address_1                           11\n",
      "DOB, DOD, FN [FIRST 3], LN, address_1        10\n",
      "DOB, DOD, FN, LN [FIRST 3], address_1        10\n",
      "DOB, DOD, FN, LN [FIRST 3]                    9\n",
      "DOD, FN, LN, address_1                        8\n",
      "DOB, FN, LN [first 1], address_1              8\n",
      "DOB, DOD, FN, address_1                       7\n",
      "DOB, DOD, FN                                  7\n",
      "DOB, DOD, LN                                  6\n",
      "DOB, DOD, FN [FIRST 1], LN                    6\n",
      "DOB, DOD                                      5\n",
      "DOB, DOD, FN [FIRST 1], LN, address_1         3\n",
      "DOB, DOD, FN, LN [FIRST 1], address_1         3\n",
      "DOB, DOD, FN, LN [FIRST 1]                    2\n",
      "DOB, DOD, address_1                           2\n",
      "DOD, FN [first 3], LN, address_1              2\n",
      "DOD, FN [first 1], LN                         1\n",
      "DOB, DOD, FN [first 3], LN [first 3]          1\n",
      "DOB, DOD, FN [first 1], LN [first 1]          1\n",
      "\n",
      "Total: 74542 matches\n"
     ]
    }
   ],
   "source": [
    "print(out['Tag'].value_counts().to_frame())\n",
    "print('\\nTotal: {} matches'.format(len(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No SSN columns detected, skipping SSN LD calc.\n"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance as ld\n",
    "\n",
    "def ldist(x, col1, col2):\n",
    "    if not pd.isna(x[col1]) and not pd.isna(x[col2]):\n",
    "        return ld(str(x[col1]), str(x[col2]))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def daydist(x, col1, col2):\n",
    "    if not pd.isna(x[col1]) and not pd.isna(x[col2]):\n",
    "        return (x[dfdob] - x[matchtodob]).total_seconds()/86400\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "if 'matchtossn' in locals():\n",
    "    out['ssn_diff'] = out.apply(lambda x : ldist(x, 'SSN', 'ssn'), axis = 1)\n",
    "else:\n",
    "    print('No SSN columns detected, skipping SSN LD calc.')\n",
    "\n",
    "#levenshtein difference between DOB strings\n",
    "out['date_ld_diff'] = out.apply(lambda x : ldist(x, dfdob, matchtodob), axis = 1)\n",
    "\n",
    "#datetime difference between DOB strings\n",
    "out['date_day_diff'] = out.apply(lambda x : daydist(x, dfdob, matchtodob), axis = 1)\n",
    "\n",
    "\n",
    "## Doing these differences for DOD too! Just change vars \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(r\"Z:\\Marco Mathias\\OSCAR\\Match names\\OscarMatching_DC_Full_FirstPass.xlsx\") as writer:\n",
    "    out.to_excel(writer, index = False, sheet_name='matched')\n",
    "    df.to_excel(writer, sheet_name='unmatched', index = False)\n",
    "\n",
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions initialized, loading input file\n",
      "Files loaded\n",
      "Running distance script...\n",
      "bukisuhl subset of bruriabukisuhl (71.979)\n",
      "rossstrange subset of rossstrangedunn (85.745)\n",
      "salvadorfloreslopez subset of salvadorflores (85.899)\n",
      "eileengallo subset of eileengalloross (85.745)\n",
      "irmagonzalez subset of irmagonzalezmendez (81.778)\n",
      "manuelosorio subset of manuelosoriopleitez (79.586)\n",
      "joannwilliams subset of joannwilliamsaustin (82.766)\n",
      "fidelsalvatierra subset of fidelsalvatierramora (89.444)\n",
      "juanmataolvera subset of juanmata (76.003)\n",
      "jesussandoval subset of jesussandovalvalencia (78.812)\n",
      "fernandoalvarez subset of fernandoalvarezcalles (84.505)\n",
      "stephaniejacksonhutchinson subset of stephaniejackson (78.643)\n",
      "erniefields subset of erniefieldsjr. (88.628)\n",
      "edwinescobarcastro subset of edwinescobar (81.778)\n",
      "arceliahorta subset of arceliahortarodriguez (76.003)\n",
      "mariavaldespedraza subset of mariavaldes (78.446)\n",
      "barbaraorangestarks subset of barbaraorange (82.766)\n",
      "caroltaylorbarrett subset of caroltaylor (78.446)\n",
      "williamwood subset of williamwoodjr. (88.628)\n",
      "irisbritos subset of irisbritosalvarez (76.988)\n",
      "sylviaschiff subset of sylviaschiffkatz (86.676)\n",
      "lesliecohenseidman subset of lesliecohen (78.446)\n",
      "robertozuniga subset of robertozunigagomez (85.056)\n",
      "andreastonefield subset of andreastonefieldpolgar (85.278)\n",
      "jorgeandrade subset of jorgeandradedeleon (81.778)\n",
      "javiermendozaserafin subset of javiermendoza (80.824)\n",
      "marciastillman subset of marciastillmantull (88.285)\n",
      "edithsumano subset of edithsumanomendoza (78.446)\n",
      "cherylvalladares subset of cherylvalladaresgorden (85.278)\n",
      "reginapeters subset of reginapeterswright (81.778)\n",
      "hectormolina subset of hectormolinamedina (81.778)\n",
      "anaperez subset of anaperezreyes (78.643)\n",
      "johndoe298985 subset of johndoe (73.846)\n",
      "ceciliamartinez subset of ceciliamartinezmendoza (82.659)\n",
      "angellopez subset of angellopezhernandez (73.067)\n",
      "marialopezguerra subset of marialopez (79.285)\n",
      "lindaalexanderbailey subset of lindaalexander (83.671)\n",
      "mariarodriguez subset of mariarodriguezlopez (85.899)\n",
      "rodolfomagallon subset of rodolfomagallonsalcedo (82.659)\n",
      "nicolasmori subset of nicolasmoriperez (82.913)\n",
      "miriambirch subset of miriambirchpennington (72.947)\n",
      "jorgegonzalezordiano subset of jorgegonzalez (80.824)\n",
      "paulinopayan subset of paulinopayanmartinez (77.733)\n",
      "juliomolica subset of juliomolicapichioni (76.354)\n",
      "josearmenta subset of josearmentarodriguez (74.594)\n",
      "martingomez subset of martingomezrodriguez (74.594)\n",
      "josefinaflores subset of josefinafloresdeortega (80.004)\n",
      "claudiaromero subset of claudiaromerocordero (80.824)\n",
      "hectorcapristoramirez subset of hectorcapristo (81.778)\n",
      "juansegovia subset of juansegoviaromero (80.691)\n",
      "socorropadillabautista subset of socorropadilla (80.004)\n",
      "beverlycrochettgillespie subset of beverlycrochett (79.285)\n",
      "luisromero subset of luisromeromoreno (79.285)\n",
      "joseleon subset of joseleonparamo (76.003)\n",
      "agripinagarcia subset of agripinagarciahernandez (78.335)\n",
      "antoniomendoza subset of antoniomendozahernandez (78.335)\n",
      "(74542, 50)\n",
      "Done with distance script, saving...\n",
      "Matched cases saved\n",
      "Unmatched cases saved\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import jellyfish\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "import textdistance as td\n",
    "\n",
    "i=0\n",
    "\n",
    "def calc_distances(df_string, matchto_string):\n",
    "    outdict = {}\n",
    "    s1 = df_string\n",
    "    s2 = matchto_string\n",
    "    outdict['s1'] = s1\n",
    "    outdict['s2'] = s2\n",
    "    \n",
    "    #want to output to dataframe tbh. See how to explode dict into columns\n",
    "    #outdict['l_d'] = round(100*(1-ld(s1,s2)/max(len(s1), len(s2))),3)\n",
    "    outdict['jelly_jaro'] = round(100*jellyfish.jaro_similarity(s1, s2),3)\n",
    "    outdict['jelly_jaro_winkler'] = round(100*jellyfish.jaro_winkler_similarity(s1, s2),3)\n",
    "    outdict['jelly_damerau'] = round(100*(1-jellyfish.damerau_levenshtein_distance(s1, s2)/max(len(s1),len(s2))),3)\n",
    "    #outdict['jelly_jaccard'] = jellyfish.jaccard_similarity(s1, s2)\n",
    "    outdict['fuzzy'] = fuzz.ratio(s1, s2)\n",
    "    outdict['ratcliff-obershelp'] = round(100*max([td.ratcliff_obershelp(s1, s2), td.ratcliff_obershelp(s2,s1)]),3)\n",
    "\n",
    "    #outdict['combined_perc'] = (outdict['l_d']+outdict['jelly_jaro']+outdict['jelly_jaro_winkler']+outdict['jelly_damerau']+outdict['fuzzy'])/5\n",
    "    outdict['combined_unbiased'] = round((outdict['jelly_jaro']+outdict['jelly_jaro_winkler']+outdict['jelly_damerau']+outdict['fuzzy']+outdict['ratcliff-obershelp'])/5,3)\n",
    "    #print(outdict)\n",
    "    return outdict\n",
    "\n",
    "def issubset_combo(dffull, matchtofull):\n",
    "    \n",
    "    if dffull in matchtofull: #subset\n",
    "        return True\n",
    "    elif matchtofull in dffull: #subset other way\n",
    "        return True\n",
    "    \n",
    "\n",
    "def calc_distances_withswitch(x, dffn, dfmn, dfln, matchtofn, matchtomn, matchtoln, sep, out_format, sort_var):\n",
    "    df_fn = x[dffn]\n",
    "    df_ln = x[dfln]\n",
    "    df_mn = x[dfmn]\n",
    "    matchto_fn = x[matchtofn]\n",
    "    matchto_ln = x[matchtoln]\n",
    "    matchto_mn = x[matchtomn]\n",
    "\n",
    "    #checking for nan values\n",
    "\n",
    "    if sum([ pd.isna(i) for i in [df_fn, df_mn, df_ln, matchto_fn, matchto_mn, matchto_ln] ]) == 0: #if none of these four values are blank\n",
    "\n",
    "        if sort_var == 'sort':\n",
    "            dffull = sep.join([df_fn, df_ln])\n",
    "            dffull_reverse = sep.join([df_ln, df_fn])\n",
    "            matchtofull = sep.join([matchto_fn, matchto_ln])\n",
    "        elif sort_var == 'normal':\n",
    "            dffull = sep.join([df_fn, df_ln])\n",
    "            dffull_reverse = sep.join([df_ln, df_fn])\n",
    "            matchtofull = sep.join([matchto_fn, matchto_ln])\n",
    "        elif sort_var != 'sort' and sort_var != 'normal':\n",
    "            raise Exception('Please only pass \"sort\" and \"normal\" to sort_var argument')\n",
    "        \n",
    "\n",
    "        normal = calc_distances(dffull,matchtofull)\n",
    "        reverse = calc_distances(dffull_reverse,matchtofull)\n",
    "\n",
    "        if normal['combined_unbiased'] > reverse['combined_unbiased']:\n",
    "            if normal['combined_unbiased'] < 90 :\n",
    "                if issubset_combo(dffull, matchtofull):\n",
    "                    print('{} subset of {} ({})'.format(dffull, matchtofull, normal['combined_unbiased']))\n",
    "\n",
    "            if out_format == 'full':\n",
    "                return normal\n",
    "            elif out_format == 'score_only':\n",
    "                return normal['combined_unbiased']\n",
    "            else:\n",
    "                raise Exception('Could not understand argument \"{}\". Please only pass \"full\" or \"score_only\"'.format(out_format))\n",
    "        else:\n",
    "            if reverse['combined_unbiased'] < 90 :\n",
    "                if issubset_combo(dffull, matchtofull):\n",
    "                    print('{} subset of {} ({})'.format(dffull, matchtofull, reverse['combined_unbiased']))\n",
    "            \n",
    "            if out_format == 'full':\n",
    "                return reverse\n",
    "            elif out_format == 'score_only':\n",
    "                return reverse['combined_unbiased']\n",
    "            else:\n",
    "                raise Exception('Could not understand argument \"{}\". Please only pass \"full\" or \"score_only\"'.format(out_format))\n",
    "    else:\n",
    "        return None\n",
    "print('Functions initialized, loading input file')\n",
    "filep = r\"Z:\\Marco Mathias\\OSCAR\\Match names\\OscarMatching_DC_Full_FirstPass.xlsx\"\n",
    "# file = pd.read_excel(filep, sheet_name='matched')\n",
    "# file_unm = pd.read_excel(filep, sheet_name='unmatched')\n",
    "print('Files loaded')\n",
    "# dffn = 'First Name'\n",
    "# dfln = 'Last Name'\n",
    "# matchtofn = 'FNAME'\n",
    "# matchtoln = 'LNAME'\n",
    "print('Running distance script...')\n",
    "out['outdict'] = out.apply(lambda x : calc_distances_withswitch(x, dffn, dfmn, dfln, matchtofn, matchtomn, matchtoln, '', 'score_only', 'sort'), axis = 1 )\n",
    "print(out.shape)\n",
    "print('Done with distance script, saving...')\n",
    "\n",
    "out = out[[i for i in out.columns if i not in [dffn1,dffn2,dffn3,matchtofn1,matchtofn2,matchtofn3,dfln1,dfln2,dfln3,matchtoln1,matchtoln2,matchtoln3,'datecheck']]]\n",
    "\n",
    "with pd.ExcelWriter(filep)  as writer:\n",
    "    out.to_excel(writer, sheet_name='matched', index = False)\n",
    "    print('Matched cases saved')\n",
    "    df.to_excel(writer,sheet_name='unmatched', index = False)\n",
    "    print('Unmatched cases saved')\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## First matching stage: DOD + DOB + FN + LN (in correct spot) -- using for loops\n",
    "# from tqdm.notebook import tqdm_notebook\n",
    "# import re\n",
    "# op=0\n",
    "# for k in tqdm_notebook(range(0,len(df))):\n",
    "#     first_name_df = str(df[dffn].iloc[k])\n",
    "#     last_name_df = str(df[dfln].iloc[k])\n",
    "#     dob_df = df[dfdob].iloc[k]\n",
    "#     doa_df = df[dfdod].iloc[k]\n",
    "#     #doa_dfull = dfull1['F20'].iloc[k]\n",
    "#     #tempdf = dfull1[dfull1['F8'] == dob_df]\n",
    "\n",
    "#     #if first_name_df == 'alan' and last_name_df =='hempleman':\n",
    "#         #tempdf3 = dfull1[dfull1['F5'] == 'hempleman']\n",
    "#     tempdf = matchto[(matchto[matchtodob] == dob_df) & (matchto[matchtodod] == doa_df) & (matchto[matchtofn] == first_name_df) & (matchto[matchtoln] == last_name_df)] #bottleneck\n",
    "#         #a = pd.to_datetime(tempdf3['F8'].iloc[1])\n",
    "        \n",
    "#     # #print(first_name_df, last_name_df, dob_df, doa_df,' ',len(tempdf))\n",
    "#     if not tempdf.empty:\n",
    "#         #print(first_name_df, last_name_df, dob_df, doa_df,' ',len(tempdf))\n",
    "    \n",
    "#         if first_name_df in [i for i in tempdf[matchtofn].values] and last_name_df in [i for i in tempdf[matchtoln].values]:\n",
    "#             for z in range(0,len(tempdf)):\n",
    "#                 if tempdf[matchtofn].iloc[z] == first_name_df and tempdf[matchtoln].iloc[z] == last_name_df:\n",
    "#                         df.loc[k,matchtotarget] = tempdf[matchtotarget].iloc[z]\n",
    "#                         #df.loc[k, 'F2'] = tempdf['F2'].iloc[z]\n",
    "#                         df.loc[k, 'Done'] = 1\n",
    "#                         df.loc[k, 'Tag'] = 'Full name, DOB, DOA'\n",
    "#                         df.loc[k, matchtofn] = tempdf[matchtofn].iloc[z].lower()\n",
    "#                         df.loc[k, matchtoln] = tempdf[matchtoln].iloc[z].lower()\n",
    "#                         df.loc[k, matchtodob] = tempdf[matchtodob].iloc[z]\n",
    "#                         df.loc[k, matchtodod] = tempdf[matchtodod].iloc[z]\n",
    "#                         op=op+1\n",
    "#                         print(op,end='\\r')\n",
    "                            \n",
    "                \n",
    "# fin = df[~pd.isna(df['Tag'])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
